{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo de este enfoque, sera generar un/os modelo/s para predecir los atributos del dia siguiente al ultimo disponible en el dataset. Aqui se aplicaran 2 enfoques:\n",
    "\n",
    "- Un modelo que prediga todas las variablse en simultaneo (con el objetivo de captar la interrelacion entre las mismas).\n",
    "- Un modelo que prediga solamente la variable target (incialmente se realizaran pruebas con la variable Close, y luego se procedera a usar la variable Tendencia).\n",
    "\n",
    "Una vez realiza la prediccion de los atributos del dia siguiente, se procedera a realizar la prediccion de la Tendencia/Close, se realimientara el dataset, y se procedere a predecir otro dia, repitiendo esto N veces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-09 08:10:15.893781: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Flatten, Conv1D, MaxPooling1D, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pmdarima.arima import auto_arima\n",
    "from statsmodels.tsa.api import VAR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from neuralprophet import NeuralProphet\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
    "import datetime\n",
    "from prophet import Prophet\n",
    "from prophet.diagnostics import performance_metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.base import RegressorMixin\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from sklearn.metrics import make_scorer\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.base import clone\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from skopt import BayesSearchCV\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    'Open',\n",
    "    'High',\n",
    "    'Low',\n",
    "    'Close',\n",
    "    'Number of trades',\n",
    "    'Close_BTCUSDT',\n",
    "    'Volume_BTCUSDT',\n",
    "    'Number_of_trades_BTCUSDT',\n",
    "    'Close_ETHUSDT',\n",
    "    'Volume_ETHUSDT',\n",
    "    'Number_of_trades_ETHUSDT',\n",
    "    'Close_BNBUSDT',\n",
    "    'Volume_BNBUSDT',\n",
    "    'Number_of_trades_BNBUSDT',\n",
    "    'SMA_20',\n",
    "    'EMA_20',\n",
    "    'Upper_Band',\n",
    "    'Middle_Band',\n",
    "    'Lower_Band',\n",
    "    'RSI',\n",
    "    'buy_1000x_high_coinbase',\n",
    "    'sell_1000x_high_coinbase',\n",
    "    'total_trades_coinbase',\t\n",
    "    'Tweets_Utilizados',\n",
    "    'Tweets_Utilizados_coin',\n",
    "    'Tweets_Utilizados_referentes',\n",
    "    'Tweets_Utilizados_whale_alert',\n",
    "    'Buy_1000x_high',\n",
    "    'sell_1000x_high',\n",
    "    'total_trades_binance'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('/Users/mmarchetta/Desktop/Tesis-2024/data-visualization/final_dataset.csv')\n",
    "\n",
    "dataset['Open_time'] = pd.to_datetime(dataset['Open_time'])\n",
    "dates = dataset['Open_time'][:-5]\n",
    "\n",
    "# dataset.drop(['Sentimiento'], axis=1, inplace=True)\n",
    "# dataset.drop(['Sentimiento_coin'], axis=1, inplace=True)\n",
    "# dataset.drop(['Sentimiento_referentes'], axis=1, inplace=True)\n",
    "# dataset.drop(columns=['Open_time'], inplace=True)\n",
    "\n",
    "dataset = dataset.round(2) # Limitar los valores float a 2 decimales en todo el dataframe\n",
    "\n",
    "feature_dataset = dataset[columns]\n",
    "# feature_dataset.drop(['Tendencia'], axis=1, inplace=True)\n",
    "\n",
    "validation = feature_dataset[-5:]\n",
    "feature_dataset = feature_dataset[:-5]\n",
    "\n",
    "n_days_to_predict = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Number of trades</th>\n",
       "      <th>Close_BTCUSDT</th>\n",
       "      <th>Volume_BTCUSDT</th>\n",
       "      <th>Number_of_trades_BTCUSDT</th>\n",
       "      <th>Close_ETHUSDT</th>\n",
       "      <th>Volume_ETHUSDT</th>\n",
       "      <th>Number_of_trades_ETHUSDT</th>\n",
       "      <th>Close_BNBUSDT</th>\n",
       "      <th>Volume_BNBUSDT</th>\n",
       "      <th>Number_of_trades_BNBUSDT</th>\n",
       "      <th>SMA_20</th>\n",
       "      <th>EMA_20</th>\n",
       "      <th>Upper_Band</th>\n",
       "      <th>Middle_Band</th>\n",
       "      <th>Lower_Band</th>\n",
       "      <th>RSI</th>\n",
       "      <th>buy_1000x_high_coinbase</th>\n",
       "      <th>sell_1000x_high_coinbase</th>\n",
       "      <th>total_trades_coinbase</th>\n",
       "      <th>Tweets_Utilizados</th>\n",
       "      <th>Tweets_Utilizados_coin</th>\n",
       "      <th>Tweets_Utilizados_referentes</th>\n",
       "      <th>Tweets_Utilizados_whale_alert</th>\n",
       "      <th>Buy_1000x_high</th>\n",
       "      <th>sell_1000x_high</th>\n",
       "      <th>total_trades_binance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28.84</td>\n",
       "      <td>30.26</td>\n",
       "      <td>27.50</td>\n",
       "      <td>27.71</td>\n",
       "      <td>449178.0</td>\n",
       "      <td>42147.35</td>\n",
       "      <td>39776.84</td>\n",
       "      <td>1001487.0</td>\n",
       "      <td>2925.59</td>\n",
       "      <td>510130.73</td>\n",
       "      <td>1043885.0</td>\n",
       "      <td>335.5</td>\n",
       "      <td>956544.07</td>\n",
       "      <td>457187.0</td>\n",
       "      <td>31.85</td>\n",
       "      <td>30.83</td>\n",
       "      <td>38.30</td>\n",
       "      <td>31.85</td>\n",
       "      <td>25.39</td>\n",
       "      <td>44.11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>139</td>\n",
       "      <td>135</td>\n",
       "      <td>1.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>525.0</td>\n",
       "      <td>364.0</td>\n",
       "      <td>270000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27.72</td>\n",
       "      <td>28.38</td>\n",
       "      <td>26.14</td>\n",
       "      <td>26.31</td>\n",
       "      <td>362304.0</td>\n",
       "      <td>41026.54</td>\n",
       "      <td>43372.26</td>\n",
       "      <td>1045389.0</td>\n",
       "      <td>2804.91</td>\n",
       "      <td>511325.46</td>\n",
       "      <td>928494.0</td>\n",
       "      <td>333.0</td>\n",
       "      <td>922077.23</td>\n",
       "      <td>417006.0</td>\n",
       "      <td>31.77</td>\n",
       "      <td>30.40</td>\n",
       "      <td>38.44</td>\n",
       "      <td>31.77</td>\n",
       "      <td>25.11</td>\n",
       "      <td>41.83</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5273.0</td>\n",
       "      <td>93</td>\n",
       "      <td>122</td>\n",
       "      <td>2.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>472.0</td>\n",
       "      <td>331.0</td>\n",
       "      <td>204000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26.31</td>\n",
       "      <td>28.59</td>\n",
       "      <td>26.11</td>\n",
       "      <td>27.28</td>\n",
       "      <td>376232.0</td>\n",
       "      <td>41524.28</td>\n",
       "      <td>33511.53</td>\n",
       "      <td>884909.0</td>\n",
       "      <td>2850.45</td>\n",
       "      <td>411305.09</td>\n",
       "      <td>748804.0</td>\n",
       "      <td>367.7</td>\n",
       "      <td>1696420.04</td>\n",
       "      <td>653011.0</td>\n",
       "      <td>31.65</td>\n",
       "      <td>30.10</td>\n",
       "      <td>38.55</td>\n",
       "      <td>31.65</td>\n",
       "      <td>24.74</td>\n",
       "      <td>43.99</td>\n",
       "      <td>22.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>54144.0</td>\n",
       "      <td>112</td>\n",
       "      <td>145</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>495.0</td>\n",
       "      <td>216000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27.28</td>\n",
       "      <td>28.99</td>\n",
       "      <td>27.13</td>\n",
       "      <td>28.62</td>\n",
       "      <td>339737.0</td>\n",
       "      <td>43824.10</td>\n",
       "      <td>46381.23</td>\n",
       "      <td>1197815.0</td>\n",
       "      <td>3000.61</td>\n",
       "      <td>506896.76</td>\n",
       "      <td>992243.0</td>\n",
       "      <td>387.5</td>\n",
       "      <td>1163674.21</td>\n",
       "      <td>551245.0</td>\n",
       "      <td>31.62</td>\n",
       "      <td>29.96</td>\n",
       "      <td>38.56</td>\n",
       "      <td>31.62</td>\n",
       "      <td>24.67</td>\n",
       "      <td>46.92</td>\n",
       "      <td>15.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>39220.0</td>\n",
       "      <td>116</td>\n",
       "      <td>147</td>\n",
       "      <td>2.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>419.0</td>\n",
       "      <td>464.0</td>\n",
       "      <td>202000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28.61</td>\n",
       "      <td>32.33</td>\n",
       "      <td>28.50</td>\n",
       "      <td>31.94</td>\n",
       "      <td>735059.0</td>\n",
       "      <td>48141.61</td>\n",
       "      <td>66244.87</td>\n",
       "      <td>1771237.0</td>\n",
       "      <td>3309.91</td>\n",
       "      <td>648714.62</td>\n",
       "      <td>1446386.0</td>\n",
       "      <td>421.5</td>\n",
       "      <td>1440336.04</td>\n",
       "      <td>727854.0</td>\n",
       "      <td>31.64</td>\n",
       "      <td>30.15</td>\n",
       "      <td>38.58</td>\n",
       "      <td>31.64</td>\n",
       "      <td>24.69</td>\n",
       "      <td>53.42</td>\n",
       "      <td>24.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>63183.0</td>\n",
       "      <td>171</td>\n",
       "      <td>141</td>\n",
       "      <td>1.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>477.0</td>\n",
       "      <td>664.0</td>\n",
       "      <td>492000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Open   High    Low  Close  Number of trades  Close_BTCUSDT  \\\n",
       "0  28.84  30.26  27.50  27.71          449178.0       42147.35   \n",
       "1  27.72  28.38  26.14  26.31          362304.0       41026.54   \n",
       "2  26.31  28.59  26.11  27.28          376232.0       41524.28   \n",
       "3  27.28  28.99  27.13  28.62          339737.0       43824.10   \n",
       "4  28.61  32.33  28.50  31.94          735059.0       48141.61   \n",
       "\n",
       "   Volume_BTCUSDT  Number_of_trades_BTCUSDT  Close_ETHUSDT  Volume_ETHUSDT  \\\n",
       "0        39776.84                 1001487.0        2925.59       510130.73   \n",
       "1        43372.26                 1045389.0        2804.91       511325.46   \n",
       "2        33511.53                  884909.0        2850.45       411305.09   \n",
       "3        46381.23                 1197815.0        3000.61       506896.76   \n",
       "4        66244.87                 1771237.0        3309.91       648714.62   \n",
       "\n",
       "   Number_of_trades_ETHUSDT  Close_BNBUSDT  Volume_BNBUSDT  \\\n",
       "0                 1043885.0          335.5       956544.07   \n",
       "1                  928494.0          333.0       922077.23   \n",
       "2                  748804.0          367.7      1696420.04   \n",
       "3                  992243.0          387.5      1163674.21   \n",
       "4                 1446386.0          421.5      1440336.04   \n",
       "\n",
       "   Number_of_trades_BNBUSDT  SMA_20  EMA_20  Upper_Band  Middle_Band  \\\n",
       "0                  457187.0   31.85   30.83       38.30        31.85   \n",
       "1                  417006.0   31.77   30.40       38.44        31.77   \n",
       "2                  653011.0   31.65   30.10       38.55        31.65   \n",
       "3                  551245.0   31.62   29.96       38.56        31.62   \n",
       "4                  727854.0   31.64   30.15       38.58        31.64   \n",
       "\n",
       "   Lower_Band    RSI  buy_1000x_high_coinbase  sell_1000x_high_coinbase  \\\n",
       "0       25.39  44.11                      0.0                       0.0   \n",
       "1       25.11  41.83                      4.0                       2.0   \n",
       "2       24.74  43.99                     22.0                      40.0   \n",
       "3       24.67  46.92                     15.0                      23.0   \n",
       "4       24.69  53.42                     24.0                      35.0   \n",
       "\n",
       "   total_trades_coinbase  Tweets_Utilizados  Tweets_Utilizados_coin  \\\n",
       "0                    0.0                139                     135   \n",
       "1                 5273.0                 93                     122   \n",
       "2                54144.0                112                     145   \n",
       "3                39220.0                116                     147   \n",
       "4                63183.0                171                     141   \n",
       "\n",
       "   Tweets_Utilizados_referentes  Tweets_Utilizados_whale_alert  \\\n",
       "0                           1.0                           81.0   \n",
       "1                           2.0                           87.0   \n",
       "2                           0.0                           64.0   \n",
       "3                           2.0                           77.0   \n",
       "4                           1.0                           71.0   \n",
       "\n",
       "   Buy_1000x_high  sell_1000x_high  total_trades_binance  \n",
       "0           525.0            364.0              270000.0  \n",
       "1           472.0            331.0              204000.0  \n",
       "2           594.0            495.0              216000.0  \n",
       "3           419.0            464.0              202000.0  \n",
       "4           477.0            664.0              492000.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(903, 30)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Number of trades</th>\n",
       "      <th>Close_BTCUSDT</th>\n",
       "      <th>Volume_BTCUSDT</th>\n",
       "      <th>Number_of_trades_BTCUSDT</th>\n",
       "      <th>Close_ETHUSDT</th>\n",
       "      <th>Volume_ETHUSDT</th>\n",
       "      <th>Number_of_trades_ETHUSDT</th>\n",
       "      <th>Close_BNBUSDT</th>\n",
       "      <th>Volume_BNBUSDT</th>\n",
       "      <th>Number_of_trades_BNBUSDT</th>\n",
       "      <th>SMA_20</th>\n",
       "      <th>EMA_20</th>\n",
       "      <th>Upper_Band</th>\n",
       "      <th>Middle_Band</th>\n",
       "      <th>Lower_Band</th>\n",
       "      <th>RSI</th>\n",
       "      <th>buy_1000x_high_coinbase</th>\n",
       "      <th>sell_1000x_high_coinbase</th>\n",
       "      <th>total_trades_coinbase</th>\n",
       "      <th>Tweets_Utilizados</th>\n",
       "      <th>Tweets_Utilizados_coin</th>\n",
       "      <th>Tweets_Utilizados_referentes</th>\n",
       "      <th>Tweets_Utilizados_whale_alert</th>\n",
       "      <th>Buy_1000x_high</th>\n",
       "      <th>sell_1000x_high</th>\n",
       "      <th>total_trades_binance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>903</th>\n",
       "      <td>10.08</td>\n",
       "      <td>10.46</td>\n",
       "      <td>9.60</td>\n",
       "      <td>9.90</td>\n",
       "      <td>245319.0</td>\n",
       "      <td>67609.99</td>\n",
       "      <td>55691.08</td>\n",
       "      <td>2464515.0</td>\n",
       "      <td>3520.46</td>\n",
       "      <td>570901.29</td>\n",
       "      <td>1906387.0</td>\n",
       "      <td>555.4</td>\n",
       "      <td>2284301.81</td>\n",
       "      <td>994512.0</td>\n",
       "      <td>10.06</td>\n",
       "      <td>9.95</td>\n",
       "      <td>11.86</td>\n",
       "      <td>10.06</td>\n",
       "      <td>8.26</td>\n",
       "      <td>52.48</td>\n",
       "      <td>34.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>84706.0</td>\n",
       "      <td>696</td>\n",
       "      <td>471</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>154000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>9.90</td>\n",
       "      <td>9.99</td>\n",
       "      <td>8.60</td>\n",
       "      <td>8.77</td>\n",
       "      <td>341363.0</td>\n",
       "      <td>61937.40</td>\n",
       "      <td>101005.32</td>\n",
       "      <td>3593832.0</td>\n",
       "      <td>3158.64</td>\n",
       "      <td>1049629.69</td>\n",
       "      <td>2647385.0</td>\n",
       "      <td>507.7</td>\n",
       "      <td>2551361.51</td>\n",
       "      <td>1213572.0</td>\n",
       "      <td>10.08</td>\n",
       "      <td>9.84</td>\n",
       "      <td>11.81</td>\n",
       "      <td>10.08</td>\n",
       "      <td>8.35</td>\n",
       "      <td>42.93</td>\n",
       "      <td>120.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>135180.0</td>\n",
       "      <td>961</td>\n",
       "      <td>509</td>\n",
       "      <td>1.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>534.0</td>\n",
       "      <td>433.0</td>\n",
       "      <td>221000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>8.77</td>\n",
       "      <td>9.57</td>\n",
       "      <td>8.49</td>\n",
       "      <td>9.48</td>\n",
       "      <td>267797.0</td>\n",
       "      <td>67840.51</td>\n",
       "      <td>90420.59</td>\n",
       "      <td>3549793.0</td>\n",
       "      <td>3516.53</td>\n",
       "      <td>1207322.82</td>\n",
       "      <td>2987953.0</td>\n",
       "      <td>556.8</td>\n",
       "      <td>1425296.58</td>\n",
       "      <td>809335.0</td>\n",
       "      <td>10.14</td>\n",
       "      <td>9.80</td>\n",
       "      <td>11.68</td>\n",
       "      <td>10.14</td>\n",
       "      <td>8.60</td>\n",
       "      <td>49.21</td>\n",
       "      <td>185.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>112997.0</td>\n",
       "      <td>866</td>\n",
       "      <td>555</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>473.0</td>\n",
       "      <td>386.0</td>\n",
       "      <td>171000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>9.48</td>\n",
       "      <td>9.58</td>\n",
       "      <td>9.07</td>\n",
       "      <td>9.18</td>\n",
       "      <td>156774.0</td>\n",
       "      <td>65501.27</td>\n",
       "      <td>53357.48</td>\n",
       "      <td>2388390.0</td>\n",
       "      <td>3492.85</td>\n",
       "      <td>602755.21</td>\n",
       "      <td>1791989.0</td>\n",
       "      <td>553.8</td>\n",
       "      <td>953921.37</td>\n",
       "      <td>563996.0</td>\n",
       "      <td>10.17</td>\n",
       "      <td>9.74</td>\n",
       "      <td>11.63</td>\n",
       "      <td>10.17</td>\n",
       "      <td>8.71</td>\n",
       "      <td>46.85</td>\n",
       "      <td>64.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>66543.0</td>\n",
       "      <td>692</td>\n",
       "      <td>533</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>101000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>9.18</td>\n",
       "      <td>9.37</td>\n",
       "      <td>8.69</td>\n",
       "      <td>8.94</td>\n",
       "      <td>147578.0</td>\n",
       "      <td>63796.64</td>\n",
       "      <td>51482.38</td>\n",
       "      <td>2492881.0</td>\n",
       "      <td>3336.35</td>\n",
       "      <td>558848.89</td>\n",
       "      <td>1747756.0</td>\n",
       "      <td>553.8</td>\n",
       "      <td>1181298.51</td>\n",
       "      <td>712381.0</td>\n",
       "      <td>10.14</td>\n",
       "      <td>9.67</td>\n",
       "      <td>11.67</td>\n",
       "      <td>10.14</td>\n",
       "      <td>8.62</td>\n",
       "      <td>45.00</td>\n",
       "      <td>57.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>68616.0</td>\n",
       "      <td>681</td>\n",
       "      <td>546</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>92000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Open   High   Low  Close  Number of trades  Close_BTCUSDT  \\\n",
       "903  10.08  10.46  9.60   9.90          245319.0       67609.99   \n",
       "904   9.90   9.99  8.60   8.77          341363.0       61937.40   \n",
       "905   8.77   9.57  8.49   9.48          267797.0       67840.51   \n",
       "906   9.48   9.58  9.07   9.18          156774.0       65501.27   \n",
       "907   9.18   9.37  8.69   8.94          147578.0       63796.64   \n",
       "\n",
       "     Volume_BTCUSDT  Number_of_trades_BTCUSDT  Close_ETHUSDT  Volume_ETHUSDT  \\\n",
       "903        55691.08                 2464515.0        3520.46       570901.29   \n",
       "904       101005.32                 3593832.0        3158.64      1049629.69   \n",
       "905        90420.59                 3549793.0        3516.53      1207322.82   \n",
       "906        53357.48                 2388390.0        3492.85       602755.21   \n",
       "907        51482.38                 2492881.0        3336.35       558848.89   \n",
       "\n",
       "     Number_of_trades_ETHUSDT  Close_BNBUSDT  Volume_BNBUSDT  \\\n",
       "903                 1906387.0          555.4      2284301.81   \n",
       "904                 2647385.0          507.7      2551361.51   \n",
       "905                 2987953.0          556.8      1425296.58   \n",
       "906                 1791989.0          553.8       953921.37   \n",
       "907                 1747756.0          553.8      1181298.51   \n",
       "\n",
       "     Number_of_trades_BNBUSDT  SMA_20  EMA_20  Upper_Band  Middle_Band  \\\n",
       "903                  994512.0   10.06    9.95       11.86        10.06   \n",
       "904                 1213572.0   10.08    9.84       11.81        10.08   \n",
       "905                  809335.0   10.14    9.80       11.68        10.14   \n",
       "906                  563996.0   10.17    9.74       11.63        10.17   \n",
       "907                  712381.0   10.14    9.67       11.67        10.14   \n",
       "\n",
       "     Lower_Band    RSI  buy_1000x_high_coinbase  sell_1000x_high_coinbase  \\\n",
       "903        8.26  52.48                     34.0                      43.0   \n",
       "904        8.35  42.93                    120.0                     126.0   \n",
       "905        8.60  49.21                    185.0                     117.0   \n",
       "906        8.71  46.85                     64.0                      81.0   \n",
       "907        8.62  45.00                     57.0                      66.0   \n",
       "\n",
       "     total_trades_coinbase  Tweets_Utilizados  Tweets_Utilizados_coin  \\\n",
       "903                84706.0                696                     471   \n",
       "904               135180.0                961                     509   \n",
       "905               112997.0                866                     555   \n",
       "906                66543.0                692                     533   \n",
       "907                68616.0                681                     546   \n",
       "\n",
       "     Tweets_Utilizados_referentes  Tweets_Utilizados_whale_alert  \\\n",
       "903                           0.0                           43.0   \n",
       "904                           1.0                           56.0   \n",
       "905                           1.0                           40.0   \n",
       "906                           0.0                           24.0   \n",
       "907                           0.0                           41.0   \n",
       "\n",
       "     Buy_1000x_high  sell_1000x_high  total_trades_binance  \n",
       "903           343.0            228.0              154000.0  \n",
       "904           534.0            433.0              221000.0  \n",
       "905           473.0            386.0              171000.0  \n",
       "906           350.0            290.0              101000.0  \n",
       "907           252.0            206.0               92000.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(5, 30)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(feature_dataset.head())\n",
    "print(feature_dataset.shape)\n",
    "\n",
    "display(validation.head())\n",
    "display(validation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers = {}\n",
    "for col in feature_dataset.columns:\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    feature_dataset[col] = scaler.fit_transform(np.array(feature_dataset[col]).reshape(-1, 1))\n",
    "    scalers[col] = scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Number of trades</th>\n",
       "      <th>Close_BTCUSDT</th>\n",
       "      <th>Volume_BTCUSDT</th>\n",
       "      <th>Number_of_trades_BTCUSDT</th>\n",
       "      <th>Close_ETHUSDT</th>\n",
       "      <th>Volume_ETHUSDT</th>\n",
       "      <th>Number_of_trades_ETHUSDT</th>\n",
       "      <th>Close_BNBUSDT</th>\n",
       "      <th>Volume_BNBUSDT</th>\n",
       "      <th>Number_of_trades_BNBUSDT</th>\n",
       "      <th>SMA_20</th>\n",
       "      <th>EMA_20</th>\n",
       "      <th>Upper_Band</th>\n",
       "      <th>Middle_Band</th>\n",
       "      <th>Lower_Band</th>\n",
       "      <th>RSI</th>\n",
       "      <th>buy_1000x_high_coinbase</th>\n",
       "      <th>sell_1000x_high_coinbase</th>\n",
       "      <th>total_trades_coinbase</th>\n",
       "      <th>Tweets_Utilizados</th>\n",
       "      <th>Tweets_Utilizados_coin</th>\n",
       "      <th>Tweets_Utilizados_referentes</th>\n",
       "      <th>Tweets_Utilizados_whale_alert</th>\n",
       "      <th>Buy_1000x_high</th>\n",
       "      <th>sell_1000x_high</th>\n",
       "      <th>total_trades_binance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.501993</td>\n",
       "      <td>0.517208</td>\n",
       "      <td>0.492593</td>\n",
       "      <td>0.479569</td>\n",
       "      <td>0.168108</td>\n",
       "      <td>0.460212</td>\n",
       "      <td>0.040979</td>\n",
       "      <td>0.046312</td>\n",
       "      <td>0.506304</td>\n",
       "      <td>0.126579</td>\n",
       "      <td>0.285966</td>\n",
       "      <td>0.303395</td>\n",
       "      <td>0.197596</td>\n",
       "      <td>0.238897</td>\n",
       "      <td>0.633771</td>\n",
       "      <td>0.615034</td>\n",
       "      <td>0.648008</td>\n",
       "      <td>0.633771</td>\n",
       "      <td>0.602961</td>\n",
       "      <td>0.345342</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.123457</td>\n",
       "      <td>0.136662</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.536424</td>\n",
       "      <td>0.302018</td>\n",
       "      <td>0.212508</td>\n",
       "      <td>0.140182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.479673</td>\n",
       "      <td>0.480653</td>\n",
       "      <td>0.464609</td>\n",
       "      <td>0.451664</td>\n",
       "      <td>0.134757</td>\n",
       "      <td>0.440649</td>\n",
       "      <td>0.045762</td>\n",
       "      <td>0.049256</td>\n",
       "      <td>0.474653</td>\n",
       "      <td>0.126913</td>\n",
       "      <td>0.248880</td>\n",
       "      <td>0.297919</td>\n",
       "      <td>0.189790</td>\n",
       "      <td>0.215055</td>\n",
       "      <td>0.631960</td>\n",
       "      <td>0.605239</td>\n",
       "      <td>0.650664</td>\n",
       "      <td>0.631960</td>\n",
       "      <td>0.595284</td>\n",
       "      <td>0.307138</td>\n",
       "      <td>0.016878</td>\n",
       "      <td>0.007092</td>\n",
       "      <td>0.026035</td>\n",
       "      <td>0.079772</td>\n",
       "      <td>0.119580</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.576159</td>\n",
       "      <td>0.268600</td>\n",
       "      <td>0.191449</td>\n",
       "      <td>0.104869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.451574</td>\n",
       "      <td>0.484737</td>\n",
       "      <td>0.463992</td>\n",
       "      <td>0.470999</td>\n",
       "      <td>0.140104</td>\n",
       "      <td>0.449336</td>\n",
       "      <td>0.032645</td>\n",
       "      <td>0.038494</td>\n",
       "      <td>0.486597</td>\n",
       "      <td>0.098880</td>\n",
       "      <td>0.191129</td>\n",
       "      <td>0.373932</td>\n",
       "      <td>0.365156</td>\n",
       "      <td>0.355091</td>\n",
       "      <td>0.629244</td>\n",
       "      <td>0.598405</td>\n",
       "      <td>0.652751</td>\n",
       "      <td>0.629244</td>\n",
       "      <td>0.585138</td>\n",
       "      <td>0.343331</td>\n",
       "      <td>0.092827</td>\n",
       "      <td>0.141844</td>\n",
       "      <td>0.267329</td>\n",
       "      <td>0.097816</td>\n",
       "      <td>0.149803</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.423841</td>\n",
       "      <td>0.345523</td>\n",
       "      <td>0.296107</td>\n",
       "      <td>0.111289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.470905</td>\n",
       "      <td>0.492514</td>\n",
       "      <td>0.484979</td>\n",
       "      <td>0.497708</td>\n",
       "      <td>0.126093</td>\n",
       "      <td>0.489479</td>\n",
       "      <td>0.049765</td>\n",
       "      <td>0.059477</td>\n",
       "      <td>0.525979</td>\n",
       "      <td>0.125672</td>\n",
       "      <td>0.269369</td>\n",
       "      <td>0.417306</td>\n",
       "      <td>0.244504</td>\n",
       "      <td>0.294707</td>\n",
       "      <td>0.628565</td>\n",
       "      <td>0.595216</td>\n",
       "      <td>0.652941</td>\n",
       "      <td>0.628565</td>\n",
       "      <td>0.583219</td>\n",
       "      <td>0.392426</td>\n",
       "      <td>0.063291</td>\n",
       "      <td>0.081560</td>\n",
       "      <td>0.193644</td>\n",
       "      <td>0.101614</td>\n",
       "      <td>0.152431</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.509934</td>\n",
       "      <td>0.235183</td>\n",
       "      <td>0.276324</td>\n",
       "      <td>0.103799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.497409</td>\n",
       "      <td>0.557457</td>\n",
       "      <td>0.513169</td>\n",
       "      <td>0.563883</td>\n",
       "      <td>0.277858</td>\n",
       "      <td>0.564840</td>\n",
       "      <td>0.076188</td>\n",
       "      <td>0.097929</td>\n",
       "      <td>0.607100</td>\n",
       "      <td>0.165421</td>\n",
       "      <td>0.415326</td>\n",
       "      <td>0.491785</td>\n",
       "      <td>0.307160</td>\n",
       "      <td>0.399500</td>\n",
       "      <td>0.629018</td>\n",
       "      <td>0.599544</td>\n",
       "      <td>0.653321</td>\n",
       "      <td>0.629018</td>\n",
       "      <td>0.583767</td>\n",
       "      <td>0.501340</td>\n",
       "      <td>0.101266</td>\n",
       "      <td>0.124113</td>\n",
       "      <td>0.311958</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.144547</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.470199</td>\n",
       "      <td>0.271753</td>\n",
       "      <td>0.403957</td>\n",
       "      <td>0.258962</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Open      High       Low     Close  Number of trades  Close_BTCUSDT  \\\n",
       "0  0.501993  0.517208  0.492593  0.479569          0.168108       0.460212   \n",
       "1  0.479673  0.480653  0.464609  0.451664          0.134757       0.440649   \n",
       "2  0.451574  0.484737  0.463992  0.470999          0.140104       0.449336   \n",
       "3  0.470905  0.492514  0.484979  0.497708          0.126093       0.489479   \n",
       "4  0.497409  0.557457  0.513169  0.563883          0.277858       0.564840   \n",
       "\n",
       "   Volume_BTCUSDT  Number_of_trades_BTCUSDT  Close_ETHUSDT  Volume_ETHUSDT  \\\n",
       "0        0.040979                  0.046312       0.506304        0.126579   \n",
       "1        0.045762                  0.049256       0.474653        0.126913   \n",
       "2        0.032645                  0.038494       0.486597        0.098880   \n",
       "3        0.049765                  0.059477       0.525979        0.125672   \n",
       "4        0.076188                  0.097929       0.607100        0.165421   \n",
       "\n",
       "   Number_of_trades_ETHUSDT  Close_BNBUSDT  Volume_BNBUSDT  \\\n",
       "0                  0.285966       0.303395        0.197596   \n",
       "1                  0.248880       0.297919        0.189790   \n",
       "2                  0.191129       0.373932        0.365156   \n",
       "3                  0.269369       0.417306        0.244504   \n",
       "4                  0.415326       0.491785        0.307160   \n",
       "\n",
       "   Number_of_trades_BNBUSDT    SMA_20    EMA_20  Upper_Band  Middle_Band  \\\n",
       "0                  0.238897  0.633771  0.615034    0.648008     0.633771   \n",
       "1                  0.215055  0.631960  0.605239    0.650664     0.631960   \n",
       "2                  0.355091  0.629244  0.598405    0.652751     0.629244   \n",
       "3                  0.294707  0.628565  0.595216    0.652941     0.628565   \n",
       "4                  0.399500  0.629018  0.599544    0.653321     0.629018   \n",
       "\n",
       "   Lower_Band       RSI  buy_1000x_high_coinbase  sell_1000x_high_coinbase  \\\n",
       "0    0.602961  0.345342                 0.000000                  0.000000   \n",
       "1    0.595284  0.307138                 0.016878                  0.007092   \n",
       "2    0.585138  0.343331                 0.092827                  0.141844   \n",
       "3    0.583219  0.392426                 0.063291                  0.081560   \n",
       "4    0.583767  0.501340                 0.101266                  0.124113   \n",
       "\n",
       "   total_trades_coinbase  Tweets_Utilizados  Tweets_Utilizados_coin  \\\n",
       "0               0.000000           0.123457                0.136662   \n",
       "1               0.026035           0.079772                0.119580   \n",
       "2               0.267329           0.097816                0.149803   \n",
       "3               0.193644           0.101614                0.152431   \n",
       "4               0.311958           0.153846                0.144547   \n",
       "\n",
       "   Tweets_Utilizados_referentes  Tweets_Utilizados_whale_alert  \\\n",
       "0                      0.058824                       0.536424   \n",
       "1                      0.117647                       0.576159   \n",
       "2                      0.000000                       0.423841   \n",
       "3                      0.117647                       0.509934   \n",
       "4                      0.058824                       0.470199   \n",
       "\n",
       "   Buy_1000x_high  sell_1000x_high  total_trades_binance  \n",
       "0        0.302018         0.212508              0.140182  \n",
       "1        0.268600         0.191449              0.104869  \n",
       "2        0.345523         0.296107              0.111289  \n",
       "3        0.235183         0.276324              0.103799  \n",
       "4        0.271753         0.403957              0.258962  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(903, 30)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Number of trades</th>\n",
       "      <th>Close_BTCUSDT</th>\n",
       "      <th>Volume_BTCUSDT</th>\n",
       "      <th>Number_of_trades_BTCUSDT</th>\n",
       "      <th>Close_ETHUSDT</th>\n",
       "      <th>Volume_ETHUSDT</th>\n",
       "      <th>Number_of_trades_ETHUSDT</th>\n",
       "      <th>Close_BNBUSDT</th>\n",
       "      <th>Volume_BNBUSDT</th>\n",
       "      <th>Number_of_trades_BNBUSDT</th>\n",
       "      <th>SMA_20</th>\n",
       "      <th>EMA_20</th>\n",
       "      <th>Upper_Band</th>\n",
       "      <th>Middle_Band</th>\n",
       "      <th>Lower_Band</th>\n",
       "      <th>RSI</th>\n",
       "      <th>buy_1000x_high_coinbase</th>\n",
       "      <th>sell_1000x_high_coinbase</th>\n",
       "      <th>total_trades_coinbase</th>\n",
       "      <th>Tweets_Utilizados</th>\n",
       "      <th>Tweets_Utilizados_coin</th>\n",
       "      <th>Tweets_Utilizados_referentes</th>\n",
       "      <th>Tweets_Utilizados_whale_alert</th>\n",
       "      <th>Buy_1000x_high</th>\n",
       "      <th>sell_1000x_high</th>\n",
       "      <th>total_trades_binance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>903</th>\n",
       "      <td>10.08</td>\n",
       "      <td>10.46</td>\n",
       "      <td>9.60</td>\n",
       "      <td>9.90</td>\n",
       "      <td>245319.0</td>\n",
       "      <td>67609.99</td>\n",
       "      <td>55691.08</td>\n",
       "      <td>2464515.0</td>\n",
       "      <td>3520.46</td>\n",
       "      <td>570901.29</td>\n",
       "      <td>1906387.0</td>\n",
       "      <td>555.4</td>\n",
       "      <td>2284301.81</td>\n",
       "      <td>994512.0</td>\n",
       "      <td>10.06</td>\n",
       "      <td>9.95</td>\n",
       "      <td>11.86</td>\n",
       "      <td>10.06</td>\n",
       "      <td>8.26</td>\n",
       "      <td>52.48</td>\n",
       "      <td>34.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>84706.0</td>\n",
       "      <td>696</td>\n",
       "      <td>471</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>154000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>9.90</td>\n",
       "      <td>9.99</td>\n",
       "      <td>8.60</td>\n",
       "      <td>8.77</td>\n",
       "      <td>341363.0</td>\n",
       "      <td>61937.40</td>\n",
       "      <td>101005.32</td>\n",
       "      <td>3593832.0</td>\n",
       "      <td>3158.64</td>\n",
       "      <td>1049629.69</td>\n",
       "      <td>2647385.0</td>\n",
       "      <td>507.7</td>\n",
       "      <td>2551361.51</td>\n",
       "      <td>1213572.0</td>\n",
       "      <td>10.08</td>\n",
       "      <td>9.84</td>\n",
       "      <td>11.81</td>\n",
       "      <td>10.08</td>\n",
       "      <td>8.35</td>\n",
       "      <td>42.93</td>\n",
       "      <td>120.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>135180.0</td>\n",
       "      <td>961</td>\n",
       "      <td>509</td>\n",
       "      <td>1.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>534.0</td>\n",
       "      <td>433.0</td>\n",
       "      <td>221000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>8.77</td>\n",
       "      <td>9.57</td>\n",
       "      <td>8.49</td>\n",
       "      <td>9.48</td>\n",
       "      <td>267797.0</td>\n",
       "      <td>67840.51</td>\n",
       "      <td>90420.59</td>\n",
       "      <td>3549793.0</td>\n",
       "      <td>3516.53</td>\n",
       "      <td>1207322.82</td>\n",
       "      <td>2987953.0</td>\n",
       "      <td>556.8</td>\n",
       "      <td>1425296.58</td>\n",
       "      <td>809335.0</td>\n",
       "      <td>10.14</td>\n",
       "      <td>9.80</td>\n",
       "      <td>11.68</td>\n",
       "      <td>10.14</td>\n",
       "      <td>8.60</td>\n",
       "      <td>49.21</td>\n",
       "      <td>185.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>112997.0</td>\n",
       "      <td>866</td>\n",
       "      <td>555</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>473.0</td>\n",
       "      <td>386.0</td>\n",
       "      <td>171000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>9.48</td>\n",
       "      <td>9.58</td>\n",
       "      <td>9.07</td>\n",
       "      <td>9.18</td>\n",
       "      <td>156774.0</td>\n",
       "      <td>65501.27</td>\n",
       "      <td>53357.48</td>\n",
       "      <td>2388390.0</td>\n",
       "      <td>3492.85</td>\n",
       "      <td>602755.21</td>\n",
       "      <td>1791989.0</td>\n",
       "      <td>553.8</td>\n",
       "      <td>953921.37</td>\n",
       "      <td>563996.0</td>\n",
       "      <td>10.17</td>\n",
       "      <td>9.74</td>\n",
       "      <td>11.63</td>\n",
       "      <td>10.17</td>\n",
       "      <td>8.71</td>\n",
       "      <td>46.85</td>\n",
       "      <td>64.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>66543.0</td>\n",
       "      <td>692</td>\n",
       "      <td>533</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>101000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>9.18</td>\n",
       "      <td>9.37</td>\n",
       "      <td>8.69</td>\n",
       "      <td>8.94</td>\n",
       "      <td>147578.0</td>\n",
       "      <td>63796.64</td>\n",
       "      <td>51482.38</td>\n",
       "      <td>2492881.0</td>\n",
       "      <td>3336.35</td>\n",
       "      <td>558848.89</td>\n",
       "      <td>1747756.0</td>\n",
       "      <td>553.8</td>\n",
       "      <td>1181298.51</td>\n",
       "      <td>712381.0</td>\n",
       "      <td>10.14</td>\n",
       "      <td>9.67</td>\n",
       "      <td>11.67</td>\n",
       "      <td>10.14</td>\n",
       "      <td>8.62</td>\n",
       "      <td>45.00</td>\n",
       "      <td>57.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>68616.0</td>\n",
       "      <td>681</td>\n",
       "      <td>546</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>92000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Open   High   Low  Close  Number of trades  Close_BTCUSDT  \\\n",
       "903  10.08  10.46  9.60   9.90          245319.0       67609.99   \n",
       "904   9.90   9.99  8.60   8.77          341363.0       61937.40   \n",
       "905   8.77   9.57  8.49   9.48          267797.0       67840.51   \n",
       "906   9.48   9.58  9.07   9.18          156774.0       65501.27   \n",
       "907   9.18   9.37  8.69   8.94          147578.0       63796.64   \n",
       "\n",
       "     Volume_BTCUSDT  Number_of_trades_BTCUSDT  Close_ETHUSDT  Volume_ETHUSDT  \\\n",
       "903        55691.08                 2464515.0        3520.46       570901.29   \n",
       "904       101005.32                 3593832.0        3158.64      1049629.69   \n",
       "905        90420.59                 3549793.0        3516.53      1207322.82   \n",
       "906        53357.48                 2388390.0        3492.85       602755.21   \n",
       "907        51482.38                 2492881.0        3336.35       558848.89   \n",
       "\n",
       "     Number_of_trades_ETHUSDT  Close_BNBUSDT  Volume_BNBUSDT  \\\n",
       "903                 1906387.0          555.4      2284301.81   \n",
       "904                 2647385.0          507.7      2551361.51   \n",
       "905                 2987953.0          556.8      1425296.58   \n",
       "906                 1791989.0          553.8       953921.37   \n",
       "907                 1747756.0          553.8      1181298.51   \n",
       "\n",
       "     Number_of_trades_BNBUSDT  SMA_20  EMA_20  Upper_Band  Middle_Band  \\\n",
       "903                  994512.0   10.06    9.95       11.86        10.06   \n",
       "904                 1213572.0   10.08    9.84       11.81        10.08   \n",
       "905                  809335.0   10.14    9.80       11.68        10.14   \n",
       "906                  563996.0   10.17    9.74       11.63        10.17   \n",
       "907                  712381.0   10.14    9.67       11.67        10.14   \n",
       "\n",
       "     Lower_Band    RSI  buy_1000x_high_coinbase  sell_1000x_high_coinbase  \\\n",
       "903        8.26  52.48                     34.0                      43.0   \n",
       "904        8.35  42.93                    120.0                     126.0   \n",
       "905        8.60  49.21                    185.0                     117.0   \n",
       "906        8.71  46.85                     64.0                      81.0   \n",
       "907        8.62  45.00                     57.0                      66.0   \n",
       "\n",
       "     total_trades_coinbase  Tweets_Utilizados  Tweets_Utilizados_coin  \\\n",
       "903                84706.0                696                     471   \n",
       "904               135180.0                961                     509   \n",
       "905               112997.0                866                     555   \n",
       "906                66543.0                692                     533   \n",
       "907                68616.0                681                     546   \n",
       "\n",
       "     Tweets_Utilizados_referentes  Tweets_Utilizados_whale_alert  \\\n",
       "903                           0.0                           43.0   \n",
       "904                           1.0                           56.0   \n",
       "905                           1.0                           40.0   \n",
       "906                           0.0                           24.0   \n",
       "907                           0.0                           41.0   \n",
       "\n",
       "     Buy_1000x_high  sell_1000x_high  total_trades_binance  \n",
       "903           343.0            228.0              154000.0  \n",
       "904           534.0            433.0              221000.0  \n",
       "905           473.0            386.0              171000.0  \n",
       "906           350.0            290.0              101000.0  \n",
       "907           252.0            206.0               92000.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(5, 30)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(feature_dataset.head())\n",
    "print(feature_dataset.shape)\n",
    "\n",
    "display(validation.head())\n",
    "display(validation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparo el dataset para train: cada conjunto de entrenamiento, sera una seried de N dias previos, para predecir 1 dia siguiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, n_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - n_steps):\n",
    "        end_ix = i + n_steps\n",
    "        seq_x = data.iloc[i:end_ix, :].values\n",
    "        seq_y = data.iloc[end_ix, :].values\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "n_steps = 30  # Longitud de la secuencia de entrada\n",
    "n_features = feature_dataset.shape[1]  # Nmero de caractersticas\n",
    "\n",
    "# Crear las secuencias de entrada y salida\n",
    "X, y = create_sequences(feature_dataset, n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 30)\n",
      "(30,)\n",
      "(873, 30, 30)\n",
      "(873, 30)\n",
      "(903, 30)\n"
     ]
    }
   ],
   "source": [
    "print(X[0].shape) # Cada dato de entrenamiento, es un conjunto de 30 dias con sus 64 features\n",
    "print(y[0].shape) # El target de cada dato, son los 64 features del dia siguiente\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(feature_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "873\n",
      "873\n"
     ]
    }
   ],
   "source": [
    "print(len(X))\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtencion de los mejores hiperparametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_scoring(estimator, X, y):\n",
    "    y_pred = estimator.predict(X)\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    return -mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_scoring_validation(y, y_pred):\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    return -mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vmse(y_true, y_pred):\n",
    "    return K.mean(K.square(y_true - y_pred), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb33aa26430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb2fa118e50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def create_model(activation, units, dropout, learning_rate, l2_penalty, depth, optimizer='adam'):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=int(units/2), activation=activation, input_shape=(n_steps, n_features), return_sequences=True, kernel_regularizer=l2(l2_penalty)))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    for _ in range(depth - 1):\n",
    "        model.add(LSTM(units=units, activation=activation, return_sequences=True, kernel_regularizer=l2(l2_penalty)),)\n",
    "        model.add(Dropout(dropout))\n",
    "        model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(LSTM(units=int(units*2), activation=activation, kernel_regularizer=l2(l2_penalty)))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(units=n_features))\n",
    "    \n",
    "    if optimizer == 'adam':\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == 'rmsprop':\n",
    "        optimizer = RMSprop(learning_rate=learning_rate)\n",
    "    elif optimizer == 'sgd':\n",
    "        optimizer = SGD(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=vmse, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "regressor = KerasRegressor(build_fn=create_model, verbose=0, activation='relu', units=50, dropout=0.2, learning_rate=0.1, l2_penalty=0.001, depth=2, optimizer='adam')\n",
    "\n",
    "cv = TimeSeriesSplit(n_splits=20).split(X)\n",
    "param_space = {\n",
    "    'depth': [2, 3, 4, 5],\n",
    "    'activation': ['relu', 'tanh', 'swish', 'selu'],\n",
    "    'units': [64, 128, 256, 512],\n",
    "    'dropout': [0.1, 0.2, 0.3, 0.4],\n",
    "    'learning_rate': [0.01, 0.001, 0.0001],\n",
    "    'epochs': [10, 20, 30, 50, 100],\n",
    "    'batch_size': [32, 64, 128],\n",
    "    'optimizer': ['adam', 'rmsprop', 'sgd'],\n",
    "    'l2_penalty': [0.001, 0.01, 0.1]\n",
    "}\n",
    "\n",
    "bayes_search = BayesSearchCV(regressor, param_space, scoring=custom_scoring, cv=cv, verbose=0)#10)\n",
    "bayes_result = bayes_search.fit(X, y, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: -0.01407215163397417\n",
      "Best parameters: OrderedDict([('activation', 'swish'), ('batch_size', 32), ('depth', 2), ('dropout', 0.3), ('epochs', 100), ('l2_penalty', 0.01), ('learning_rate', 0.0001), ('optimizer', 'adam'), ('units', 512)])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KerasRegressor(\n",
       "\tmodel=None\n",
       "\tbuild_fn=&lt;function create_model at 0x7fc52ab9b820&gt;\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=adam\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=32\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=0\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=100\n",
       "\tactivation=swish\n",
       "\tunits=512\n",
       "\tdropout=0.3\n",
       "\tlearning_rate=0.0001\n",
       "\tl2_penalty=0.01\n",
       "\tdepth=2\n",
       ")</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KerasRegressor</label><div class=\"sk-toggleable__content\"><pre>KerasRegressor(\n",
       "\tmodel=None\n",
       "\tbuild_fn=&lt;function create_model at 0x7fc52ab9b820&gt;\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=adam\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=32\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=0\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=100\n",
       "\tactivation=swish\n",
       "\tunits=512\n",
       "\tdropout=0.3\n",
       "\tlearning_rate=0.0001\n",
       "\tl2_penalty=0.01\n",
       "\tdepth=2\n",
       ")</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KerasRegressor(\n",
       "\tmodel=None\n",
       "\tbuild_fn=<function create_model at 0x7fc52ab9b820>\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=adam\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=32\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=0\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=100\n",
       "\tactivation=swish\n",
       "\tunits=512\n",
       "\tdropout=0.3\n",
       "\tlearning_rate=0.0001\n",
       "\tl2_penalty=0.01\n",
       "\tdepth=2\n",
       ")"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show best results\n",
    "print(\"Best score:\", bayes_result.best_score_)\n",
    "print(\"Best parameters:\", bayes_result.best_params_)\n",
    "\n",
    "# Entrenar el modelo con los mejores hiperparmetros\n",
    "best_model = bayes_result.best_estimator_\n",
    "best_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicciones con el mejor conjunto de hiper parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores predichos para los prximos 5 das:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Number of trades</th>\n",
       "      <th>Close_BTCUSDT</th>\n",
       "      <th>Volume_BTCUSDT</th>\n",
       "      <th>Number_of_trades_BTCUSDT</th>\n",
       "      <th>Close_ETHUSDT</th>\n",
       "      <th>Volume_ETHUSDT</th>\n",
       "      <th>Number_of_trades_ETHUSDT</th>\n",
       "      <th>Close_BNBUSDT</th>\n",
       "      <th>Volume_BNBUSDT</th>\n",
       "      <th>Number_of_trades_BNBUSDT</th>\n",
       "      <th>SMA_20</th>\n",
       "      <th>EMA_20</th>\n",
       "      <th>Upper_Band</th>\n",
       "      <th>Middle_Band</th>\n",
       "      <th>Lower_Band</th>\n",
       "      <th>RSI</th>\n",
       "      <th>buy_1000x_high_coinbase</th>\n",
       "      <th>sell_1000x_high_coinbase</th>\n",
       "      <th>total_trades_coinbase</th>\n",
       "      <th>Tweets_Utilizados</th>\n",
       "      <th>Tweets_Utilizados_coin</th>\n",
       "      <th>Tweets_Utilizados_referentes</th>\n",
       "      <th>Tweets_Utilizados_whale_alert</th>\n",
       "      <th>Buy_1000x_high</th>\n",
       "      <th>sell_1000x_high</th>\n",
       "      <th>total_trades_binance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>0.120167</td>\n",
       "      <td>0.128135</td>\n",
       "      <td>0.115844</td>\n",
       "      <td>0.128164</td>\n",
       "      <td>0.083843</td>\n",
       "      <td>0.918331</td>\n",
       "      <td>0.054236</td>\n",
       "      <td>0.143388</td>\n",
       "      <td>0.694908</td>\n",
       "      <td>0.128726</td>\n",
       "      <td>0.503699</td>\n",
       "      <td>0.820811</td>\n",
       "      <td>0.368893</td>\n",
       "      <td>0.443672</td>\n",
       "      <td>0.138751</td>\n",
       "      <td>0.139408</td>\n",
       "      <td>0.147628</td>\n",
       "      <td>0.138751</td>\n",
       "      <td>0.127228</td>\n",
       "      <td>0.515583</td>\n",
       "      <td>0.151899</td>\n",
       "      <td>0.170213</td>\n",
       "      <td>0.352479</td>\n",
       "      <td>0.649573</td>\n",
       "      <td>0.501971</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.139073</td>\n",
       "      <td>0.156999</td>\n",
       "      <td>0.156988</td>\n",
       "      <td>0.075976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>903</th>\n",
       "      <td>0.145938</td>\n",
       "      <td>0.139936</td>\n",
       "      <td>0.123826</td>\n",
       "      <td>0.127002</td>\n",
       "      <td>0.117857</td>\n",
       "      <td>0.944108</td>\n",
       "      <td>0.043603</td>\n",
       "      <td>0.136416</td>\n",
       "      <td>0.766444</td>\n",
       "      <td>0.151164</td>\n",
       "      <td>0.521597</td>\n",
       "      <td>0.749611</td>\n",
       "      <td>0.388442</td>\n",
       "      <td>0.459132</td>\n",
       "      <td>0.149115</td>\n",
       "      <td>0.148083</td>\n",
       "      <td>0.173791</td>\n",
       "      <td>0.151488</td>\n",
       "      <td>0.129398</td>\n",
       "      <td>0.660461</td>\n",
       "      <td>0.225255</td>\n",
       "      <td>0.203273</td>\n",
       "      <td>0.465957</td>\n",
       "      <td>0.309386</td>\n",
       "      <td>0.220505</td>\n",
       "      <td>-0.005968</td>\n",
       "      <td>0.299016</td>\n",
       "      <td>0.259091</td>\n",
       "      <td>0.210517</td>\n",
       "      <td>0.099516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>0.142274</td>\n",
       "      <td>0.135385</td>\n",
       "      <td>0.119532</td>\n",
       "      <td>0.122008</td>\n",
       "      <td>0.114829</td>\n",
       "      <td>0.943397</td>\n",
       "      <td>0.039673</td>\n",
       "      <td>0.131208</td>\n",
       "      <td>0.766763</td>\n",
       "      <td>0.151632</td>\n",
       "      <td>0.524292</td>\n",
       "      <td>0.761903</td>\n",
       "      <td>0.390306</td>\n",
       "      <td>0.461762</td>\n",
       "      <td>0.149537</td>\n",
       "      <td>0.147666</td>\n",
       "      <td>0.175026</td>\n",
       "      <td>0.151434</td>\n",
       "      <td>0.128042</td>\n",
       "      <td>0.633117</td>\n",
       "      <td>0.215688</td>\n",
       "      <td>0.195648</td>\n",
       "      <td>0.464704</td>\n",
       "      <td>0.312922</td>\n",
       "      <td>0.218579</td>\n",
       "      <td>-0.007245</td>\n",
       "      <td>0.302250</td>\n",
       "      <td>0.250347</td>\n",
       "      <td>0.200213</td>\n",
       "      <td>0.097973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>0.138880</td>\n",
       "      <td>0.131378</td>\n",
       "      <td>0.115788</td>\n",
       "      <td>0.117666</td>\n",
       "      <td>0.111902</td>\n",
       "      <td>0.942414</td>\n",
       "      <td>0.035301</td>\n",
       "      <td>0.125308</td>\n",
       "      <td>0.766938</td>\n",
       "      <td>0.151267</td>\n",
       "      <td>0.524935</td>\n",
       "      <td>0.770263</td>\n",
       "      <td>0.390368</td>\n",
       "      <td>0.461705</td>\n",
       "      <td>0.149832</td>\n",
       "      <td>0.147278</td>\n",
       "      <td>0.175866</td>\n",
       "      <td>0.151353</td>\n",
       "      <td>0.126835</td>\n",
       "      <td>0.609268</td>\n",
       "      <td>0.207779</td>\n",
       "      <td>0.189225</td>\n",
       "      <td>0.462304</td>\n",
       "      <td>0.315093</td>\n",
       "      <td>0.216597</td>\n",
       "      <td>-0.008629</td>\n",
       "      <td>0.304361</td>\n",
       "      <td>0.242077</td>\n",
       "      <td>0.191046</td>\n",
       "      <td>0.096684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>0.135788</td>\n",
       "      <td>0.127934</td>\n",
       "      <td>0.112568</td>\n",
       "      <td>0.113988</td>\n",
       "      <td>0.109203</td>\n",
       "      <td>0.941610</td>\n",
       "      <td>0.030576</td>\n",
       "      <td>0.118943</td>\n",
       "      <td>0.767371</td>\n",
       "      <td>0.150255</td>\n",
       "      <td>0.524053</td>\n",
       "      <td>0.775542</td>\n",
       "      <td>0.389196</td>\n",
       "      <td>0.459778</td>\n",
       "      <td>0.150015</td>\n",
       "      <td>0.146947</td>\n",
       "      <td>0.176422</td>\n",
       "      <td>0.151271</td>\n",
       "      <td>0.125764</td>\n",
       "      <td>0.589175</td>\n",
       "      <td>0.201520</td>\n",
       "      <td>0.184046</td>\n",
       "      <td>0.459296</td>\n",
       "      <td>0.316223</td>\n",
       "      <td>0.214703</td>\n",
       "      <td>-0.010144</td>\n",
       "      <td>0.305641</td>\n",
       "      <td>0.234364</td>\n",
       "      <td>0.183017</td>\n",
       "      <td>0.095675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>0.132907</td>\n",
       "      <td>0.124917</td>\n",
       "      <td>0.109720</td>\n",
       "      <td>0.110816</td>\n",
       "      <td>0.106741</td>\n",
       "      <td>0.941060</td>\n",
       "      <td>0.025625</td>\n",
       "      <td>0.112310</td>\n",
       "      <td>0.768111</td>\n",
       "      <td>0.148784</td>\n",
       "      <td>0.522095</td>\n",
       "      <td>0.778544</td>\n",
       "      <td>0.387236</td>\n",
       "      <td>0.456617</td>\n",
       "      <td>0.150036</td>\n",
       "      <td>0.146608</td>\n",
       "      <td>0.176719</td>\n",
       "      <td>0.151124</td>\n",
       "      <td>0.124728</td>\n",
       "      <td>0.572261</td>\n",
       "      <td>0.196570</td>\n",
       "      <td>0.179881</td>\n",
       "      <td>0.456013</td>\n",
       "      <td>0.316606</td>\n",
       "      <td>0.212948</td>\n",
       "      <td>-0.011763</td>\n",
       "      <td>0.306304</td>\n",
       "      <td>0.227222</td>\n",
       "      <td>0.176012</td>\n",
       "      <td>0.094896</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Open      High       Low     Close  Number of trades  Close_BTCUSDT  \\\n",
       "902  0.120167  0.128135  0.115844  0.128164          0.083843       0.918331   \n",
       "903  0.145938  0.139936  0.123826  0.127002          0.117857       0.944108   \n",
       "904  0.142274  0.135385  0.119532  0.122008          0.114829       0.943397   \n",
       "905  0.138880  0.131378  0.115788  0.117666          0.111902       0.942414   \n",
       "906  0.135788  0.127934  0.112568  0.113988          0.109203       0.941610   \n",
       "907  0.132907  0.124917  0.109720  0.110816          0.106741       0.941060   \n",
       "\n",
       "     Volume_BTCUSDT  Number_of_trades_BTCUSDT  Close_ETHUSDT  Volume_ETHUSDT  \\\n",
       "902        0.054236                  0.143388       0.694908        0.128726   \n",
       "903        0.043603                  0.136416       0.766444        0.151164   \n",
       "904        0.039673                  0.131208       0.766763        0.151632   \n",
       "905        0.035301                  0.125308       0.766938        0.151267   \n",
       "906        0.030576                  0.118943       0.767371        0.150255   \n",
       "907        0.025625                  0.112310       0.768111        0.148784   \n",
       "\n",
       "     Number_of_trades_ETHUSDT  Close_BNBUSDT  Volume_BNBUSDT  \\\n",
       "902                  0.503699       0.820811        0.368893   \n",
       "903                  0.521597       0.749611        0.388442   \n",
       "904                  0.524292       0.761903        0.390306   \n",
       "905                  0.524935       0.770263        0.390368   \n",
       "906                  0.524053       0.775542        0.389196   \n",
       "907                  0.522095       0.778544        0.387236   \n",
       "\n",
       "     Number_of_trades_BNBUSDT    SMA_20    EMA_20  Upper_Band  Middle_Band  \\\n",
       "902                  0.443672  0.138751  0.139408    0.147628     0.138751   \n",
       "903                  0.459132  0.149115  0.148083    0.173791     0.151488   \n",
       "904                  0.461762  0.149537  0.147666    0.175026     0.151434   \n",
       "905                  0.461705  0.149832  0.147278    0.175866     0.151353   \n",
       "906                  0.459778  0.150015  0.146947    0.176422     0.151271   \n",
       "907                  0.456617  0.150036  0.146608    0.176719     0.151124   \n",
       "\n",
       "     Lower_Band       RSI  buy_1000x_high_coinbase  sell_1000x_high_coinbase  \\\n",
       "902    0.127228  0.515583                 0.151899                  0.170213   \n",
       "903    0.129398  0.660461                 0.225255                  0.203273   \n",
       "904    0.128042  0.633117                 0.215688                  0.195648   \n",
       "905    0.126835  0.609268                 0.207779                  0.189225   \n",
       "906    0.125764  0.589175                 0.201520                  0.184046   \n",
       "907    0.124728  0.572261                 0.196570                  0.179881   \n",
       "\n",
       "     total_trades_coinbase  Tweets_Utilizados  Tweets_Utilizados_coin  \\\n",
       "902               0.352479           0.649573                0.501971   \n",
       "903               0.465957           0.309386                0.220505   \n",
       "904               0.464704           0.312922                0.218579   \n",
       "905               0.462304           0.315093                0.216597   \n",
       "906               0.459296           0.316223                0.214703   \n",
       "907               0.456013           0.316606                0.212948   \n",
       "\n",
       "     Tweets_Utilizados_referentes  Tweets_Utilizados_whale_alert  \\\n",
       "902                      0.000000                       0.139073   \n",
       "903                     -0.005968                       0.299016   \n",
       "904                     -0.007245                       0.302250   \n",
       "905                     -0.008629                       0.304361   \n",
       "906                     -0.010144                       0.305641   \n",
       "907                     -0.011763                       0.306304   \n",
       "\n",
       "     Buy_1000x_high  sell_1000x_high  total_trades_binance  \n",
       "902        0.156999         0.156988              0.075976  \n",
       "903        0.259091         0.210517              0.099516  \n",
       "904        0.250347         0.200213              0.097973  \n",
       "905        0.242077         0.191046              0.096684  \n",
       "906        0.234364         0.183017              0.095675  \n",
       "907        0.227222         0.176012              0.094896  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores predichos desnormalizados para los prximos 5 das:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Number of trades</th>\n",
       "      <th>Close_BTCUSDT</th>\n",
       "      <th>Volume_BTCUSDT</th>\n",
       "      <th>Number_of_trades_BTCUSDT</th>\n",
       "      <th>Close_ETHUSDT</th>\n",
       "      <th>Volume_ETHUSDT</th>\n",
       "      <th>Number_of_trades_ETHUSDT</th>\n",
       "      <th>Close_BNBUSDT</th>\n",
       "      <th>Volume_BNBUSDT</th>\n",
       "      <th>Number_of_trades_BNBUSDT</th>\n",
       "      <th>SMA_20</th>\n",
       "      <th>EMA_20</th>\n",
       "      <th>Upper_Band</th>\n",
       "      <th>Middle_Band</th>\n",
       "      <th>Lower_Band</th>\n",
       "      <th>RSI</th>\n",
       "      <th>buy_1000x_high_coinbase</th>\n",
       "      <th>sell_1000x_high_coinbase</th>\n",
       "      <th>total_trades_coinbase</th>\n",
       "      <th>Tweets_Utilizados</th>\n",
       "      <th>Tweets_Utilizados_coin</th>\n",
       "      <th>Tweets_Utilizados_referentes</th>\n",
       "      <th>Tweets_Utilizados_whale_alert</th>\n",
       "      <th>Buy_1000x_high</th>\n",
       "      <th>sell_1000x_high</th>\n",
       "      <th>total_trades_binance</th>\n",
       "      <th>Fecha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.973164</td>\n",
       "      <td>10.856914</td>\n",
       "      <td>9.577946</td>\n",
       "      <td>10.021710</td>\n",
       "      <td>318284.75000</td>\n",
       "      <td>69870.312500</td>\n",
       "      <td>41749.699219</td>\n",
       "      <td>2345184.250</td>\n",
       "      <td>3917.467041</td>\n",
       "      <td>597845.9375</td>\n",
       "      <td>1777044.000</td>\n",
       "      <td>539.197571</td>\n",
       "      <td>1799242.750</td>\n",
       "      <td>828352.0625</td>\n",
       "      <td>10.437915</td>\n",
       "      <td>10.330834</td>\n",
       "      <td>13.308776</td>\n",
       "      <td>10.542747</td>\n",
       "      <td>8.119136</td>\n",
       "      <td>62.916302</td>\n",
       "      <td>53.385445</td>\n",
       "      <td>57.323086</td>\n",
       "      <td>94373.570312</td>\n",
       "      <td>334.782928</td>\n",
       "      <td>198.804352</td>\n",
       "      <td>-0.101455</td>\n",
       "      <td>45.151344</td>\n",
       "      <td>456.917908</td>\n",
       "      <td>360.880188</td>\n",
       "      <td>193994.484375</td>\n",
       "      <td>2024-03-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.789328</td>\n",
       "      <td>10.622845</td>\n",
       "      <td>9.369250</td>\n",
       "      <td>9.771141</td>\n",
       "      <td>310397.43750</td>\n",
       "      <td>69829.539062</td>\n",
       "      <td>38794.878906</td>\n",
       "      <td>2267518.250</td>\n",
       "      <td>3918.681152</td>\n",
       "      <td>599516.4375</td>\n",
       "      <td>1785428.750</td>\n",
       "      <td>544.808472</td>\n",
       "      <td>1807474.375</td>\n",
       "      <td>832785.6250</td>\n",
       "      <td>10.456526</td>\n",
       "      <td>10.312521</td>\n",
       "      <td>13.373859</td>\n",
       "      <td>10.540338</td>\n",
       "      <td>8.069686</td>\n",
       "      <td>61.284393</td>\n",
       "      <td>51.118019</td>\n",
       "      <td>55.172874</td>\n",
       "      <td>94119.773438</td>\n",
       "      <td>338.506378</td>\n",
       "      <td>197.338562</td>\n",
       "      <td>-0.123160</td>\n",
       "      <td>45.639755</td>\n",
       "      <td>443.050598</td>\n",
       "      <td>344.733856</td>\n",
       "      <td>191111.343750</td>\n",
       "      <td>2024-03-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.618979</td>\n",
       "      <td>10.416780</td>\n",
       "      <td>9.187305</td>\n",
       "      <td>9.553308</td>\n",
       "      <td>302772.46875</td>\n",
       "      <td>69773.234375</td>\n",
       "      <td>35508.617188</td>\n",
       "      <td>2179543.750</td>\n",
       "      <td>3919.350830</td>\n",
       "      <td>598216.4375</td>\n",
       "      <td>1787432.000</td>\n",
       "      <td>548.625122</td>\n",
       "      <td>1807747.875</td>\n",
       "      <td>832689.1875</td>\n",
       "      <td>10.469585</td>\n",
       "      <td>10.295498</td>\n",
       "      <td>13.418129</td>\n",
       "      <td>10.536775</td>\n",
       "      <td>8.025675</td>\n",
       "      <td>59.861092</td>\n",
       "      <td>49.243637</td>\n",
       "      <td>53.361408</td>\n",
       "      <td>93633.640625</td>\n",
       "      <td>340.793243</td>\n",
       "      <td>195.830582</td>\n",
       "      <td>-0.146690</td>\n",
       "      <td>45.958565</td>\n",
       "      <td>429.934143</td>\n",
       "      <td>330.369751</td>\n",
       "      <td>188701.671875</td>\n",
       "      <td>2024-03-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.463839</td>\n",
       "      <td>10.239660</td>\n",
       "      <td>9.030800</td>\n",
       "      <td>9.368766</td>\n",
       "      <td>295742.43750</td>\n",
       "      <td>69727.179688</td>\n",
       "      <td>31956.802734</td>\n",
       "      <td>2084611.250</td>\n",
       "      <td>3921.001953</td>\n",
       "      <td>594603.6875</td>\n",
       "      <td>1784685.375</td>\n",
       "      <td>551.034790</td>\n",
       "      <td>1802572.000</td>\n",
       "      <td>829441.0625</td>\n",
       "      <td>10.477658</td>\n",
       "      <td>10.280959</td>\n",
       "      <td>13.447414</td>\n",
       "      <td>10.533132</td>\n",
       "      <td>7.986599</td>\n",
       "      <td>58.661968</td>\n",
       "      <td>47.760174</td>\n",
       "      <td>51.901089</td>\n",
       "      <td>93024.507812</td>\n",
       "      <td>341.982788</td>\n",
       "      <td>194.389267</td>\n",
       "      <td>-0.172451</td>\n",
       "      <td>46.151863</td>\n",
       "      <td>417.700867</td>\n",
       "      <td>317.788116</td>\n",
       "      <td>186816.812500</td>\n",
       "      <td>2024-03-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.319290</td>\n",
       "      <td>10.084472</td>\n",
       "      <td>8.892408</td>\n",
       "      <td>9.209630</td>\n",
       "      <td>289327.46875</td>\n",
       "      <td>69695.664062</td>\n",
       "      <td>28234.542969</td>\n",
       "      <td>1985700.875</td>\n",
       "      <td>3923.821045</td>\n",
       "      <td>589355.8750</td>\n",
       "      <td>1778593.500</td>\n",
       "      <td>552.405579</td>\n",
       "      <td>1793918.500</td>\n",
       "      <td>824113.3125</td>\n",
       "      <td>10.478598</td>\n",
       "      <td>10.266093</td>\n",
       "      <td>13.463116</td>\n",
       "      <td>10.526654</td>\n",
       "      <td>7.948835</td>\n",
       "      <td>57.652557</td>\n",
       "      <td>46.587036</td>\n",
       "      <td>50.726379</td>\n",
       "      <td>92359.414062</td>\n",
       "      <td>342.385742</td>\n",
       "      <td>193.053604</td>\n",
       "      <td>-0.199964</td>\n",
       "      <td>46.251892</td>\n",
       "      <td>406.373962</td>\n",
       "      <td>306.810425</td>\n",
       "      <td>185360.828125</td>\n",
       "      <td>2024-03-27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Open       High       Low      Close  Number of trades  Close_BTCUSDT  \\\n",
       "0  10.973164  10.856914  9.577946  10.021710      318284.75000   69870.312500   \n",
       "1  10.789328  10.622845  9.369250   9.771141      310397.43750   69829.539062   \n",
       "2  10.618979  10.416780  9.187305   9.553308      302772.46875   69773.234375   \n",
       "3  10.463839  10.239660  9.030800   9.368766      295742.43750   69727.179688   \n",
       "4  10.319290  10.084472  8.892408   9.209630      289327.46875   69695.664062   \n",
       "\n",
       "   Volume_BTCUSDT  Number_of_trades_BTCUSDT  Close_ETHUSDT  Volume_ETHUSDT  \\\n",
       "0    41749.699219               2345184.250    3917.467041     597845.9375   \n",
       "1    38794.878906               2267518.250    3918.681152     599516.4375   \n",
       "2    35508.617188               2179543.750    3919.350830     598216.4375   \n",
       "3    31956.802734               2084611.250    3921.001953     594603.6875   \n",
       "4    28234.542969               1985700.875    3923.821045     589355.8750   \n",
       "\n",
       "   Number_of_trades_ETHUSDT  Close_BNBUSDT  Volume_BNBUSDT  \\\n",
       "0               1777044.000     539.197571     1799242.750   \n",
       "1               1785428.750     544.808472     1807474.375   \n",
       "2               1787432.000     548.625122     1807747.875   \n",
       "3               1784685.375     551.034790     1802572.000   \n",
       "4               1778593.500     552.405579     1793918.500   \n",
       "\n",
       "   Number_of_trades_BNBUSDT     SMA_20     EMA_20  Upper_Band  Middle_Band  \\\n",
       "0               828352.0625  10.437915  10.330834   13.308776    10.542747   \n",
       "1               832785.6250  10.456526  10.312521   13.373859    10.540338   \n",
       "2               832689.1875  10.469585  10.295498   13.418129    10.536775   \n",
       "3               829441.0625  10.477658  10.280959   13.447414    10.533132   \n",
       "4               824113.3125  10.478598  10.266093   13.463116    10.526654   \n",
       "\n",
       "   Lower_Band        RSI  buy_1000x_high_coinbase  sell_1000x_high_coinbase  \\\n",
       "0    8.119136  62.916302                53.385445                 57.323086   \n",
       "1    8.069686  61.284393                51.118019                 55.172874   \n",
       "2    8.025675  59.861092                49.243637                 53.361408   \n",
       "3    7.986599  58.661968                47.760174                 51.901089   \n",
       "4    7.948835  57.652557                46.587036                 50.726379   \n",
       "\n",
       "   total_trades_coinbase  Tweets_Utilizados  Tweets_Utilizados_coin  \\\n",
       "0           94373.570312         334.782928              198.804352   \n",
       "1           94119.773438         338.506378              197.338562   \n",
       "2           93633.640625         340.793243              195.830582   \n",
       "3           93024.507812         341.982788              194.389267   \n",
       "4           92359.414062         342.385742              193.053604   \n",
       "\n",
       "   Tweets_Utilizados_referentes  Tweets_Utilizados_whale_alert  \\\n",
       "0                     -0.101455                      45.151344   \n",
       "1                     -0.123160                      45.639755   \n",
       "2                     -0.146690                      45.958565   \n",
       "3                     -0.172451                      46.151863   \n",
       "4                     -0.199964                      46.251892   \n",
       "\n",
       "   Buy_1000x_high  sell_1000x_high  total_trades_binance      Fecha  \n",
       "0      456.917908       360.880188         193994.484375 2024-03-23  \n",
       "1      443.050598       344.733856         191111.343750 2024-03-24  \n",
       "2      429.934143       330.369751         188701.671875 2024-03-25  \n",
       "3      417.700867       317.788116         186816.812500 2024-03-26  \n",
       "4      406.373962       306.810425         185360.828125 2024-03-27  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_days_to_predict = 5\n",
    "future_dataset = feature_dataset\n",
    "\n",
    "dataset = pd.read_csv('/Users/mmarchetta/Desktop/Tesis-2024/data-visualization/final_dataset.csv')\n",
    "dataset['Open_time'] = pd.to_datetime(dataset['Open_time'])\n",
    "dates = dataset['Open_time']\n",
    "\n",
    "# Crear un DataFrame vaco para almacenar las predicciones desnormalizadas\n",
    "predicted_values_desnormalized = pd.DataFrame(columns=future_dataset.columns)\n",
    "\n",
    "# Lista para almacenar las fechas de las predicciones\n",
    "predicted_dates = []\n",
    "\n",
    "for _ in range(n_days_to_predict):\n",
    "    # Predecir 1 da posterior al ltimo da disponible en el dataset\n",
    "    last_sequence = future_dataset.iloc[-n_steps:, :].values.reshape((1, n_steps, n_features))\n",
    "    predictions = best_model.predict(last_sequence)\n",
    "\n",
    "    # Agregar las predicciones sin desnormalizar a future_dataset\n",
    "    predicted_values_normalized = pd.DataFrame(predictions, columns=future_dataset.columns)\n",
    "    future_dataset = pd.concat([future_dataset, predicted_values_normalized], axis=0, ignore_index=True)\n",
    "\n",
    "    # Desnormalizar las predicciones y agregarlas al DataFrame de predicciones desnormalizadas\n",
    "    inverted_predictions = []\n",
    "    for i in range(len(future_dataset.columns)):\n",
    "        col = future_dataset.columns[i]\n",
    "        scaler = scalers[col]\n",
    "        prediction = predictions[:, i].reshape(-1, 1)\n",
    "        inverted_prediction = scaler.inverse_transform(prediction)\n",
    "        inverted_predictions.append(inverted_prediction)\n",
    "\n",
    "    # Calcular la fecha del prximo da\n",
    "    next_day_date = dates.iloc[-1] + pd.DateOffset(days=1)\n",
    "    predicted_dates.append(next_day_date)\n",
    "\n",
    "    # Actualizar la fecha del prximo da en el DataFrame principal\n",
    "    dates = dates.append(pd.Series([next_day_date], name='Fecha'))\n",
    "\n",
    "    # Crear un DataFrame con las predicciones desnormalizadas\n",
    "    predicted_values_desnormalized = pd.concat([predicted_values_desnormalized,\n",
    "                                                pd.DataFrame(np.concatenate(inverted_predictions, axis=1),\n",
    "                                                             columns=future_dataset.columns)], \n",
    "                                                ignore_index=True)\n",
    "\n",
    "# Agregar las fechas al DataFrame de predicciones desnormalizadas\n",
    "predicted_values_desnormalized['Fecha'] = predicted_dates\n",
    "\n",
    "print(\"Valores predichos para los prximos {} das:\".format(n_days_to_predict))\n",
    "display(future_dataset.tail(n_days_to_predict + 1))\n",
    "\n",
    "print(\"Valores predichos desnormalizados para los prximos {} das:\".format(n_days_to_predict))\n",
    "display(predicted_values_desnormalized.tail(n_days_to_predict))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardado de los mejores hiperparametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 mejores modelos:\n",
      "Modelo 1\n",
      "Hiperparmetros: OrderedDict([('activation', 'swish'), ('batch_size', 128), ('depth', 3), ('dropout', 0.2), ('epochs', 20), ('l2_penalty', 0.01), ('learning_rate', 0.001), ('optimizer', 'rmsprop'), ('units', 64)])\n",
      "Puntaje: -0.02766625692726051\n",
      "Modelo 2\n",
      "Hiperparmetros: OrderedDict([('activation', 'relu'), ('batch_size', 64), ('depth', 4), ('dropout', 0.2), ('epochs', 50), ('l2_penalty', 0.001), ('learning_rate', 0.01), ('optimizer', 'rmsprop'), ('units', 128)])\n",
      "Puntaje: -0.031976217665667626\n",
      "Modelo 3\n",
      "Hiperparmetros: OrderedDict([('activation', 'swish'), ('batch_size', 128), ('depth', 4), ('dropout', 0.2), ('epochs', 10), ('l2_penalty', 0.001), ('learning_rate', 0.001), ('optimizer', 'sgd'), ('units', 64)])\n",
      "Puntaje: -0.029549170450538113\n",
      "Modelo 4\n",
      "Hiperparmetros: OrderedDict([('activation', 'swish'), ('batch_size', 128), ('depth', 5), ('dropout', 0.3), ('epochs', 20), ('l2_penalty', 0.01), ('learning_rate', 0.0001), ('optimizer', 'sgd'), ('units', 128)])\n",
      "Puntaje: -0.03947803989401757\n",
      "Modelo 5\n",
      "Hiperparmetros: OrderedDict([('activation', 'selu'), ('batch_size', 128), ('depth', 5), ('dropout', 0.2), ('epochs', 10), ('l2_penalty', 0.1), ('learning_rate', 0.001), ('optimizer', 'rmsprop'), ('units', 256)])\n",
      "Puntaje: -3.6060609716367877\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Obtener los hiperparmetros y puntajes de los 5 mejores modelos\n",
    "top_n_models = 5\n",
    "best_params_list = []\n",
    "best_scores_list = []\n",
    "\n",
    "for i in range(min(top_n_models, len(bayes_search.cv_results_['params']))):\n",
    "    best_params_list.append(bayes_search.cv_results_['params'][i])\n",
    "    best_scores_list.append(bayes_search.cv_results_['mean_test_score'][i])\n",
    "\n",
    "# Guardar los hiperparmetros de los 5 mejores modelos en un archivo JSON\n",
    "with open('top_5_hyperparameters.json', 'w') as f:\n",
    "    json.dump({'best_params': best_params_list, 'best_scores': best_scores_list}, f)\n",
    "\n",
    "# O imprimir los hiperparmetros\n",
    "print(\"Top 5 mejores modelos:\")\n",
    "for i in range(len(best_params_list)):\n",
    "    print(\"Modelo\", i+1)\n",
    "    print(\"Hiperparmetros:\", best_params_list[i])\n",
    "    print(\"Puntaje:\", best_scores_list[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Armado de un ensamble con los mejores 5 hiperparametros usando la mejor semilla en cada caso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prime_seeds(n):\n",
    "    seeds = []\n",
    "    num = 70001  # Comenzamos desde el primer nmero primo mayor que 70000\n",
    "    while len(seeds) < n:\n",
    "        is_prime = True\n",
    "        for i in range(2, int(num**0.5) + 1):\n",
    "            if num % i == 0:\n",
    "                is_prime = False\n",
    "                break\n",
    "        if is_prime:\n",
    "            seeds.append(num)\n",
    "        num += 1\n",
    "    return seeds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_days(ensemble, feature_dataset, scalers, n_steps, n_features, n_days_to_predict):\n",
    "    future_dataset = feature_dataset.copy()\n",
    "\n",
    "    # Leer el conjunto de datos original para obtener las fechas\n",
    "    dataset = pd.read_csv('/Users/mmarchetta/Desktop/Tesis-2024/data-visualization/final_dataset.csv')\n",
    "    dataset['Open_time'] = pd.to_datetime(dataset['Open_time'])\n",
    "    dates = dataset['Open_time'][:-n_days_to_predict]\n",
    "\n",
    "    # Crear un DataFrame vaco para almacenar las predicciones desnormalizadas\n",
    "    predicted_values_desnormalized = pd.DataFrame(columns=future_dataset.columns)\n",
    "\n",
    "    # Lista para almacenar las fechas de las predicciones\n",
    "    predicted_dates = []\n",
    "\n",
    "    for _ in range(n_days_to_predict):\n",
    "        # Predecir 1 da posterior al ltimo da disponible en el dataset\n",
    "        last_sequence = future_dataset.iloc[-n_steps:, :].values.reshape((1, n_steps, n_features))\n",
    "        predictions = ensemble.predict(last_sequence)\n",
    "\n",
    "        # Agregar las predicciones sin desnormalizar a future_dataset\n",
    "        predicted_values_normalized = pd.DataFrame(predictions, columns=future_dataset.columns)\n",
    "        future_dataset = pd.concat([future_dataset, predicted_values_normalized], axis=0, ignore_index=True)\n",
    "\n",
    "        # Desnormalizar las predicciones y agregarlas al DataFrame de predicciones desnormalizadas\n",
    "        inverted_predictions = []\n",
    "        for i in range(len(future_dataset.columns)):\n",
    "            col = future_dataset.columns[i]\n",
    "            scaler = scalers[col]\n",
    "            prediction = predictions[:, i].reshape(-1, 1)\n",
    "            inverted_prediction = scaler.inverse_transform(prediction)\n",
    "            inverted_predictions.append(inverted_prediction)\n",
    "\n",
    "        # Calcular la fecha del prximo da\n",
    "        next_day_date = dates.iloc[-1] + pd.DateOffset(days=1)\n",
    "        predicted_dates.append(next_day_date)\n",
    "\n",
    "        # Actualizar la fecha del prximo da en el DataFrame principal\n",
    "        dates = dates.append(pd.Series([next_day_date], name='Fecha'))\n",
    "\n",
    "        # Crear un DataFrame con las predicciones desnormalizadas\n",
    "        predicted_values_desnormalized = pd.concat([predicted_values_desnormalized,\n",
    "                                                    pd.DataFrame(np.concatenate(inverted_predictions, axis=1),\n",
    "                                                                 columns=future_dataset.columns)], \n",
    "                                                    ignore_index=True)\n",
    "\n",
    "    # Agregar las fechas al DataFrame de predicciones desnormalizadas\n",
    "    predicted_values_desnormalized['Fecha'] = predicted_dates\n",
    "\n",
    "    return future_dataset, predicted_values_desnormalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clase personalizada para hacer el ensamble, dado que sklearn no provee ninguna clase que permita hacer ensmble\n",
    "## de modelos re regresion multivariados\n",
    "class MultivariableVotingRegressor:\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for model in self.models:\n",
    "            model.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Hacer predicciones con cada modelo\n",
    "        predictions = [model.predict(X) for model in self.models]\n",
    "    \n",
    "        # Calcular el promedio de las predicciones\n",
    "        average_predictions = np.mean(predictions, axis=0)\n",
    "    \n",
    "        return average_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 5s 356ms/step - loss: 1.1002 - accuracy: 0.3963\n",
      "Epoch 7/50\n",
      "14/14 [==============================] - 4s 317ms/step - loss: 1.0157 - accuracy: 0.3975\n",
      "Epoch 8/50\n",
      "14/14 [==============================] - 4s 298ms/step - loss: 0.9449 - accuracy: 0.4124\n",
      "Epoch 9/50\n",
      "14/14 [==============================] - 4s 273ms/step - loss: 0.8810 - accuracy: 0.4112\n",
      "Epoch 10/50\n",
      "14/14 [==============================] - 4s 289ms/step - loss: 0.8272 - accuracy: 0.4215\n",
      "Epoch 11/50\n",
      "14/14 [==============================] - 4s 264ms/step - loss: 0.7794 - accuracy: 0.4089\n",
      "Epoch 12/50\n",
      "14/14 [==============================] - 4s 263ms/step - loss: 0.7380 - accuracy: 0.4101\n",
      "Epoch 13/50\n",
      "14/14 [==============================] - 4s 256ms/step - loss: 0.6997 - accuracy: 0.4330\n",
      "Epoch 14/50\n",
      "14/14 [==============================] - 2s 129ms/step - loss: 0.6660 - accuracy: 0.4238\n",
      "Epoch 15/50\n",
      "14/14 [==============================] - 2s 132ms/step - loss: 0.6340 - accuracy: 0.4399\n",
      "Epoch 16/50\n",
      "14/14 [==============================] - 3s 206ms/step - loss: 0.6050 - accuracy: 0.4273\n",
      "Epoch 17/50\n",
      "14/14 [==============================] - 4s 252ms/step - loss: 0.5811 - accuracy: 0.4341\n",
      "Epoch 18/50\n",
      "14/14 [==============================] - 3s 249ms/step - loss: 0.5561 - accuracy: 0.4204\n",
      "Epoch 19/50\n",
      "14/14 [==============================] - 3s 250ms/step - loss: 0.5328 - accuracy: 0.4284\n",
      "Epoch 20/50\n",
      "14/14 [==============================] - 3s 248ms/step - loss: 0.5121 - accuracy: 0.4399\n",
      "Epoch 21/50\n",
      "14/14 [==============================] - 3s 234ms/step - loss: 0.4920 - accuracy: 0.4341\n",
      "Epoch 22/50\n",
      "14/14 [==============================] - 4s 260ms/step - loss: 0.4739 - accuracy: 0.4399\n",
      "Epoch 23/50\n",
      "14/14 [==============================] - 4s 310ms/step - loss: 0.4578 - accuracy: 0.4422\n",
      "Epoch 24/50\n",
      "14/14 [==============================] - 4s 283ms/step - loss: 0.4414 - accuracy: 0.4593\n",
      "Epoch 25/50\n",
      "14/14 [==============================] - 4s 276ms/step - loss: 0.4274 - accuracy: 0.4479\n",
      "Epoch 26/50\n",
      "14/14 [==============================] - 4s 253ms/step - loss: 0.4123 - accuracy: 0.4376\n",
      "Epoch 27/50\n",
      "14/14 [==============================] - 3s 242ms/step - loss: 0.4000 - accuracy: 0.4341\n",
      "Epoch 28/50\n",
      "14/14 [==============================] - 3s 192ms/step - loss: 0.3873 - accuracy: 0.4513\n",
      "Epoch 29/50\n",
      "14/14 [==============================] - 3s 212ms/step - loss: 0.3771 - accuracy: 0.4284\n",
      "Epoch 30/50\n",
      "14/14 [==============================] - 3s 243ms/step - loss: 0.3640 - accuracy: 0.4353\n",
      "Epoch 31/50\n",
      "14/14 [==============================] - 4s 300ms/step - loss: 0.3525 - accuracy: 0.4490\n",
      "Epoch 32/50\n",
      "14/14 [==============================] - 4s 293ms/step - loss: 0.3416 - accuracy: 0.4422\n",
      "Epoch 33/50\n",
      "14/14 [==============================] - 5s 380ms/step - loss: 0.3331 - accuracy: 0.4318\n",
      "Epoch 34/50\n",
      "14/14 [==============================] - 5s 358ms/step - loss: 0.3224 - accuracy: 0.4330\n",
      "Epoch 35/50\n",
      "14/14 [==============================] - 8s 560ms/step - loss: 0.3129 - accuracy: 0.4605\n",
      "Epoch 36/50\n",
      "14/14 [==============================] - 9s 645ms/step - loss: 0.3050 - accuracy: 0.4273\n",
      "Epoch 37/50\n",
      "14/14 [==============================] - 11s 800ms/step - loss: 0.2967 - accuracy: 0.4570\n",
      "Epoch 38/50\n",
      "14/14 [==============================] - 8s 548ms/step - loss: 0.2889 - accuracy: 0.4422\n",
      "Epoch 39/50\n",
      "14/14 [==============================] - 5s 344ms/step - loss: 0.2822 - accuracy: 0.4444\n",
      "Epoch 40/50\n",
      "14/14 [==============================] - 5s 365ms/step - loss: 0.2755 - accuracy: 0.4410\n",
      "Epoch 41/50\n",
      "14/14 [==============================] - 4s 251ms/step - loss: 0.2676 - accuracy: 0.4364\n",
      "Epoch 42/50\n",
      "14/14 [==============================] - 3s 241ms/step - loss: 0.2603 - accuracy: 0.4513\n",
      "Epoch 43/50\n",
      "14/14 [==============================] - 5s 347ms/step - loss: 0.2537 - accuracy: 0.4811\n",
      "Epoch 44/50\n",
      "14/14 [==============================] - 5s 372ms/step - loss: 0.2474 - accuracy: 0.4548\n",
      "Epoch 45/50\n",
      "14/14 [==============================] - 5s 349ms/step - loss: 0.2414 - accuracy: 0.4662\n",
      "Epoch 46/50\n",
      "14/14 [==============================] - 5s 320ms/step - loss: 0.2358 - accuracy: 0.4376\n",
      "Epoch 47/50\n",
      "14/14 [==============================] - 4s 321ms/step - loss: 0.2300 - accuracy: 0.4479\n",
      "Epoch 48/50\n",
      "14/14 [==============================] - 4s 311ms/step - loss: 0.2243 - accuracy: 0.4639\n",
      "Epoch 49/50\n",
      "14/14 [==============================] - 4s 276ms/step - loss: 0.2194 - accuracy: 0.4651\n",
      "Epoch 50/50\n",
      "14/14 [==============================] - 4s 292ms/step - loss: 0.2150 - accuracy: 0.4696\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "model number: 1, seed number: 59 error: -622831671352.3943\n",
      "Epoch 1/50\n",
      "14/14 [==============================] - 32s 266ms/step - loss: 1.6363 - accuracy: 0.0733\n",
      "Epoch 2/50\n",
      "14/14 [==============================] - 3s 212ms/step - loss: 1.4506 - accuracy: 0.2016\n",
      "Epoch 3/50\n",
      "14/14 [==============================] - 2s 141ms/step - loss: 1.4172 - accuracy: 0.3024\n",
      "Epoch 4/50\n",
      "14/14 [==============================] - 2s 162ms/step - loss: 1.3093 - accuracy: 0.3723\n",
      "Epoch 5/50\n",
      "14/14 [==============================] - 4s 271ms/step - loss: 1.2013 - accuracy: 0.3918\n",
      "Epoch 6/50\n",
      "14/14 [==============================] - 4s 274ms/step - loss: 1.1104 - accuracy: 0.3986\n",
      "Epoch 7/50\n",
      "14/14 [==============================] - 4s 248ms/step - loss: 1.0381 - accuracy: 0.3906\n",
      "Epoch 8/50\n",
      "14/14 [==============================] - 4s 280ms/step - loss: 0.9788 - accuracy: 0.4215\n",
      "Epoch 9/50\n",
      "14/14 [==============================] - 3s 241ms/step - loss: 0.9286 - accuracy: 0.4353\n",
      "Epoch 10/50\n",
      "14/14 [==============================] - 3s 241ms/step - loss: 0.8864 - accuracy: 0.4215\n",
      "Epoch 11/50\n",
      "14/14 [==============================] - 4s 241ms/step - loss: 0.8494 - accuracy: 0.4147\n",
      "Epoch 12/50\n",
      "14/14 [==============================] - 3s 234ms/step - loss: 0.8170 - accuracy: 0.4261\n",
      "Epoch 13/50\n",
      "14/14 [==============================] - 3s 231ms/step - loss: 0.7872 - accuracy: 0.4490\n",
      "Epoch 14/50\n",
      "14/14 [==============================] - 3s 240ms/step - loss: 0.7620 - accuracy: 0.4307\n",
      "Epoch 15/50\n",
      "14/14 [==============================] - 3s 234ms/step - loss: 0.7382 - accuracy: 0.4170\n",
      "Epoch 16/50\n",
      "14/14 [==============================] - 2s 158ms/step - loss: 0.7163 - accuracy: 0.4410\n",
      "Epoch 17/50\n",
      "14/14 [==============================] - 2s 131ms/step - loss: 0.6960 - accuracy: 0.4479\n",
      "Epoch 18/50\n",
      "14/14 [==============================] - 2s 163ms/step - loss: 0.6781 - accuracy: 0.4410\n",
      "Epoch 19/50\n",
      "14/14 [==============================] - 3s 239ms/step - loss: 0.6603 - accuracy: 0.4353\n",
      "Epoch 20/50\n",
      "14/14 [==============================] - 3s 245ms/step - loss: 0.6447 - accuracy: 0.4296\n",
      "Epoch 21/50\n",
      "14/14 [==============================] - 3s 240ms/step - loss: 0.6281 - accuracy: 0.4548\n",
      "Epoch 22/50\n",
      "14/14 [==============================] - 4s 251ms/step - loss: 0.6156 - accuracy: 0.4318\n",
      "Epoch 23/50\n",
      "14/14 [==============================] - 3s 238ms/step - loss: 0.6005 - accuracy: 0.4559\n",
      "Epoch 24/50\n",
      "14/14 [==============================] - 4s 260ms/step - loss: 0.5889 - accuracy: 0.4341\n",
      "Epoch 25/50\n",
      "14/14 [==============================] - 3s 234ms/step - loss: 0.5755 - accuracy: 0.4456\n",
      "Epoch 26/50\n",
      "14/14 [==============================] - 3s 249ms/step - loss: 0.5637 - accuracy: 0.4387\n",
      "Epoch 27/50\n",
      "14/14 [==============================] - 6s 457ms/step - loss: 0.5523 - accuracy: 0.4593\n",
      "Epoch 28/50\n",
      "14/14 [==============================] - 6s 409ms/step - loss: 0.5416 - accuracy: 0.4536\n",
      "Epoch 29/50\n",
      "14/14 [==============================] - 7s 472ms/step - loss: 0.5322 - accuracy: 0.4364\n",
      "Epoch 30/50\n",
      "14/14 [==============================] - 6s 403ms/step - loss: 0.5218 - accuracy: 0.4353\n",
      "Epoch 31/50\n",
      "14/14 [==============================] - 5s 370ms/step - loss: 0.5120 - accuracy: 0.4582\n",
      "Epoch 32/50\n",
      "14/14 [==============================] - 5s 359ms/step - loss: 0.5019 - accuracy: 0.4719\n",
      "Epoch 33/50\n",
      "14/14 [==============================] - 5s 371ms/step - loss: 0.4943 - accuracy: 0.4422\n",
      "Epoch 34/50\n",
      "14/14 [==============================] - 5s 331ms/step - loss: 0.4864 - accuracy: 0.4479\n",
      "Epoch 35/50\n",
      "14/14 [==============================] - 5s 331ms/step - loss: 0.4777 - accuracy: 0.4674\n",
      "Epoch 36/50\n",
      "14/14 [==============================] - 3s 212ms/step - loss: 0.4691 - accuracy: 0.4444\n",
      "Epoch 37/50\n",
      "14/14 [==============================] - 3s 185ms/step - loss: 0.4617 - accuracy: 0.4444\n",
      "Epoch 38/50\n",
      "14/14 [==============================] - 4s 292ms/step - loss: 0.4537 - accuracy: 0.4674\n",
      "Epoch 39/50\n",
      "14/14 [==============================] - 4s 284ms/step - loss: 0.4468 - accuracy: 0.4548\n",
      "Epoch 40/50\n",
      "14/14 [==============================] - 4s 298ms/step - loss: 0.4396 - accuracy: 0.4353\n",
      "Epoch 41/50\n",
      "14/14 [==============================] - 4s 269ms/step - loss: 0.4387 - accuracy: 0.4238\n",
      "Epoch 42/50\n",
      "14/14 [==============================] - 4s 280ms/step - loss: 0.4301 - accuracy: 0.4204\n",
      "Epoch 43/50\n",
      "14/14 [==============================] - 4s 276ms/step - loss: 0.4219 - accuracy: 0.4456\n",
      "Epoch 44/50\n",
      "14/14 [==============================] - 4s 284ms/step - loss: 0.4141 - accuracy: 0.4639\n",
      "Epoch 45/50\n",
      "14/14 [==============================] - 4s 289ms/step - loss: 0.4083 - accuracy: 0.4570\n",
      "Epoch 46/50\n",
      "14/14 [==============================] - 5s 338ms/step - loss: 0.4016 - accuracy: 0.4719\n",
      "Epoch 47/50\n",
      "14/14 [==============================] - 2s 160ms/step - loss: 0.3961 - accuracy: 0.4731\n",
      "Epoch 48/50\n",
      "14/14 [==============================] - 2s 144ms/step - loss: 0.3904 - accuracy: 0.4800\n",
      "Epoch 49/50\n",
      "14/14 [==============================] - 3s 234ms/step - loss: 0.3837 - accuracy: 0.4845\n",
      "Epoch 50/50\n",
      "14/14 [==============================] - 4s 271ms/step - loss: 0.3789 - accuracy: 0.4719\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "model number: 1, seed number: 60 error: -622831666270.3679\n",
      "Epoch 1/50\n",
      "14/14 [==============================] - 28s 297ms/step - loss: 1.5938 - accuracy: 0.0871\n",
      "Epoch 2/50\n",
      "14/14 [==============================] - 4s 297ms/step - loss: 1.4549 - accuracy: 0.3597\n",
      "Epoch 3/50\n",
      "14/14 [==============================] - 4s 310ms/step - loss: 1.5227 - accuracy: 0.3895\n",
      "Epoch 4/50\n",
      "14/14 [==============================] - 4s 277ms/step - loss: 1.4590 - accuracy: 0.4044\n",
      "Epoch 5/50\n",
      "14/14 [==============================] - 4s 272ms/step - loss: 1.3749 - accuracy: 0.4021\n",
      "Epoch 6/50\n",
      "14/14 [==============================] - 4s 278ms/step - loss: 1.2935 - accuracy: 0.4147\n",
      "Epoch 7/50\n",
      "14/14 [==============================] - 4s 303ms/step - loss: 1.2234 - accuracy: 0.4158\n",
      "Epoch 8/50\n",
      "14/14 [==============================] - 3s 191ms/step - loss: 1.1613 - accuracy: 0.4158\n",
      "Epoch 9/50\n",
      "14/14 [==============================] - 2s 167ms/step - loss: 1.1061 - accuracy: 0.4273\n",
      "Epoch 10/50\n",
      "14/14 [==============================] - 2s 166ms/step - loss: 1.0559 - accuracy: 0.4273\n",
      "Epoch 11/50\n",
      "14/14 [==============================] - 2s 179ms/step - loss: 1.0125 - accuracy: 0.4364\n",
      "Epoch 12/50\n",
      "14/14 [==============================] - 2s 176ms/step - loss: 0.9718 - accuracy: 0.4341\n",
      "Epoch 13/50\n",
      "14/14 [==============================] - 2s 171ms/step - loss: 0.9341 - accuracy: 0.4284\n",
      "Epoch 14/50\n",
      "14/14 [==============================] - 2s 166ms/step - loss: 0.9001 - accuracy: 0.4502\n",
      "Epoch 15/50\n",
      "14/14 [==============================] - 2s 165ms/step - loss: 0.8676 - accuracy: 0.4410\n",
      "Epoch 16/50\n",
      "14/14 [==============================] - 20s 2s/step - loss: 0.8387 - accuracy: 0.4502\n",
      "Epoch 17/50\n",
      "14/14 [==============================] - 8s 527ms/step - loss: 0.8111 - accuracy: 0.4353\n",
      "Epoch 18/50\n",
      "14/14 [==============================] - 6s 398ms/step - loss: 0.7855 - accuracy: 0.4296\n",
      "Epoch 19/50\n",
      "14/14 [==============================] - 4s 295ms/step - loss: 0.7600 - accuracy: 0.4444\n",
      "Epoch 20/50\n",
      "14/14 [==============================] - 6s 440ms/step - loss: 0.7369 - accuracy: 0.4467\n",
      "Epoch 21/50\n",
      "14/14 [==============================] - 6s 458ms/step - loss: 0.7152 - accuracy: 0.4456\n",
      "Epoch 22/50\n",
      "14/14 [==============================] - 6s 422ms/step - loss: 0.6939 - accuracy: 0.4548\n",
      "Epoch 23/50\n",
      "14/14 [==============================] - 5s 384ms/step - loss: 0.6744 - accuracy: 0.4444\n",
      "Epoch 24/50\n",
      "14/14 [==============================] - 5s 354ms/step - loss: 0.6567 - accuracy: 0.4227\n",
      "Epoch 25/50\n",
      "14/14 [==============================] - 4s 284ms/step - loss: 0.6381 - accuracy: 0.4410\n",
      "Epoch 26/50\n",
      "14/14 [==============================] - 3s 242ms/step - loss: 0.6218 - accuracy: 0.4433\n",
      "Epoch 27/50\n",
      "14/14 [==============================] - 5s 329ms/step - loss: 0.6048 - accuracy: 0.4502\n",
      "Epoch 28/50\n",
      "14/14 [==============================] - 5s 351ms/step - loss: 0.5901 - accuracy: 0.4525\n",
      "Epoch 29/50\n",
      "14/14 [==============================] - 5s 377ms/step - loss: 0.5747 - accuracy: 0.4456\n",
      "Epoch 30/50\n",
      "14/14 [==============================] - 5s 365ms/step - loss: 0.5610 - accuracy: 0.4456\n",
      "Epoch 31/50\n",
      "14/14 [==============================] - 5s 372ms/step - loss: 0.5477 - accuracy: 0.4490\n",
      "Epoch 32/50\n",
      "14/14 [==============================] - 5s 371ms/step - loss: 0.5341 - accuracy: 0.4536\n",
      "Epoch 33/50\n",
      "14/14 [==============================] - 5s 365ms/step - loss: 0.5230 - accuracy: 0.4387\n",
      "Epoch 34/50\n",
      "14/14 [==============================] - 3s 226ms/step - loss: 0.5109 - accuracy: 0.4525\n",
      "Epoch 35/50\n",
      "14/14 [==============================] - 5s 349ms/step - loss: 0.5029 - accuracy: 0.4261\n",
      "Epoch 36/50\n",
      "14/14 [==============================] - 6s 429ms/step - loss: 0.4907 - accuracy: 0.4307\n",
      "Epoch 37/50\n",
      "14/14 [==============================] - 5s 390ms/step - loss: 0.4785 - accuracy: 0.4399\n",
      "Epoch 38/50\n",
      "14/14 [==============================] - 5s 362ms/step - loss: 0.4672 - accuracy: 0.4215\n",
      "Epoch 39/50\n",
      "14/14 [==============================] - 5s 369ms/step - loss: 0.4574 - accuracy: 0.4273\n",
      "Epoch 40/50\n",
      "14/14 [==============================] - 5s 369ms/step - loss: 0.4475 - accuracy: 0.4261\n",
      "Epoch 41/50\n",
      "14/14 [==============================] - 5s 355ms/step - loss: 0.4369 - accuracy: 0.4341\n",
      "Epoch 42/50\n",
      "14/14 [==============================] - 3s 221ms/step - loss: 0.4284 - accuracy: 0.4570\n",
      "Epoch 43/50\n",
      "14/14 [==============================] - 5s 354ms/step - loss: 0.4193 - accuracy: 0.4639\n",
      "Epoch 44/50\n",
      "14/14 [==============================] - 5s 389ms/step - loss: 0.4113 - accuracy: 0.4570\n",
      "Epoch 45/50\n",
      "14/14 [==============================] - 5s 332ms/step - loss: 0.4038 - accuracy: 0.4616\n",
      "Epoch 46/50\n",
      "14/14 [==============================] - 5s 324ms/step - loss: 0.3967 - accuracy: 0.4639\n",
      "Epoch 47/50\n",
      "14/14 [==============================] - 5s 326ms/step - loss: 0.3884 - accuracy: 0.4490\n",
      "Epoch 48/50\n",
      "14/14 [==============================] - 5s 324ms/step - loss: 0.3850 - accuracy: 0.4296\n",
      "Epoch 49/50\n",
      "14/14 [==============================] - 5s 347ms/step - loss: 0.3750 - accuracy: 0.4536\n",
      "Epoch 50/50\n",
      "14/14 [==============================] - 3s 198ms/step - loss: 0.3682 - accuracy: 0.4536\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "model number: 1, seed number: 61 error: -622831681951.7517\n",
      "Epoch 1/50\n",
      "14/14 [==============================] - 46s 357ms/step - loss: 1.5611 - accuracy: 0.1145\n",
      "Epoch 2/50\n",
      "14/14 [==============================] - 4s 289ms/step - loss: 1.4039 - accuracy: 0.2268\n",
      "Epoch 3/50\n",
      "14/14 [==============================] - 2s 164ms/step - loss: 1.2704 - accuracy: 0.3448\n",
      "Epoch 4/50\n",
      "14/14 [==============================] - 4s 290ms/step - loss: 1.0762 - accuracy: 0.3608\n",
      "Epoch 5/50\n",
      "14/14 [==============================] - 4s 308ms/step - loss: 0.9037 - accuracy: 0.3918\n",
      "Epoch 6/50\n",
      "14/14 [==============================] - 4s 316ms/step - loss: 0.7637 - accuracy: 0.3998\n",
      "Epoch 7/50\n",
      "14/14 [==============================] - 4s 317ms/step - loss: 0.6587 - accuracy: 0.3929\n",
      "Epoch 8/50\n",
      "14/14 [==============================] - 5s 328ms/step - loss: 0.5762 - accuracy: 0.3998\n",
      "Epoch 9/50\n",
      "14/14 [==============================] - 5s 335ms/step - loss: 0.5101 - accuracy: 0.3918\n",
      "Epoch 10/50\n",
      "14/14 [==============================] - 4s 316ms/step - loss: 0.4547 - accuracy: 0.3952\n",
      "Epoch 11/50\n",
      "14/14 [==============================] - 4s 301ms/step - loss: 0.4085 - accuracy: 0.4158\n",
      "Epoch 12/50\n",
      "14/14 [==============================] - 3s 170ms/step - loss: 0.3694 - accuracy: 0.4204\n",
      "Epoch 13/50\n",
      "14/14 [==============================] - 2s 177ms/step - loss: 0.3359 - accuracy: 0.4215\n",
      "Epoch 14/50\n",
      "14/14 [==============================] - 4s 311ms/step - loss: 0.3077 - accuracy: 0.3998\n",
      "Epoch 15/50\n",
      "14/14 [==============================] - 4s 304ms/step - loss: 0.2816 - accuracy: 0.4227\n",
      "Epoch 16/50\n",
      "14/14 [==============================] - 4s 296ms/step - loss: 0.2596 - accuracy: 0.4170\n",
      "Epoch 17/50\n",
      "14/14 [==============================] - 4s 297ms/step - loss: 0.2404 - accuracy: 0.4261\n",
      "Epoch 18/50\n",
      "14/14 [==============================] - 4s 276ms/step - loss: 0.2238 - accuracy: 0.4525\n",
      "Epoch 19/50\n",
      "14/14 [==============================] - 4s 268ms/step - loss: 0.2080 - accuracy: 0.4479\n",
      "Epoch 20/50\n",
      "14/14 [==============================] - 4s 269ms/step - loss: 0.1937 - accuracy: 0.4570\n",
      "Epoch 21/50\n",
      "14/14 [==============================] - 4s 277ms/step - loss: 0.1816 - accuracy: 0.4662\n",
      "Epoch 22/50\n",
      "14/14 [==============================] - 3s 208ms/step - loss: 0.1706 - accuracy: 0.4536\n",
      "Epoch 23/50\n",
      "14/14 [==============================] - 2s 141ms/step - loss: 0.1604 - accuracy: 0.4467\n",
      "Epoch 24/50\n",
      "14/14 [==============================] - 3s 188ms/step - loss: 0.1498 - accuracy: 0.4559\n",
      "Epoch 25/50\n",
      "14/14 [==============================] - 4s 257ms/step - loss: 0.1415 - accuracy: 0.4616\n",
      "Epoch 26/50\n",
      "14/14 [==============================] - 4s 274ms/step - loss: 0.1346 - accuracy: 0.4536\n",
      "Epoch 27/50\n",
      "14/14 [==============================] - 4s 251ms/step - loss: 0.1273 - accuracy: 0.4559\n",
      "Epoch 28/50\n",
      "14/14 [==============================] - 4s 254ms/step - loss: 0.1193 - accuracy: 0.4651\n",
      "Epoch 29/50\n",
      "14/14 [==============================] - 4s 263ms/step - loss: 0.1134 - accuracy: 0.4800\n",
      "Epoch 30/50\n",
      "14/14 [==============================] - 4s 269ms/step - loss: 0.1086 - accuracy: 0.4456\n",
      "Epoch 31/50\n",
      "14/14 [==============================] - 4s 269ms/step - loss: 0.1020 - accuracy: 0.4696\n",
      "Epoch 32/50\n",
      "14/14 [==============================] - 4s 286ms/step - loss: 0.0976 - accuracy: 0.4639\n",
      "Epoch 33/50\n",
      "14/14 [==============================] - 4s 275ms/step - loss: 0.0939 - accuracy: 0.4754\n",
      "Epoch 34/50\n",
      "14/14 [==============================] - 3s 204ms/step - loss: 0.0896 - accuracy: 0.4410\n",
      "Epoch 35/50\n",
      "14/14 [==============================] - 2s 159ms/step - loss: 0.0856 - accuracy: 0.4708\n",
      "Epoch 36/50\n",
      "14/14 [==============================] - 3s 229ms/step - loss: 0.0820 - accuracy: 0.4616\n",
      "Epoch 37/50\n",
      "14/14 [==============================] - 4s 279ms/step - loss: 0.0783 - accuracy: 0.4616\n",
      "Epoch 38/50\n",
      "14/14 [==============================] - 4s 287ms/step - loss: 0.0750 - accuracy: 0.4719\n",
      "Epoch 39/50\n",
      "14/14 [==============================] - 4s 270ms/step - loss: 0.0720 - accuracy: 0.4525\n",
      "Epoch 40/50\n",
      "14/14 [==============================] - 4s 274ms/step - loss: 0.0695 - accuracy: 0.4685\n",
      "Epoch 41/50\n",
      "14/14 [==============================] - 4s 265ms/step - loss: 0.0667 - accuracy: 0.4467\n",
      "Epoch 42/50\n",
      "14/14 [==============================] - 4s 272ms/step - loss: 0.0632 - accuracy: 0.4731\n",
      "Epoch 43/50\n",
      "14/14 [==============================] - 4s 263ms/step - loss: 0.0615 - accuracy: 0.4857\n",
      "Epoch 44/50\n",
      "14/14 [==============================] - 4s 282ms/step - loss: 0.0584 - accuracy: 0.4788\n",
      "Epoch 45/50\n",
      "14/14 [==============================] - 4s 269ms/step - loss: 0.0564 - accuracy: 0.4685\n",
      "Epoch 46/50\n",
      "14/14 [==============================] - 2s 157ms/step - loss: 0.0549 - accuracy: 0.5006\n",
      "Epoch 47/50\n",
      "14/14 [==============================] - 2s 162ms/step - loss: 0.0555 - accuracy: 0.4674\n",
      "Epoch 48/50\n",
      "14/14 [==============================] - 4s 246ms/step - loss: 0.0516 - accuracy: 0.4467\n",
      "Epoch 49/50\n",
      "14/14 [==============================] - 4s 273ms/step - loss: 0.0498 - accuracy: 0.4731\n",
      "Epoch 50/50\n",
      "14/14 [==============================] - 4s 269ms/step - loss: 0.0475 - accuracy: 0.4834\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "model number: 1, seed number: 62 error: -622831673196.535\n",
      "Epoch 1/50\n",
      "14/14 [==============================] - 31s 291ms/step - loss: 1.5822 - accuracy: 0.1409\n",
      "Epoch 2/50\n",
      "14/14 [==============================] - 4s 298ms/step - loss: 1.3757 - accuracy: 0.2165\n",
      "Epoch 3/50\n",
      "14/14 [==============================] - 4s 307ms/step - loss: 1.2590 - accuracy: 0.3711\n",
      "Epoch 4/50\n",
      "14/14 [==============================] - 4s 282ms/step - loss: 1.0765 - accuracy: 0.3986\n",
      "Epoch 5/50\n",
      "14/14 [==============================] - 4s 308ms/step - loss: 0.9147 - accuracy: 0.4009\n",
      "Epoch 6/50\n",
      "14/14 [==============================] - 3s 184ms/step - loss: 0.7931 - accuracy: 0.4147\n",
      "Epoch 7/50\n",
      "14/14 [==============================] - 3s 184ms/step - loss: 0.6981 - accuracy: 0.4112\n",
      "Epoch 8/50\n",
      "14/14 [==============================] - 4s 300ms/step - loss: 0.6242 - accuracy: 0.4124\n",
      "Epoch 9/50\n",
      "14/14 [==============================] - 4s 292ms/step - loss: 0.5652 - accuracy: 0.4055\n",
      "Epoch 10/50\n",
      "14/14 [==============================] - 4s 294ms/step - loss: 0.5186 - accuracy: 0.4330\n",
      "Epoch 11/50\n",
      "14/14 [==============================] - 4s 308ms/step - loss: 0.4785 - accuracy: 0.4318\n",
      "Epoch 12/50\n",
      "14/14 [==============================] - 4s 297ms/step - loss: 0.4444 - accuracy: 0.4376\n",
      "Epoch 13/50\n",
      "14/14 [==============================] - 4s 320ms/step - loss: 0.4172 - accuracy: 0.4559\n",
      "Epoch 14/50\n",
      "14/14 [==============================] - 4s 300ms/step - loss: 0.3913 - accuracy: 0.4513\n",
      "Epoch 15/50\n",
      "14/14 [==============================] - 4s 287ms/step - loss: 0.3702 - accuracy: 0.4376\n",
      "Epoch 16/50\n",
      "14/14 [==============================] - 2s 160ms/step - loss: 0.3517 - accuracy: 0.4548\n",
      "Epoch 17/50\n",
      "14/14 [==============================] - 3s 187ms/step - loss: 0.3345 - accuracy: 0.4456\n",
      "Epoch 18/50\n",
      "14/14 [==============================] - 4s 283ms/step - loss: 0.3214 - accuracy: 0.4387\n",
      "Epoch 19/50\n",
      "14/14 [==============================] - 4s 295ms/step - loss: 0.3070 - accuracy: 0.4605\n",
      "Epoch 20/50\n",
      "14/14 [==============================] - 4s 280ms/step - loss: 0.2950 - accuracy: 0.4651\n",
      "Epoch 21/50\n",
      "14/14 [==============================] - 4s 268ms/step - loss: 0.2842 - accuracy: 0.4559\n",
      "Epoch 22/50\n",
      "14/14 [==============================] - 4s 287ms/step - loss: 0.2739 - accuracy: 0.4754\n",
      "Epoch 23/50\n",
      "14/14 [==============================] - 4s 284ms/step - loss: 0.2644 - accuracy: 0.4525\n",
      "Epoch 24/50\n",
      "14/14 [==============================] - 4s 265ms/step - loss: 0.2551 - accuracy: 0.4536\n",
      "Epoch 25/50\n",
      "14/14 [==============================] - 4s 271ms/step - loss: 0.2487 - accuracy: 0.4593\n",
      "Epoch 26/50\n",
      "14/14 [==============================] - 4s 249ms/step - loss: 0.2419 - accuracy: 0.4559\n",
      "Epoch 27/50\n",
      "14/14 [==============================] - 2s 151ms/step - loss: 0.2351 - accuracy: 0.4467\n",
      "Epoch 28/50\n",
      "14/14 [==============================] - 3s 212ms/step - loss: 0.2293 - accuracy: 0.4467\n",
      "Epoch 29/50\n",
      "14/14 [==============================] - 4s 291ms/step - loss: 0.2216 - accuracy: 0.4479\n",
      "Epoch 30/50\n",
      "14/14 [==============================] - 4s 297ms/step - loss: 0.2159 - accuracy: 0.4662\n",
      "Epoch 31/50\n",
      "14/14 [==============================] - 4s 294ms/step - loss: 0.2106 - accuracy: 0.4731\n",
      "Epoch 32/50\n",
      "14/14 [==============================] - 4s 299ms/step - loss: 0.2058 - accuracy: 0.4708\n",
      "Epoch 33/50\n",
      "14/14 [==============================] - 4s 301ms/step - loss: 0.2008 - accuracy: 0.4502\n",
      "Epoch 34/50\n",
      "14/14 [==============================] - 4s 288ms/step - loss: 0.1956 - accuracy: 0.4628\n",
      "Epoch 35/50\n",
      "14/14 [==============================] - 4s 281ms/step - loss: 0.1909 - accuracy: 0.4777\n",
      "Epoch 36/50\n",
      "14/14 [==============================] - 4s 306ms/step - loss: 0.1863 - accuracy: 0.4582\n",
      "Epoch 37/50\n",
      "14/14 [==============================] - 2s 168ms/step - loss: 0.1834 - accuracy: 0.4651\n",
      "Epoch 38/50\n",
      "14/14 [==============================] - 2s 144ms/step - loss: 0.1798 - accuracy: 0.4559\n",
      "Epoch 39/50\n",
      "14/14 [==============================] - 3s 245ms/step - loss: 0.1758 - accuracy: 0.4731\n",
      "Epoch 40/50\n",
      "14/14 [==============================] - 4s 258ms/step - loss: 0.1735 - accuracy: 0.4353\n",
      "Epoch 41/50\n",
      "14/14 [==============================] - 4s 266ms/step - loss: 0.1704 - accuracy: 0.4250\n",
      "Epoch 42/50\n",
      "14/14 [==============================] - 4s 264ms/step - loss: 0.1665 - accuracy: 0.4479\n",
      "Epoch 43/50\n",
      "14/14 [==============================] - 4s 248ms/step - loss: 0.1631 - accuracy: 0.4479\n",
      "Epoch 44/50\n",
      "14/14 [==============================] - 4s 253ms/step - loss: 0.1592 - accuracy: 0.4318\n",
      "Epoch 45/50\n",
      "14/14 [==============================] - 4s 272ms/step - loss: 0.1567 - accuracy: 0.4456\n",
      "Epoch 46/50\n",
      "14/14 [==============================] - 4s 260ms/step - loss: 0.1533 - accuracy: 0.4502\n",
      "Epoch 47/50\n",
      "14/14 [==============================] - 4s 265ms/step - loss: 0.1506 - accuracy: 0.4399\n",
      "Epoch 48/50\n",
      "14/14 [==============================] - 4s 276ms/step - loss: 0.1493 - accuracy: 0.4467\n",
      "Epoch 49/50\n",
      "14/14 [==============================] - 3s 171ms/step - loss: 0.1463 - accuracy: 0.4570\n",
      "Epoch 50/50\n",
      "14/14 [==============================] - 3s 171ms/step - loss: 0.1433 - accuracy: 0.4628\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 0s 154ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "model number: 1, seed number: 63 error: -622831657359.3993\n",
      "Epoch 1/50\n",
      "14/14 [==============================] - 29s 326ms/step - loss: 1.6023 - accuracy: 0.1168\n",
      "Epoch 2/50\n",
      "14/14 [==============================] - 5s 351ms/step - loss: 1.3725 - accuracy: 0.1890\n",
      "Epoch 3/50\n",
      "14/14 [==============================] - 5s 345ms/step - loss: 1.2552 - accuracy: 0.3482\n",
      "Epoch 4/50\n",
      "14/14 [==============================] - 4s 312ms/step - loss: 1.0656 - accuracy: 0.3849\n",
      "Epoch 5/50\n",
      "14/14 [==============================] - 4s 283ms/step - loss: 0.8907 - accuracy: 0.4124\n",
      "Epoch 6/50\n",
      "14/14 [==============================] - 4s 304ms/step - loss: 0.7489 - accuracy: 0.4009\n",
      "Epoch 7/50\n",
      "14/14 [==============================] - 4s 293ms/step - loss: 0.6408 - accuracy: 0.4089\n",
      "Epoch 8/50\n",
      "14/14 [==============================] - 4s 304ms/step - loss: 0.5554 - accuracy: 0.4089\n",
      "Epoch 9/50\n",
      "14/14 [==============================] - 2s 158ms/step - loss: 0.4887 - accuracy: 0.4089\n",
      "Epoch 10/50\n",
      "14/14 [==============================] - 3s 184ms/step - loss: 0.4318 - accuracy: 0.4318\n",
      "Epoch 11/50\n",
      "14/14 [==============================] - 4s 308ms/step - loss: 0.3868 - accuracy: 0.4112\n",
      "Epoch 12/50\n",
      "14/14 [==============================] - 4s 300ms/step - loss: 0.3484 - accuracy: 0.4353\n",
      "Epoch 13/50\n",
      "14/14 [==============================] - 4s 294ms/step - loss: 0.3172 - accuracy: 0.4536\n",
      "Epoch 14/50\n",
      "14/14 [==============================] - 4s 311ms/step - loss: 0.2888 - accuracy: 0.4330\n",
      "Epoch 15/50\n",
      "14/14 [==============================] - 4s 307ms/step - loss: 0.2650 - accuracy: 0.4307\n",
      "Epoch 16/50\n",
      "14/14 [==============================] - 6s 453ms/step - loss: 0.2442 - accuracy: 0.4456\n",
      "Epoch 17/50\n",
      "14/14 [==============================] - 8s 549ms/step - loss: 0.2269 - accuracy: 0.4605\n",
      "Epoch 18/50\n",
      "14/14 [==============================] - 5s 319ms/step - loss: 0.2106 - accuracy: 0.4376\n",
      "Epoch 19/50\n",
      "14/14 [==============================] - 7s 516ms/step - loss: 0.1989 - accuracy: 0.4467\n",
      "Epoch 20/50\n",
      "14/14 [==============================] - 7s 489ms/step - loss: 0.1879 - accuracy: 0.4536\n",
      "Epoch 21/50\n",
      "14/14 [==============================] - 7s 478ms/step - loss: 0.1786 - accuracy: 0.4170\n",
      "Epoch 22/50\n",
      "14/14 [==============================] - 6s 446ms/step - loss: 0.1668 - accuracy: 0.4444\n",
      "Epoch 23/50\n",
      "14/14 [==============================] - 6s 422ms/step - loss: 0.1580 - accuracy: 0.4525\n",
      "Epoch 24/50\n",
      "14/14 [==============================] - 3s 216ms/step - loss: 0.1488 - accuracy: 0.4433\n",
      "Epoch 25/50\n",
      "14/14 [==============================] - 6s 409ms/step - loss: 0.1411 - accuracy: 0.4307\n",
      "Epoch 26/50\n",
      "14/14 [==============================] - 5s 358ms/step - loss: 0.1356 - accuracy: 0.4593\n",
      "Epoch 27/50\n",
      "14/14 [==============================] - 6s 414ms/step - loss: 0.1301 - accuracy: 0.4307\n",
      "Epoch 28/50\n",
      "14/14 [==============================] - 6s 397ms/step - loss: 0.1251 - accuracy: 0.4387\n",
      "Epoch 29/50\n",
      "14/14 [==============================] - 5s 384ms/step - loss: 0.1196 - accuracy: 0.4593\n",
      "Epoch 30/50\n",
      "14/14 [==============================] - 6s 402ms/step - loss: 0.1149 - accuracy: 0.4593\n",
      "Epoch 31/50\n",
      "14/14 [==============================] - 2s 173ms/step - loss: 0.1118 - accuracy: 0.3975\n",
      "Epoch 32/50\n",
      "14/14 [==============================] - 4s 289ms/step - loss: 0.1069 - accuracy: 0.4559\n",
      "Epoch 33/50\n",
      "14/14 [==============================] - 5s 326ms/step - loss: 0.1031 - accuracy: 0.4536\n",
      "Epoch 34/50\n",
      "14/14 [==============================] - 5s 348ms/step - loss: 0.0988 - accuracy: 0.4570\n",
      "Epoch 35/50\n",
      "14/14 [==============================] - 5s 342ms/step - loss: 0.0953 - accuracy: 0.4742\n",
      "Epoch 36/50\n",
      "14/14 [==============================] - 5s 340ms/step - loss: 0.0925 - accuracy: 0.4593\n",
      "Epoch 37/50\n",
      "14/14 [==============================] - 5s 374ms/step - loss: 0.0901 - accuracy: 0.4845\n",
      "Epoch 38/50\n",
      "14/14 [==============================] - 5s 332ms/step - loss: 0.0875 - accuracy: 0.4582\n",
      "Epoch 39/50\n",
      "14/14 [==============================] - 4s 287ms/step - loss: 0.0845 - accuracy: 0.4777\n",
      "Epoch 40/50\n",
      "14/14 [==============================] - 3s 185ms/step - loss: 0.0824 - accuracy: 0.4696\n",
      "Epoch 41/50\n",
      "14/14 [==============================] - 4s 314ms/step - loss: 0.0799 - accuracy: 0.4834\n",
      "Epoch 42/50\n",
      "14/14 [==============================] - 5s 364ms/step - loss: 0.0779 - accuracy: 0.4674\n",
      "Epoch 43/50\n",
      "14/14 [==============================] - 5s 361ms/step - loss: 0.0761 - accuracy: 0.4593\n",
      "Epoch 44/50\n",
      "14/14 [==============================] - 5s 353ms/step - loss: 0.0754 - accuracy: 0.4605\n",
      "Epoch 45/50\n",
      "14/14 [==============================] - 5s 372ms/step - loss: 0.0749 - accuracy: 0.4376\n",
      "Epoch 46/50\n",
      "14/14 [==============================] - 5s 326ms/step - loss: 0.0721 - accuracy: 0.4467\n",
      "Epoch 47/50\n",
      "14/14 [==============================] - 4s 309ms/step - loss: 0.0703 - accuracy: 0.4662\n",
      "Epoch 48/50\n",
      "14/14 [==============================] - 2s 150ms/step - loss: 0.0675 - accuracy: 0.4685\n",
      "Epoch 49/50\n",
      "14/14 [==============================] - 2s 130ms/step - loss: 0.0687 - accuracy: 0.4353\n",
      "Epoch 50/50\n",
      "14/14 [==============================] - 3s 246ms/step - loss: 0.0656 - accuracy: 0.4616\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "model number: 1, seed number: 64 error: -622831684490.7415\n",
      "Epoch 1/50\n",
      "14/14 [==============================] - 27s 277ms/step - loss: 1.6052 - accuracy: 0.0733\n",
      "Epoch 2/50\n",
      "14/14 [==============================] - 4s 316ms/step - loss: 1.4377 - accuracy: 0.1684\n",
      "Epoch 3/50\n",
      "14/14 [==============================] - 4s 274ms/step - loss: 1.3270 - accuracy: 0.3024\n",
      "Epoch 4/50\n",
      "14/14 [==============================] - 4s 279ms/step - loss: 1.1481 - accuracy: 0.3608\n",
      "Epoch 5/50\n",
      "14/14 [==============================] - 4s 279ms/step - loss: 0.9915 - accuracy: 0.4032\n",
      "Epoch 6/50\n",
      "14/14 [==============================] - 4s 271ms/step - loss: 0.8665 - accuracy: 0.3963\n",
      "Epoch 7/50\n",
      "14/14 [==============================] - 4s 271ms/step - loss: 0.7654 - accuracy: 0.4032\n",
      "Epoch 8/50\n",
      "14/14 [==============================] - 4s 272ms/step - loss: 0.6838 - accuracy: 0.3975\n",
      "Epoch 9/50\n",
      "14/14 [==============================] - 4s 295ms/step - loss: 0.6198 - accuracy: 0.4066\n",
      "Epoch 10/50\n",
      "14/14 [==============================] - 2s 156ms/step - loss: 0.5624 - accuracy: 0.4044\n",
      "Epoch 11/50\n",
      "14/14 [==============================] - 2s 150ms/step - loss: 0.5136 - accuracy: 0.4181\n",
      "Epoch 12/50\n",
      "14/14 [==============================] - 4s 278ms/step - loss: 0.4714 - accuracy: 0.4078\n",
      "Epoch 13/50\n",
      "14/14 [==============================] - 4s 280ms/step - loss: 0.4346 - accuracy: 0.4250\n",
      "Epoch 14/50\n",
      "14/14 [==============================] - 4s 291ms/step - loss: 0.4030 - accuracy: 0.4273\n",
      "Epoch 15/50\n",
      "14/14 [==============================] - 4s 264ms/step - loss: 0.3752 - accuracy: 0.4307\n",
      "Epoch 16/50\n",
      "14/14 [==============================] - 4s 289ms/step - loss: 0.3505 - accuracy: 0.4158\n",
      "Epoch 17/50\n",
      "14/14 [==============================] - 4s 290ms/step - loss: 0.3269 - accuracy: 0.4582\n",
      "Epoch 18/50\n",
      "14/14 [==============================] - 4s 300ms/step - loss: 0.3064 - accuracy: 0.4307\n",
      "Epoch 19/50\n",
      "14/14 [==============================] - 4s 284ms/step - loss: 0.2889 - accuracy: 0.4376\n",
      "Epoch 20/50\n",
      "14/14 [==============================] - 4s 265ms/step - loss: 0.2728 - accuracy: 0.4513\n",
      "Epoch 21/50\n",
      "14/14 [==============================] - 2s 152ms/step - loss: 0.2568 - accuracy: 0.4513\n",
      "Epoch 22/50\n",
      "14/14 [==============================] - 2s 162ms/step - loss: 0.2427 - accuracy: 0.4433\n",
      "Epoch 23/50\n",
      "14/14 [==============================] - 4s 298ms/step - loss: 0.2299 - accuracy: 0.4353\n",
      "Epoch 24/50\n",
      "14/14 [==============================] - 4s 300ms/step - loss: 0.2192 - accuracy: 0.4490\n",
      "Epoch 25/50\n",
      "14/14 [==============================] - 4s 278ms/step - loss: 0.2073 - accuracy: 0.4628\n",
      "Epoch 26/50\n",
      "14/14 [==============================] - 4s 283ms/step - loss: 0.1974 - accuracy: 0.4582\n",
      "Epoch 27/50\n",
      "14/14 [==============================] - 4s 301ms/step - loss: 0.1881 - accuracy: 0.4696\n",
      "Epoch 28/50\n",
      "14/14 [==============================] - 4s 320ms/step - loss: 0.1787 - accuracy: 0.4536\n",
      "Epoch 29/50\n",
      "14/14 [==============================] - 4s 310ms/step - loss: 0.1714 - accuracy: 0.4364\n",
      "Epoch 30/50\n",
      "14/14 [==============================] - 4s 303ms/step - loss: 0.1651 - accuracy: 0.4467\n",
      "Epoch 31/50\n",
      "14/14 [==============================] - 4s 252ms/step - loss: 0.1575 - accuracy: 0.4433\n",
      "Epoch 32/50\n",
      "14/14 [==============================] - 2s 172ms/step - loss: 0.1505 - accuracy: 0.4582\n",
      "Epoch 33/50\n",
      "14/14 [==============================] - 3s 245ms/step - loss: 0.1459 - accuracy: 0.4250\n",
      "Epoch 34/50\n",
      "14/14 [==============================] - 4s 311ms/step - loss: 0.1392 - accuracy: 0.4582\n",
      "Epoch 35/50\n",
      "14/14 [==============================] - 5s 326ms/step - loss: 0.1323 - accuracy: 0.4525\n",
      "Epoch 36/50\n",
      "14/14 [==============================] - 4s 307ms/step - loss: 0.1264 - accuracy: 0.4628\n",
      "Epoch 37/50\n",
      "14/14 [==============================] - 5s 328ms/step - loss: 0.1227 - accuracy: 0.4548\n",
      "Epoch 38/50\n",
      "14/14 [==============================] - 5s 342ms/step - loss: 0.1195 - accuracy: 0.4410\n",
      "Epoch 39/50\n",
      "14/14 [==============================] - 5s 377ms/step - loss: 0.1145 - accuracy: 0.4296\n",
      "Epoch 40/50\n",
      "14/14 [==============================] - 5s 346ms/step - loss: 0.1108 - accuracy: 0.4353\n",
      "Epoch 41/50\n",
      "14/14 [==============================] - 5s 320ms/step - loss: 0.1054 - accuracy: 0.4765\n",
      "Epoch 42/50\n",
      "14/14 [==============================] - 3s 172ms/step - loss: 0.1029 - accuracy: 0.4387\n",
      "Epoch 43/50\n",
      "14/14 [==============================] - 3s 195ms/step - loss: 0.0986 - accuracy: 0.4662\n",
      "Epoch 44/50\n",
      "14/14 [==============================] - 4s 313ms/step - loss: 0.0958 - accuracy: 0.4490\n",
      "Epoch 45/50\n",
      "14/14 [==============================] - 4s 301ms/step - loss: 0.0923 - accuracy: 0.4605\n",
      "Epoch 46/50\n",
      "14/14 [==============================] - 4s 290ms/step - loss: 0.0887 - accuracy: 0.4548\n",
      "Epoch 47/50\n",
      "14/14 [==============================] - 4s 294ms/step - loss: 0.0858 - accuracy: 0.4639\n",
      "Epoch 48/50\n",
      "14/14 [==============================] - 5s 325ms/step - loss: 0.0835 - accuracy: 0.4628\n",
      "Epoch 49/50\n",
      "14/14 [==============================] - 4s 274ms/step - loss: 0.0806 - accuracy: 0.4719\n",
      "Epoch 50/50\n",
      "14/14 [==============================] - 4s 276ms/step - loss: 0.0781 - accuracy: 0.4765\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "model number: 1, seed number: 65 error: -622831668868.9979\n",
      "Epoch 1/50\n",
      "14/14 [==============================] - 31s 338ms/step - loss: 1.5647 - accuracy: 0.0997\n",
      "Epoch 2/50\n",
      "14/14 [==============================] - 4s 301ms/step - loss: 1.3769 - accuracy: 0.1913\n",
      "Epoch 3/50\n",
      "14/14 [==============================] - 4s 253ms/step - loss: 1.2380 - accuracy: 0.3184\n",
      "Epoch 4/50\n",
      "14/14 [==============================] - 5s 368ms/step - loss: 1.0492 - accuracy: 0.3631\n",
      "Epoch 5/50\n",
      "14/14 [==============================] - 7s 497ms/step - loss: 0.8860 - accuracy: 0.3906\n",
      "Epoch 6/50\n",
      "14/14 [==============================] - 6s 405ms/step - loss: 0.7625 - accuracy: 0.4055\n",
      "Epoch 7/50\n",
      "14/14 [==============================] - 6s 426ms/step - loss: 0.6687 - accuracy: 0.4192\n",
      "Epoch 8/50\n",
      "14/14 [==============================] - 6s 416ms/step - loss: 0.5953 - accuracy: 0.4204\n",
      "Epoch 9/50\n",
      "14/14 [==============================] - 6s 416ms/step - loss: 0.5383 - accuracy: 0.4170\n",
      "Epoch 10/50\n",
      "14/14 [==============================] - 5s 351ms/step - loss: 0.4926 - accuracy: 0.4422\n",
      "Epoch 11/50\n",
      "14/14 [==============================] - 3s 243ms/step - loss: 0.4541 - accuracy: 0.4353\n",
      "Epoch 12/50\n",
      "14/14 [==============================] - 6s 408ms/step - loss: 0.4219 - accuracy: 0.4353\n",
      "Epoch 13/50\n",
      "14/14 [==============================] - 6s 419ms/step - loss: 0.3957 - accuracy: 0.4318\n",
      "Epoch 14/50\n",
      "14/14 [==============================] - 6s 399ms/step - loss: 0.3721 - accuracy: 0.4570\n",
      "Epoch 15/50\n",
      "14/14 [==============================] - 6s 398ms/step - loss: 0.3519 - accuracy: 0.4731\n",
      "Epoch 16/50\n",
      "14/14 [==============================] - 5s 385ms/step - loss: 0.3342 - accuracy: 0.4490\n",
      "Epoch 17/50\n",
      "14/14 [==============================] - 6s 400ms/step - loss: 0.3186 - accuracy: 0.4651\n",
      "Epoch 18/50\n",
      "14/14 [==============================] - 4s 272ms/step - loss: 0.3053 - accuracy: 0.4353\n",
      "Epoch 19/50\n",
      "14/14 [==============================] - 6s 417ms/step - loss: 0.2925 - accuracy: 0.4708\n",
      "Epoch 20/50\n",
      "14/14 [==============================] - 7s 498ms/step - loss: 0.2817 - accuracy: 0.4628\n",
      "Epoch 21/50\n",
      "14/14 [==============================] - 6s 440ms/step - loss: 0.2713 - accuracy: 0.4616\n",
      "Epoch 22/50\n",
      "14/14 [==============================] - 7s 469ms/step - loss: 0.2624 - accuracy: 0.4765\n",
      "Epoch 23/50\n",
      "14/14 [==============================] - 6s 454ms/step - loss: 0.2543 - accuracy: 0.4765\n",
      "Epoch 24/50\n",
      "14/14 [==============================] - 7s 471ms/step - loss: 0.2461 - accuracy: 0.4696\n",
      "Epoch 25/50\n",
      "14/14 [==============================] - 4s 308ms/step - loss: 0.2389 - accuracy: 0.4674\n",
      "Epoch 26/50\n",
      "14/14 [==============================] - 6s 451ms/step - loss: 0.2319 - accuracy: 0.4662\n",
      "Epoch 27/50\n",
      "14/14 [==============================] - 6s 439ms/step - loss: 0.2264 - accuracy: 0.4605\n",
      "Epoch 28/50\n",
      "14/14 [==============================] - 6s 442ms/step - loss: 0.2209 - accuracy: 0.4651\n",
      "Epoch 29/50\n",
      "14/14 [==============================] - 6s 447ms/step - loss: 0.2154 - accuracy: 0.4708\n",
      "Epoch 30/50\n",
      "14/14 [==============================] - 6s 396ms/step - loss: 0.2096 - accuracy: 0.4490\n",
      "Epoch 31/50\n",
      "14/14 [==============================] - 5s 371ms/step - loss: 0.2057 - accuracy: 0.4662\n",
      "Epoch 32/50\n",
      "14/14 [==============================] - 4s 278ms/step - loss: 0.2014 - accuracy: 0.4788\n",
      "Epoch 33/50\n",
      "14/14 [==============================] - 3s 234ms/step - loss: 0.1960 - accuracy: 0.4708\n",
      "Epoch 34/50\n",
      "14/14 [==============================] - 5s 344ms/step - loss: 0.1916 - accuracy: 0.4525\n",
      "Epoch 35/50\n",
      "14/14 [==============================] - 5s 374ms/step - loss: 0.1888 - accuracy: 0.4719\n",
      "Epoch 36/50\n",
      "14/14 [==============================] - 5s 372ms/step - loss: 0.1847 - accuracy: 0.4479\n",
      "Epoch 37/50\n",
      "14/14 [==============================] - 5s 367ms/step - loss: 0.1813 - accuracy: 0.4502\n",
      "Epoch 38/50\n",
      "14/14 [==============================] - 5s 330ms/step - loss: 0.1784 - accuracy: 0.4422\n",
      "Epoch 39/50\n",
      "14/14 [==============================] - 5s 349ms/step - loss: 0.1775 - accuracy: 0.4444\n",
      "Epoch 40/50\n",
      "14/14 [==============================] - 4s 318ms/step - loss: 0.1784 - accuracy: 0.4124\n",
      "Epoch 41/50\n",
      "14/14 [==============================] - 5s 322ms/step - loss: 0.1784 - accuracy: 0.4296\n",
      "Epoch 42/50\n",
      "14/14 [==============================] - 3s 223ms/step - loss: 0.1734 - accuracy: 0.4204\n",
      "Epoch 43/50\n",
      "14/14 [==============================] - 4s 275ms/step - loss: 0.1729 - accuracy: 0.4685\n",
      "Epoch 44/50\n",
      "14/14 [==============================] - 5s 345ms/step - loss: 0.1641 - accuracy: 0.4318\n",
      "Epoch 45/50\n",
      "14/14 [==============================] - 5s 328ms/step - loss: 0.1603 - accuracy: 0.4525\n",
      "Epoch 46/50\n",
      "14/14 [==============================] - 5s 344ms/step - loss: 0.1564 - accuracy: 0.4548\n",
      "Epoch 47/50\n",
      "14/14 [==============================] - 5s 342ms/step - loss: 0.1534 - accuracy: 0.4341\n",
      "Epoch 48/50\n",
      "14/14 [==============================] - 5s 351ms/step - loss: 0.1506 - accuracy: 0.4719\n",
      "Epoch 49/50\n",
      "14/14 [==============================] - 4s 303ms/step - loss: 0.1482 - accuracy: 0.4605\n",
      "Epoch 50/50\n",
      "14/14 [==============================] - 4s 281ms/step - loss: 0.1477 - accuracy: 0.4582\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "model number: 1, seed number: 66 error: -622831676371.9507\n",
      "Epoch 1/50\n",
      "14/14 [==============================] - 46s 419ms/step - loss: 1.6080 - accuracy: 0.0871\n",
      "Epoch 2/50\n",
      "14/14 [==============================] - 6s 411ms/step - loss: 1.4733 - accuracy: 0.1730\n",
      "Epoch 3/50\n",
      "14/14 [==============================] - 6s 426ms/step - loss: 1.3754 - accuracy: 0.2646\n",
      "Epoch 4/50\n",
      "14/14 [==============================] - 5s 353ms/step - loss: 1.2161 - accuracy: 0.3746\n",
      "Epoch 5/50\n",
      "14/14 [==============================] - 3s 248ms/step - loss: 1.0833 - accuracy: 0.3906\n",
      "Epoch 6/50\n",
      "14/14 [==============================] - 5s 377ms/step - loss: 0.9701 - accuracy: 0.3998\n",
      "Epoch 7/50\n",
      "14/14 [==============================] - 5s 383ms/step - loss: 0.8832 - accuracy: 0.4170\n",
      "Epoch 8/50\n",
      "14/14 [==============================] - 5s 366ms/step - loss: 0.8121 - accuracy: 0.4318\n",
      "Epoch 9/50\n",
      "14/14 [==============================] - 5s 354ms/step - loss: 0.7539 - accuracy: 0.4055\n",
      "Epoch 10/50\n",
      "14/14 [==============================] - 5s 340ms/step - loss: 0.7039 - accuracy: 0.4284\n",
      "Epoch 11/50\n",
      "14/14 [==============================] - 5s 324ms/step - loss: 0.6641 - accuracy: 0.4628\n",
      "Epoch 12/50\n",
      "14/14 [==============================] - 4s 253ms/step - loss: 0.6272 - accuracy: 0.4433\n",
      "Epoch 13/50\n",
      "14/14 [==============================] - 2s 173ms/step - loss: 0.5964 - accuracy: 0.4433\n",
      "Epoch 14/50\n",
      "14/14 [==============================] - 4s 303ms/step - loss: 0.5684 - accuracy: 0.4456\n",
      "Epoch 15/50\n",
      "14/14 [==============================] - 5s 343ms/step - loss: 0.5441 - accuracy: 0.4433\n",
      "Epoch 16/50\n",
      "14/14 [==============================] - 4s 319ms/step - loss: 0.5232 - accuracy: 0.4674\n",
      "Epoch 17/50\n",
      "14/14 [==============================] - 4s 316ms/step - loss: 0.5037 - accuracy: 0.4422\n",
      "Epoch 18/50\n",
      "14/14 [==============================] - 5s 346ms/step - loss: 0.4859 - accuracy: 0.4651\n",
      "Epoch 19/50\n",
      "14/14 [==============================] - 5s 355ms/step - loss: 0.4705 - accuracy: 0.4719\n",
      "Epoch 20/50\n",
      "14/14 [==============================] - 5s 368ms/step - loss: 0.4567 - accuracy: 0.4719\n",
      "Epoch 21/50\n",
      "14/14 [==============================] - 4s 241ms/step - loss: 0.4440 - accuracy: 0.4719\n",
      "Epoch 22/50\n",
      "14/14 [==============================] - 3s 224ms/step - loss: 0.4321 - accuracy: 0.4662\n",
      "Epoch 23/50\n",
      "14/14 [==============================] - 5s 337ms/step - loss: 0.4207 - accuracy: 0.4754\n",
      "Epoch 24/50\n",
      "14/14 [==============================] - 5s 324ms/step - loss: 0.4106 - accuracy: 0.4639\n",
      "Epoch 25/50\n",
      "14/14 [==============================] - 4s 294ms/step - loss: 0.4015 - accuracy: 0.4754\n",
      "Epoch 26/50\n",
      "14/14 [==============================] - 4s 301ms/step - loss: 0.3929 - accuracy: 0.4811\n",
      "Epoch 27/50\n",
      "14/14 [==============================] - 4s 317ms/step - loss: 0.3857 - accuracy: 0.4444\n",
      "Epoch 28/50\n",
      "14/14 [==============================] - 4s 322ms/step - loss: 0.3777 - accuracy: 0.4719\n",
      "Epoch 29/50\n",
      "14/14 [==============================] - 4s 318ms/step - loss: 0.3703 - accuracy: 0.4845\n",
      "Epoch 30/50\n",
      "14/14 [==============================] - 4s 291ms/step - loss: 0.3639 - accuracy: 0.4570\n",
      "Epoch 31/50\n",
      "14/14 [==============================] - 3s 205ms/step - loss: 0.3573 - accuracy: 0.4800\n",
      "Epoch 32/50\n",
      "14/14 [==============================] - 3s 200ms/step - loss: 0.3517 - accuracy: 0.4811\n",
      "Epoch 33/50\n",
      "14/14 [==============================] - 3s 196ms/step - loss: 0.3471 - accuracy: 0.4513\n",
      "Epoch 34/50\n",
      "14/14 [==============================] - 3s 196ms/step - loss: 0.3415 - accuracy: 0.4616\n",
      "Epoch 35/50\n",
      "14/14 [==============================] - 3s 180ms/step - loss: 0.3365 - accuracy: 0.4639\n",
      "Epoch 36/50\n",
      "14/14 [==============================] - 2s 180ms/step - loss: 0.3323 - accuracy: 0.4857\n",
      "Epoch 37/50\n",
      "14/14 [==============================] - 3s 191ms/step - loss: 0.3276 - accuracy: 0.4754\n",
      "Epoch 38/50\n",
      "14/14 [==============================] - 4s 306ms/step - loss: 0.3231 - accuracy: 0.4605\n",
      "Epoch 39/50\n",
      "14/14 [==============================] - 4s 290ms/step - loss: 0.3195 - accuracy: 0.4696\n",
      "Epoch 40/50\n",
      "14/14 [==============================] - 4s 290ms/step - loss: 0.3174 - accuracy: 0.4525\n",
      "Epoch 41/50\n",
      "14/14 [==============================] - 4s 291ms/step - loss: 0.3139 - accuracy: 0.4433\n",
      "Epoch 42/50\n",
      "14/14 [==============================] - 4s 273ms/step - loss: 0.3093 - accuracy: 0.4777\n",
      "Epoch 43/50\n",
      "14/14 [==============================] - 4s 287ms/step - loss: 0.3083 - accuracy: 0.4548\n",
      "Epoch 44/50\n",
      "14/14 [==============================] - 4s 286ms/step - loss: 0.3033 - accuracy: 0.4605\n",
      "Epoch 45/50\n",
      "14/14 [==============================] - 4s 288ms/step - loss: 0.3009 - accuracy: 0.4719\n",
      "Epoch 46/50\n",
      "14/14 [==============================] - 3s 230ms/step - loss: 0.2984 - accuracy: 0.4318\n",
      "Epoch 47/50\n",
      "14/14 [==============================] - 3s 180ms/step - loss: 0.2951 - accuracy: 0.4307\n",
      "Epoch 48/50\n",
      "14/14 [==============================] - 5s 331ms/step - loss: 0.2923 - accuracy: 0.4639\n",
      "Epoch 49/50\n",
      "14/14 [==============================] - 5s 325ms/step - loss: 0.2900 - accuracy: 0.4662\n",
      "Epoch 50/50\n",
      "14/14 [==============================] - 5s 320ms/step - loss: 0.2865 - accuracy: 0.4719\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 139ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "model number: 1, seed number: 67 error: -622831662102.5082\n",
      "Epoch 1/50\n",
      "14/14 [==============================] - 46s 285ms/step - loss: 1.5374 - accuracy: 0.0848\n",
      "Epoch 2/50\n",
      "14/14 [==============================] - 5s 343ms/step - loss: 1.3131 - accuracy: 0.2600\n",
      "Epoch 3/50\n",
      "14/14 [==============================] - 5s 354ms/step - loss: 1.1396 - accuracy: 0.3585\n",
      "Epoch 4/50\n",
      "14/14 [==============================] - 5s 365ms/step - loss: 0.9241 - accuracy: 0.3860\n",
      "Epoch 5/50\n",
      "14/14 [==============================] - 5s 388ms/step - loss: 0.7462 - accuracy: 0.4055\n",
      "Epoch 6/50\n",
      "14/14 [==============================] - 6s 398ms/step - loss: 0.6183 - accuracy: 0.4044\n",
      "Epoch 7/50\n",
      "14/14 [==============================] - 7s 497ms/step - loss: 0.5242 - accuracy: 0.4238\n",
      "Epoch 8/50\n",
      "14/14 [==============================] - 5s 383ms/step - loss: 0.4516 - accuracy: 0.4296\n",
      "Epoch 9/50\n",
      "14/14 [==============================] - 5s 391ms/step - loss: 0.3969 - accuracy: 0.4055\n",
      "Epoch 10/50\n",
      "14/14 [==============================] - 4s 246ms/step - loss: 0.3546 - accuracy: 0.4227\n",
      "Epoch 11/50\n",
      "14/14 [==============================] - 4s 259ms/step - loss: 0.3188 - accuracy: 0.4296\n",
      "Epoch 12/50\n",
      "14/14 [==============================] - 5s 382ms/step - loss: 0.2888 - accuracy: 0.4307\n",
      "Epoch 13/50\n",
      "14/14 [==============================] - 5s 369ms/step - loss: 0.2649 - accuracy: 0.4147\n",
      "Epoch 14/50\n",
      "14/14 [==============================] - 5s 354ms/step - loss: 0.2444 - accuracy: 0.4353\n",
      "Epoch 15/50\n",
      "14/14 [==============================] - 5s 386ms/step - loss: 0.2268 - accuracy: 0.4582\n",
      "Epoch 16/50\n",
      "14/14 [==============================] - 6s 414ms/step - loss: 0.2117 - accuracy: 0.4341\n",
      "Epoch 17/50\n",
      "14/14 [==============================] - 6s 442ms/step - loss: 0.2003 - accuracy: 0.4490\n",
      "Epoch 18/50\n",
      "14/14 [==============================] - 6s 450ms/step - loss: 0.1887 - accuracy: 0.4525\n",
      "Epoch 19/50\n",
      "14/14 [==============================] - 7s 522ms/step - loss: 0.1776 - accuracy: 0.4479\n",
      "Epoch 20/50\n",
      "14/14 [==============================] - 6s 458ms/step - loss: 0.1687 - accuracy: 0.4662\n",
      "Epoch 21/50\n",
      "14/14 [==============================] - 5s 353ms/step - loss: 0.1613 - accuracy: 0.4685\n",
      "Epoch 22/50\n",
      "14/14 [==============================] - 6s 441ms/step - loss: 0.1554 - accuracy: 0.4513\n",
      "Epoch 23/50\n",
      "14/14 [==============================] - 8s 558ms/step - loss: 0.1486 - accuracy: 0.4525\n",
      "Epoch 24/50\n",
      "14/14 [==============================] - 7s 471ms/step - loss: 0.1426 - accuracy: 0.4410\n",
      "Epoch 25/50\n",
      "14/14 [==============================] - 6s 433ms/step - loss: 0.1373 - accuracy: 0.4444\n",
      "Epoch 26/50\n",
      "14/14 [==============================] - 7s 512ms/step - loss: 0.1319 - accuracy: 0.4708\n",
      "Epoch 27/50\n",
      "14/14 [==============================] - 7s 487ms/step - loss: 0.1277 - accuracy: 0.4490\n",
      "Epoch 28/50\n",
      "14/14 [==============================] - 7s 491ms/step - loss: 0.1232 - accuracy: 0.4456\n",
      "Epoch 29/50\n",
      "14/14 [==============================] - 6s 435ms/step - loss: 0.1196 - accuracy: 0.4490\n",
      "Epoch 30/50\n",
      "14/14 [==============================] - 6s 425ms/step - loss: 0.1166 - accuracy: 0.4559\n",
      "Epoch 31/50\n",
      "14/14 [==============================] - 4s 273ms/step - loss: 0.1122 - accuracy: 0.4593\n",
      "Epoch 32/50\n",
      "14/14 [==============================] - 4s 260ms/step - loss: 0.1093 - accuracy: 0.4559\n",
      "Epoch 33/50\n",
      "14/14 [==============================] - 5s 337ms/step - loss: 0.1069 - accuracy: 0.4605\n",
      "Epoch 34/50\n",
      "14/14 [==============================] - 6s 416ms/step - loss: 0.1029 - accuracy: 0.4433\n",
      "Epoch 35/50\n",
      "14/14 [==============================] - 5s 359ms/step - loss: 0.1010 - accuracy: 0.4685\n",
      "Epoch 36/50\n",
      "14/14 [==============================] - 5s 388ms/step - loss: 0.0989 - accuracy: 0.4422\n",
      "Epoch 37/50\n",
      "14/14 [==============================] - 5s 349ms/step - loss: 0.0963 - accuracy: 0.4433\n",
      "Epoch 38/50\n",
      "14/14 [==============================] - 5s 384ms/step - loss: 0.0942 - accuracy: 0.4387\n",
      "Epoch 39/50\n",
      "14/14 [==============================] - 5s 377ms/step - loss: 0.0923 - accuracy: 0.4513\n",
      "Epoch 40/50\n",
      "14/14 [==============================] - 5s 385ms/step - loss: 0.0908 - accuracy: 0.4525\n",
      "Epoch 41/50\n",
      "14/14 [==============================] - 3s 237ms/step - loss: 0.0890 - accuracy: 0.4341\n",
      "Epoch 42/50\n",
      "14/14 [==============================] - 5s 346ms/step - loss: 0.0874 - accuracy: 0.4525\n",
      "Epoch 43/50\n",
      "14/14 [==============================] - 5s 367ms/step - loss: 0.0838 - accuracy: 0.4502\n",
      "Epoch 44/50\n",
      "14/14 [==============================] - 4s 309ms/step - loss: 0.0827 - accuracy: 0.4490\n",
      "Epoch 45/50\n",
      "14/14 [==============================] - 5s 330ms/step - loss: 0.0814 - accuracy: 0.4444\n",
      "Epoch 46/50\n",
      "14/14 [==============================] - 5s 345ms/step - loss: 0.0787 - accuracy: 0.4696\n",
      "Epoch 47/50\n",
      "14/14 [==============================] - 5s 345ms/step - loss: 0.0779 - accuracy: 0.4651\n",
      "Epoch 48/50\n",
      "14/14 [==============================] - 5s 328ms/step - loss: 0.0764 - accuracy: 0.4685\n",
      "Epoch 49/50\n",
      "14/14 [==============================] - 5s 330ms/step - loss: 0.0757 - accuracy: 0.4616\n",
      "Epoch 50/50\n",
      "14/14 [==============================] - 3s 227ms/step - loss: 0.0735 - accuracy: 0.4777\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "model number: 1, seed number: 68 error: -622831672889.9069\n",
      "Epoch 1/50\n",
      "14/14 [==============================] - 44s 332ms/step - loss: 1.5937 - accuracy: 0.0607\n",
      "Epoch 2/50\n",
      "14/14 [==============================] - 5s 376ms/step - loss: 1.5117 - accuracy: 0.1775\n",
      "Epoch 3/50\n",
      "14/14 [==============================] - 4s 248ms/step - loss: 1.5538 - accuracy: 0.2795\n",
      "Epoch 4/50\n",
      "14/14 [==============================] - 4s 307ms/step - loss: 1.4969 - accuracy: 0.3162\n",
      "Epoch 5/50\n",
      "14/14 [==============================] - 6s 464ms/step - loss: 1.4345 - accuracy: 0.3643\n",
      "Epoch 6/50\n",
      "14/14 [==============================] - 5s 375ms/step - loss: 1.3748 - accuracy: 0.3666\n",
      "Epoch 7/50\n",
      "14/14 [==============================] - 6s 386ms/step - loss: 1.3263 - accuracy: 0.3860\n",
      "Epoch 8/50\n",
      "14/14 [==============================] - 5s 378ms/step - loss: 1.2859 - accuracy: 0.3952\n",
      "Epoch 9/50\n",
      "14/14 [==============================] - 6s 432ms/step - loss: 1.2491 - accuracy: 0.3975\n",
      "Epoch 10/50\n",
      "14/14 [==============================] - 6s 406ms/step - loss: 1.2173 - accuracy: 0.3975\n",
      "Epoch 11/50\n",
      "14/14 [==============================] - 6s 428ms/step - loss: 1.1898 - accuracy: 0.4066\n",
      "Epoch 12/50\n",
      "14/14 [==============================] - 6s 414ms/step - loss: 1.1651 - accuracy: 0.4181\n",
      "Epoch 13/50\n",
      "14/14 [==============================] - 4s 277ms/step - loss: 1.1427 - accuracy: 0.4192\n",
      "Epoch 14/50\n",
      "14/14 [==============================] - 6s 402ms/step - loss: 1.1209 - accuracy: 0.4089\n",
      "Epoch 15/50\n",
      "14/14 [==============================] - 6s 406ms/step - loss: 1.1011 - accuracy: 0.4124\n",
      "Epoch 16/50\n",
      "14/14 [==============================] - 6s 458ms/step - loss: 1.0811 - accuracy: 0.4124\n",
      "Epoch 17/50\n",
      "14/14 [==============================] - 6s 451ms/step - loss: 1.0642 - accuracy: 0.4227\n",
      "Epoch 18/50\n",
      "14/14 [==============================] - 6s 434ms/step - loss: 1.0478 - accuracy: 0.4261\n",
      "Epoch 19/50\n",
      "14/14 [==============================] - 6s 430ms/step - loss: 1.0342 - accuracy: 0.4238\n",
      "Epoch 20/50\n",
      "14/14 [==============================] - 6s 451ms/step - loss: 1.0171 - accuracy: 0.4399\n",
      "Epoch 21/50\n",
      "14/14 [==============================] - 6s 431ms/step - loss: 1.0034 - accuracy: 0.4387\n",
      "Epoch 22/50\n",
      "14/14 [==============================] - 4s 261ms/step - loss: 0.9890 - accuracy: 0.4364\n",
      "Epoch 23/50\n",
      "14/14 [==============================] - 4s 281ms/step - loss: 0.9758 - accuracy: 0.4444\n",
      "Epoch 24/50\n",
      "14/14 [==============================] - 6s 402ms/step - loss: 0.9644 - accuracy: 0.4513\n",
      "Epoch 25/50\n",
      "14/14 [==============================] - 5s 387ms/step - loss: 0.9534 - accuracy: 0.4273\n",
      "Epoch 26/50\n",
      "14/14 [==============================] - 6s 400ms/step - loss: 0.9417 - accuracy: 0.4387\n",
      "Epoch 27/50\n",
      "14/14 [==============================] - 6s 432ms/step - loss: 0.9320 - accuracy: 0.4307\n",
      "Epoch 28/50\n",
      "14/14 [==============================] - 6s 447ms/step - loss: 0.9211 - accuracy: 0.4399\n",
      "Epoch 29/50\n",
      "14/14 [==============================] - 6s 443ms/step - loss: 0.9134 - accuracy: 0.4192\n",
      "Epoch 30/50\n",
      "14/14 [==============================] - 7s 472ms/step - loss: 0.9017 - accuracy: 0.4192\n",
      "Epoch 31/50\n",
      "14/14 [==============================] - 7s 493ms/step - loss: 0.8913 - accuracy: 0.4502\n",
      "Epoch 32/50\n",
      "14/14 [==============================] - 4s 270ms/step - loss: 0.8817 - accuracy: 0.4387\n",
      "Epoch 33/50\n",
      "14/14 [==============================] - 5s 373ms/step - loss: 0.8724 - accuracy: 0.4502\n",
      "Epoch 34/50\n",
      "14/14 [==============================] - 6s 413ms/step - loss: 0.8642 - accuracy: 0.4341\n",
      "Epoch 35/50\n",
      "14/14 [==============================] - 6s 427ms/step - loss: 0.8556 - accuracy: 0.4582\n",
      "Epoch 36/50\n",
      "14/14 [==============================] - 6s 442ms/step - loss: 0.8470 - accuracy: 0.4422\n",
      "Epoch 37/50\n",
      "14/14 [==============================] - 6s 421ms/step - loss: 0.8411 - accuracy: 0.4284\n",
      "Epoch 38/50\n",
      "14/14 [==============================] - 6s 412ms/step - loss: 0.8311 - accuracy: 0.4525\n",
      "Epoch 39/50\n",
      "14/14 [==============================] - 7s 465ms/step - loss: 0.8241 - accuracy: 0.4467\n",
      "Epoch 40/50\n",
      "14/14 [==============================] - 6s 453ms/step - loss: 0.8170 - accuracy: 0.4456\n",
      "Epoch 41/50\n",
      "14/14 [==============================] - 7s 470ms/step - loss: 0.8112 - accuracy: 0.4490\n",
      "Epoch 42/50\n",
      "14/14 [==============================] - 4s 283ms/step - loss: 0.8062 - accuracy: 0.4353\n",
      "Epoch 43/50\n",
      "14/14 [==============================] - 6s 406ms/step - loss: 0.7995 - accuracy: 0.4089\n",
      "Epoch 44/50\n",
      "14/14 [==============================] - 7s 477ms/step - loss: 0.8029 - accuracy: 0.3448\n",
      "Epoch 45/50\n",
      "14/14 [==============================] - 7s 504ms/step - loss: 0.8116 - accuracy: 0.3471\n",
      "Epoch 46/50\n",
      "14/14 [==============================] - 7s 478ms/step - loss: 0.7963 - accuracy: 0.3940\n",
      "Epoch 47/50\n",
      "14/14 [==============================] - 7s 462ms/step - loss: 0.7803 - accuracy: 0.4273\n",
      "Epoch 48/50\n",
      "14/14 [==============================] - 7s 472ms/step - loss: 0.7686 - accuracy: 0.4502\n",
      "Epoch 49/50\n",
      "14/14 [==============================] - 7s 492ms/step - loss: 0.7600 - accuracy: 0.4318\n",
      "Epoch 50/50\n",
      "14/14 [==============================] - 8s 568ms/step - loss: 0.7525 - accuracy: 0.4433\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "model number: 1, seed number: 69 error: -622831664183.1409\n",
      "Epoch 1/50\n",
      "14/14 [==============================] - 56s 451ms/step - loss: 1.5879 - accuracy: 0.0825\n",
      "Epoch 2/50\n",
      "14/14 [==============================] - 7s 486ms/step - loss: 1.4417 - accuracy: 0.1283\n",
      "Epoch 3/50\n",
      "14/14 [==============================] - 7s 494ms/step - loss: 1.4096 - accuracy: 0.2749\n",
      "Epoch 4/50\n",
      "14/14 [==============================] - 7s 482ms/step - loss: 1.2774 - accuracy: 0.3207\n",
      "Epoch 5/50\n",
      "14/14 [==============================] - 7s 459ms/step - loss: 1.1451 - accuracy: 0.3562\n",
      "Epoch 6/50\n",
      "14/14 [==============================] - 6s 428ms/step - loss: 1.0327 - accuracy: 0.3792\n",
      "Epoch 7/50\n",
      "14/14 [==============================] - 6s 458ms/step - loss: 0.9390 - accuracy: 0.3872\n",
      "Epoch 8/50\n",
      "14/14 [==============================] - 7s 485ms/step - loss: 0.8615 - accuracy: 0.3963\n",
      "Epoch 9/50\n",
      "14/14 [==============================] - 7s 490ms/step - loss: 0.7967 - accuracy: 0.3998\n",
      "Epoch 10/50\n",
      "14/14 [==============================] - 5s 386ms/step - loss: 0.7399 - accuracy: 0.4009\n",
      "Epoch 11/50\n",
      "14/14 [==============================] - 4s 311ms/step - loss: 0.6912 - accuracy: 0.4147\n",
      "Epoch 12/50\n",
      "14/14 [==============================] - 8s 579ms/step - loss: 0.6476 - accuracy: 0.4296\n",
      "Epoch 13/50\n",
      "14/14 [==============================] - 13s 966ms/step - loss: 0.6114 - accuracy: 0.4170\n",
      "Epoch 14/50\n",
      "14/14 [==============================] - 12s 806ms/step - loss: 0.5766 - accuracy: 0.4353\n",
      "Epoch 15/50\n",
      "14/14 [==============================] - 11s 825ms/step - loss: 0.5468 - accuracy: 0.4089\n",
      "Epoch 16/50\n",
      "14/14 [==============================] - 10s 728ms/step - loss: 0.5208 - accuracy: 0.4341\n",
      "Epoch 17/50\n",
      "14/14 [==============================] - 8s 565ms/step - loss: 0.4948 - accuracy: 0.4330\n",
      "Epoch 18/50\n",
      "14/14 [==============================] - 9s 673ms/step - loss: 0.4727 - accuracy: 0.4387\n",
      "Epoch 19/50\n",
      "14/14 [==============================] - 11s 812ms/step - loss: 0.4516 - accuracy: 0.4467\n",
      "Epoch 20/50\n",
      "14/14 [==============================] - 10s 733ms/step - loss: 0.4330 - accuracy: 0.4456\n",
      "Epoch 21/50\n",
      "14/14 [==============================] - 9s 658ms/step - loss: 0.4157 - accuracy: 0.4559\n",
      "Epoch 22/50\n",
      "14/14 [==============================] - 10s 704ms/step - loss: 0.3993 - accuracy: 0.4410\n",
      "Epoch 23/50\n",
      "14/14 [==============================] - 11s 755ms/step - loss: 0.3853 - accuracy: 0.4250\n",
      "Epoch 24/50\n",
      "14/14 [==============================] - 7s 466ms/step - loss: 0.3711 - accuracy: 0.4318\n",
      "Epoch 25/50\n",
      "14/14 [==============================] - 10s 696ms/step - loss: 0.3575 - accuracy: 0.4422\n",
      "Epoch 26/50\n",
      "14/14 [==============================] - 10s 721ms/step - loss: 0.3450 - accuracy: 0.4353\n",
      "Epoch 27/50\n",
      "14/14 [==============================] - 10s 717ms/step - loss: 0.3334 - accuracy: 0.4433\n",
      "Epoch 28/50\n",
      "14/14 [==============================] - 7s 504ms/step - loss: 0.3225 - accuracy: 0.4570\n",
      "Epoch 29/50\n",
      "14/14 [==============================] - 8s 553ms/step - loss: 0.3127 - accuracy: 0.4387\n",
      "Epoch 30/50\n",
      "14/14 [==============================] - 7s 517ms/step - loss: 0.3027 - accuracy: 0.4364\n",
      "Epoch 31/50\n",
      "14/14 [==============================] - 6s 460ms/step - loss: 0.2945 - accuracy: 0.4433\n",
      "Epoch 32/50\n",
      "14/14 [==============================] - 10s 755ms/step - loss: 0.2851 - accuracy: 0.4548\n",
      "Epoch 33/50\n",
      "14/14 [==============================] - 8s 587ms/step - loss: 0.2776 - accuracy: 0.4387\n",
      "Epoch 34/50\n",
      "14/14 [==============================] - 8s 590ms/step - loss: 0.2696 - accuracy: 0.4399\n",
      "Epoch 35/50\n",
      "14/14 [==============================] - 8s 574ms/step - loss: 0.2626 - accuracy: 0.4490\n",
      "Epoch 36/50\n",
      "14/14 [==============================] - 8s 547ms/step - loss: 0.2548 - accuracy: 0.4490\n",
      "Epoch 37/50\n",
      "14/14 [==============================] - 5s 313ms/step - loss: 0.2480 - accuracy: 0.4490\n",
      "Epoch 38/50\n",
      "14/14 [==============================] - 6s 424ms/step - loss: 0.2415 - accuracy: 0.4570\n",
      "Epoch 39/50\n",
      "14/14 [==============================] - 6s 453ms/step - loss: 0.2353 - accuracy: 0.4605\n",
      "Epoch 40/50\n",
      "14/14 [==============================] - 7s 476ms/step - loss: 0.2300 - accuracy: 0.4525\n",
      "Epoch 41/50\n",
      "14/14 [==============================] - 6s 456ms/step - loss: 0.2254 - accuracy: 0.4387\n",
      "Epoch 42/50\n",
      "14/14 [==============================] - 7s 490ms/step - loss: 0.2197 - accuracy: 0.4296\n",
      "Epoch 43/50\n",
      "14/14 [==============================] - 9s 682ms/step - loss: 0.2133 - accuracy: 0.4582\n",
      "Epoch 44/50\n",
      "14/14 [==============================] - 10s 715ms/step - loss: 0.2089 - accuracy: 0.4662\n",
      "Epoch 45/50\n",
      "14/14 [==============================] - 9s 601ms/step - loss: 0.2037 - accuracy: 0.4639\n",
      "Epoch 46/50\n",
      "14/14 [==============================] - 7s 501ms/step - loss: 0.1999 - accuracy: 0.4536\n",
      "Epoch 47/50\n",
      "14/14 [==============================] - 10s 692ms/step - loss: 0.1951 - accuracy: 0.4353\n",
      "Epoch 48/50\n",
      "14/14 [==============================] - 7s 524ms/step - loss: 0.1908 - accuracy: 0.4467\n",
      "Epoch 49/50\n",
      "14/14 [==============================] - 8s 578ms/step - loss: 0.1873 - accuracy: 0.4261\n",
      "Epoch 50/50\n",
      "14/14 [==============================] - 10s 693ms/step - loss: 0.1825 - accuracy: 0.4570\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "1/1 [==============================] - 0s 159ms/step\n",
      "1/1 [==============================] - 0s 141ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "model number: 1, seed number: 70 error: -622831663228.5826\n",
      "Epoch 1/50\n",
      "14/14 [==============================] - 45s 504ms/step - loss: 1.6308 - accuracy: 0.0928\n",
      "Epoch 2/50\n",
      "14/14 [==============================] - 7s 486ms/step - loss: 1.4152 - accuracy: 0.2875\n",
      "Epoch 3/50\n",
      "14/14 [==============================] - 5s 385ms/step - loss: 1.3847 - accuracy: 0.3436\n",
      "Epoch 4/50\n",
      "14/14 [==============================] - 5s 355ms/step - loss: 1.2568 - accuracy: 0.3975\n",
      "Epoch 5/50\n",
      "14/14 [==============================] - 5s 372ms/step - loss: 1.1285 - accuracy: 0.3986\n",
      "Epoch 6/50\n",
      "14/14 [==============================] - 6s 427ms/step - loss: 1.0195 - accuracy: 0.4078\n",
      "Epoch 7/50\n",
      "14/14 [==============================] - 3s 206ms/step - loss: 0.9257 - accuracy: 0.4330\n",
      "Epoch 8/50\n",
      "14/14 [==============================] - 5s 334ms/step - loss: 0.8502 - accuracy: 0.4341\n",
      "Epoch 9/50\n",
      "14/14 [==============================] - 6s 446ms/step - loss: 0.7828 - accuracy: 0.4238\n",
      "Epoch 10/50\n",
      "14/14 [==============================] - 6s 441ms/step - loss: 0.7266 - accuracy: 0.4284\n",
      "Epoch 11/50\n",
      "14/14 [==============================] - 5s 368ms/step - loss: 0.6769 - accuracy: 0.4330\n",
      "Epoch 12/50\n",
      "14/14 [==============================] - 5s 371ms/step - loss: 0.6342 - accuracy: 0.4387\n",
      "Epoch 13/50\n",
      "14/14 [==============================] - 6s 448ms/step - loss: 0.5960 - accuracy: 0.4456\n",
      "Epoch 14/50\n",
      "14/14 [==============================] - 6s 414ms/step - loss: 0.5631 - accuracy: 0.4376\n",
      "Epoch 15/50\n",
      "14/14 [==============================] - 6s 399ms/step - loss: 0.5315 - accuracy: 0.4318\n",
      "Epoch 16/50\n",
      "14/14 [==============================] - 4s 254ms/step - loss: 0.5053 - accuracy: 0.4341\n",
      "Epoch 17/50\n",
      "14/14 [==============================] - 5s 347ms/step - loss: 0.4787 - accuracy: 0.4204\n",
      "Epoch 18/50\n",
      "14/14 [==============================] - 6s 424ms/step - loss: 0.4547 - accuracy: 0.4204\n",
      "Epoch 19/50\n",
      "14/14 [==============================] - 5s 385ms/step - loss: 0.4339 - accuracy: 0.4399\n",
      "Epoch 20/50\n",
      "14/14 [==============================] - 5s 379ms/step - loss: 0.4123 - accuracy: 0.4582\n",
      "Epoch 21/50\n",
      "14/14 [==============================] - 5s 367ms/step - loss: 0.3935 - accuracy: 0.4353\n",
      "Epoch 22/50\n",
      "14/14 [==============================] - 5s 374ms/step - loss: 0.3758 - accuracy: 0.4662\n",
      "Epoch 23/50\n",
      "14/14 [==============================] - 5s 325ms/step - loss: 0.3603 - accuracy: 0.4570\n",
      "Epoch 24/50\n",
      "14/14 [==============================] - 4s 321ms/step - loss: 0.3462 - accuracy: 0.4387\n",
      "Epoch 25/50\n",
      "14/14 [==============================] - 5s 330ms/step - loss: 0.3340 - accuracy: 0.4433\n",
      "Epoch 26/50\n",
      "14/14 [==============================] - 3s 193ms/step - loss: 0.3201 - accuracy: 0.4513\n",
      "Epoch 27/50\n",
      "14/14 [==============================] - 3s 181ms/step - loss: 0.3072 - accuracy: 0.4399\n",
      "Epoch 28/50\n",
      "14/14 [==============================] - 4s 306ms/step - loss: 0.2957 - accuracy: 0.4616\n",
      "Epoch 29/50\n",
      "14/14 [==============================] - 5s 362ms/step - loss: 0.2847 - accuracy: 0.4410\n",
      "Epoch 30/50\n",
      "14/14 [==============================] - 5s 354ms/step - loss: 0.2756 - accuracy: 0.4399\n",
      "Epoch 31/50\n",
      "14/14 [==============================] - 5s 354ms/step - loss: 0.2651 - accuracy: 0.4651\n",
      "Epoch 32/50\n",
      "14/14 [==============================] - 5s 327ms/step - loss: 0.2563 - accuracy: 0.4513\n",
      "Epoch 33/50\n",
      "14/14 [==============================] - 5s 327ms/step - loss: 0.2483 - accuracy: 0.4444\n",
      "Epoch 34/50\n",
      "14/14 [==============================] - 5s 374ms/step - loss: 0.2395 - accuracy: 0.4639\n",
      "Epoch 35/50\n",
      "14/14 [==============================] - 5s 371ms/step - loss: 0.2319 - accuracy: 0.4410\n",
      "Epoch 36/50\n",
      "14/14 [==============================] - 5s 378ms/step - loss: 0.2242 - accuracy: 0.4502\n",
      "Epoch 37/50\n",
      "14/14 [==============================] - 5s 361ms/step - loss: 0.2166 - accuracy: 0.4525\n",
      "Epoch 38/50\n",
      "14/14 [==============================] - 5s 338ms/step - loss: 0.2103 - accuracy: 0.4696\n",
      "Epoch 39/50\n",
      "14/14 [==============================] - 4s 260ms/step - loss: 0.2037 - accuracy: 0.4559\n",
      "Epoch 40/50\n",
      "14/14 [==============================] - 4s 262ms/step - loss: 0.1979 - accuracy: 0.4559\n",
      "Epoch 41/50\n",
      "14/14 [==============================] - 8s 559ms/step - loss: 0.1927 - accuracy: 0.4502\n",
      "Epoch 42/50\n",
      "14/14 [==============================] - 9s 610ms/step - loss: 0.1861 - accuracy: 0.4536\n",
      "Epoch 43/50\n",
      "14/14 [==============================] - 6s 417ms/step - loss: 0.1807 - accuracy: 0.4593\n",
      "Epoch 44/50\n",
      "14/14 [==============================] - 5s 372ms/step - loss: 0.1767 - accuracy: 0.4559\n",
      "Epoch 45/50\n",
      "14/14 [==============================] - 6s 400ms/step - loss: 0.1726 - accuracy: 0.4605\n",
      "Epoch 46/50\n",
      "14/14 [==============================] - 5s 353ms/step - loss: 0.1668 - accuracy: 0.4593\n",
      "Epoch 47/50\n",
      "14/14 [==============================] - 6s 397ms/step - loss: 0.1618 - accuracy: 0.4548\n",
      "Epoch 48/50\n",
      "14/14 [==============================] - 5s 389ms/step - loss: 0.1579 - accuracy: 0.4754\n",
      "Epoch 49/50\n",
      "14/14 [==============================] - 5s 372ms/step - loss: 0.1528 - accuracy: 0.4788\n",
      "Epoch 50/50\n",
      "14/14 [==============================] - 5s 359ms/step - loss: 0.1484 - accuracy: 0.4628\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "model number: 1, seed number: 71 error: -622831672832.6426\n",
      "Epoch 1/50\n",
      "14/14 [==============================] - 46s 395ms/step - loss: 1.6532 - accuracy: 0.0596\n",
      "Epoch 2/50\n",
      "14/14 [==============================] - 5s 377ms/step - loss: 1.4304 - accuracy: 0.1649\n",
      "Epoch 3/50\n",
      "14/14 [==============================] - 6s 399ms/step - loss: 1.3750 - accuracy: 0.2921\n",
      "Epoch 4/50\n",
      "14/14 [==============================] - 4s 278ms/step - loss: 1.2251 - accuracy: 0.3173\n",
      "Epoch 5/50\n",
      "14/14 [==============================] - 4s 254ms/step - loss: 1.0755 - accuracy: 0.3792\n",
      "Epoch 6/50\n",
      "14/14 [==============================] - 4s 253ms/step - loss: 0.9531 - accuracy: 0.3929\n",
      "Epoch 7/50\n",
      "14/14 [==============================] - 5s 357ms/step - loss: 0.8538 - accuracy: 0.3837\n",
      "Epoch 8/50\n",
      "14/14 [==============================] - 5s 343ms/step - loss: 0.7710 - accuracy: 0.3906\n",
      "Epoch 9/50\n",
      "14/14 [==============================] - 6s 404ms/step - loss: 0.7009 - accuracy: 0.4387\n",
      "Epoch 10/50\n",
      "14/14 [==============================] - 7s 477ms/step - loss: 0.6426 - accuracy: 0.4456\n",
      "Epoch 11/50\n",
      "14/14 [==============================] - 6s 455ms/step - loss: 0.5917 - accuracy: 0.4273\n",
      "Epoch 12/50\n",
      "14/14 [==============================] - 7s 492ms/step - loss: 0.5486 - accuracy: 0.4353\n",
      "Epoch 13/50\n",
      "14/14 [==============================] - 8s 549ms/step - loss: 0.5105 - accuracy: 0.4353\n",
      "Epoch 14/50\n",
      "14/14 [==============================] - 6s 427ms/step - loss: 0.4771 - accuracy: 0.4261\n",
      "Epoch 15/50\n",
      "14/14 [==============================] - 13s 922ms/step - loss: 0.4462 - accuracy: 0.4399\n",
      "Epoch 16/50\n",
      "14/14 [==============================] - 13s 904ms/step - loss: 0.4194 - accuracy: 0.4410\n",
      "Epoch 17/50\n",
      "14/14 [==============================] - 14s 1s/step - loss: 0.3945 - accuracy: 0.4433\n",
      "Epoch 18/50\n",
      "14/14 [==============================] - 9s 621ms/step - loss: 0.3717 - accuracy: 0.4513\n",
      "Epoch 19/50\n",
      "14/14 [==============================] - 6s 464ms/step - loss: 0.3517 - accuracy: 0.4765\n",
      "Epoch 20/50\n",
      "14/14 [==============================] - 10s 683ms/step - loss: 0.3340 - accuracy: 0.4536\n",
      "Epoch 21/50\n",
      "14/14 [==============================] - 10s 677ms/step - loss: 0.3167 - accuracy: 0.4582\n",
      "Epoch 22/50\n",
      "14/14 [==============================] - 10s 690ms/step - loss: 0.3008 - accuracy: 0.4639\n",
      "Epoch 23/50\n",
      "14/14 [==============================] - 10s 710ms/step - loss: 0.2870 - accuracy: 0.4513\n",
      "Epoch 24/50\n",
      "14/14 [==============================] - 10s 711ms/step - loss: 0.2733 - accuracy: 0.4570\n",
      "Epoch 25/50\n",
      "14/14 [==============================] - 9s 627ms/step - loss: 0.2605 - accuracy: 0.4639\n",
      "Epoch 26/50\n",
      "14/14 [==============================] - 6s 451ms/step - loss: 0.2514 - accuracy: 0.4444\n",
      "Epoch 27/50\n",
      "14/14 [==============================] - 5s 332ms/step - loss: 0.2398 - accuracy: 0.4651\n",
      "Epoch 28/50\n",
      "14/14 [==============================] - 6s 465ms/step - loss: 0.2301 - accuracy: 0.4662\n",
      "Epoch 29/50\n",
      "14/14 [==============================] - 6s 440ms/step - loss: 0.2201 - accuracy: 0.4696\n",
      "Epoch 30/50\n",
      "14/14 [==============================] - 6s 416ms/step - loss: 0.2107 - accuracy: 0.4513\n",
      "Epoch 31/50\n",
      "14/14 [==============================] - 5s 382ms/step - loss: 0.2035 - accuracy: 0.4536\n",
      "Epoch 32/50\n",
      "14/14 [==============================] - 5s 366ms/step - loss: 0.1950 - accuracy: 0.4719\n",
      "Epoch 33/50\n",
      "14/14 [==============================] - 6s 431ms/step - loss: 0.1887 - accuracy: 0.4605\n",
      "Epoch 34/50\n",
      "14/14 [==============================] - 6s 413ms/step - loss: 0.1805 - accuracy: 0.4777\n",
      "Epoch 35/50\n",
      "14/14 [==============================] - 7s 517ms/step - loss: 0.1742 - accuracy: 0.4696\n",
      "Epoch 36/50\n",
      "14/14 [==============================] - 8s 527ms/step - loss: 0.1676 - accuracy: 0.4605\n",
      "Epoch 37/50\n",
      "14/14 [==============================] - 5s 392ms/step - loss: 0.1630 - accuracy: 0.4593\n",
      "Epoch 38/50\n",
      "14/14 [==============================] - 5s 329ms/step - loss: 0.1564 - accuracy: 0.4536\n",
      "Epoch 39/50\n",
      "14/14 [==============================] - 4s 269ms/step - loss: 0.1511 - accuracy: 0.4834\n",
      "Epoch 40/50\n",
      "14/14 [==============================] - 5s 359ms/step - loss: 0.1476 - accuracy: 0.4662\n",
      "Epoch 41/50\n",
      "14/14 [==============================] - 6s 442ms/step - loss: 0.1471 - accuracy: 0.4582\n",
      "Epoch 42/50\n",
      "14/14 [==============================] - 6s 457ms/step - loss: 0.1441 - accuracy: 0.4399\n",
      "Epoch 43/50\n",
      "14/14 [==============================] - 7s 480ms/step - loss: 0.1371 - accuracy: 0.4834\n",
      "Epoch 44/50\n",
      "14/14 [==============================] - 6s 400ms/step - loss: 0.1306 - accuracy: 0.4754\n",
      "Epoch 45/50\n",
      "14/14 [==============================] - 6s 458ms/step - loss: 0.1265 - accuracy: 0.4696\n",
      "Epoch 46/50\n",
      "14/14 [==============================] - 7s 488ms/step - loss: 0.1219 - accuracy: 0.4845\n",
      "Epoch 47/50\n",
      "14/14 [==============================] - 6s 440ms/step - loss: 0.1184 - accuracy: 0.4742\n",
      "Epoch 48/50\n",
      "14/14 [==============================] - 5s 380ms/step - loss: 0.1151 - accuracy: 0.4685\n",
      "Epoch 49/50\n",
      "14/14 [==============================] - 5s 356ms/step - loss: 0.1121 - accuracy: 0.4788\n",
      "Epoch 50/50\n",
      "14/14 [==============================] - 6s 426ms/step - loss: 0.1082 - accuracy: 0.4548\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "model number: 1, seed number: 72 error: -622831662567.2114\n",
      "Epoch 1/50\n",
      "14/14 [==============================] - 62s 640ms/step - loss: 1.5737 - accuracy: 0.1088\n",
      "Epoch 2/50\n",
      "14/14 [==============================] - 7s 520ms/step - loss: 1.3191 - accuracy: 0.2806\n",
      "Epoch 3/50\n",
      "14/14 [==============================] - 8s 535ms/step - loss: 1.1366 - accuracy: 0.3677\n",
      "Epoch 4/50\n",
      "14/14 [==============================] - 7s 501ms/step - loss: 0.9093 - accuracy: 0.3883\n",
      "Epoch 5/50\n",
      "14/14 [==============================] - 8s 563ms/step - loss: 0.7193 - accuracy: 0.3929\n",
      "Epoch 6/50\n",
      "14/14 [==============================] - 8s 556ms/step - loss: 0.5782 - accuracy: 0.3975\n",
      "Epoch 7/50\n",
      "14/14 [==============================] - 9s 631ms/step - loss: 0.4773 - accuracy: 0.4147\n",
      "Epoch 8/50\n",
      "14/14 [==============================] - 7s 473ms/step - loss: 0.3981 - accuracy: 0.4273\n",
      "Epoch 9/50\n",
      "14/14 [==============================] - 6s 423ms/step - loss: 0.3378 - accuracy: 0.4135\n",
      "Epoch 10/50\n",
      "14/14 [==============================] - 9s 592ms/step - loss: 0.2896 - accuracy: 0.4204\n",
      "Epoch 11/50\n",
      "14/14 [==============================] - 8s 584ms/step - loss: 0.2522 - accuracy: 0.4284\n",
      "Epoch 12/50\n",
      "14/14 [==============================] - 7s 490ms/step - loss: 0.2210 - accuracy: 0.4433\n",
      "Epoch 13/50\n",
      "14/14 [==============================] - 8s 546ms/step - loss: 0.1978 - accuracy: 0.4227\n",
      "Epoch 14/50\n",
      "14/14 [==============================] - 7s 515ms/step - loss: 0.1763 - accuracy: 0.4387\n",
      "Epoch 15/50\n",
      "14/14 [==============================] - 8s 600ms/step - loss: 0.1575 - accuracy: 0.4548\n",
      "Epoch 16/50\n",
      "14/14 [==============================] - 8s 548ms/step - loss: 0.1427 - accuracy: 0.4479\n",
      "Epoch 17/50\n",
      "14/14 [==============================] - 7s 486ms/step - loss: 0.1293 - accuracy: 0.4685\n",
      "Epoch 18/50\n",
      "14/14 [==============================] - 6s 393ms/step - loss: 0.1196 - accuracy: 0.4318\n",
      "Epoch 19/50\n",
      "14/14 [==============================] - 6s 446ms/step - loss: 0.1100 - accuracy: 0.4605\n",
      "Epoch 20/50\n",
      "14/14 [==============================] - 8s 560ms/step - loss: 0.1019 - accuracy: 0.4777\n",
      "Epoch 21/50\n",
      "14/14 [==============================] - 9s 597ms/step - loss: 0.0954 - accuracy: 0.4616\n",
      "Epoch 22/50\n",
      "14/14 [==============================] - 7s 523ms/step - loss: 0.0883 - accuracy: 0.4605\n",
      "Epoch 23/50\n",
      "14/14 [==============================] - 7s 540ms/step - loss: 0.0833 - accuracy: 0.4433\n",
      "Epoch 24/50\n",
      "14/14 [==============================] - 7s 506ms/step - loss: 0.0787 - accuracy: 0.4479\n",
      "Epoch 25/50\n",
      "14/14 [==============================] - 7s 473ms/step - loss: 0.0741 - accuracy: 0.4616\n",
      "Epoch 26/50\n",
      "14/14 [==============================] - 7s 487ms/step - loss: 0.0686 - accuracy: 0.4593\n",
      "Epoch 27/50\n",
      "14/14 [==============================] - 6s 446ms/step - loss: 0.0655 - accuracy: 0.4628\n",
      "Epoch 28/50\n",
      "14/14 [==============================] - 4s 300ms/step - loss: 0.0618 - accuracy: 0.4525\n",
      "Epoch 29/50\n",
      "14/14 [==============================] - 4s 309ms/step - loss: 0.0611 - accuracy: 0.4662\n",
      "Epoch 30/50\n",
      "14/14 [==============================] - 5s 379ms/step - loss: 0.0571 - accuracy: 0.4250\n",
      "Epoch 31/50\n",
      "14/14 [==============================] - 6s 409ms/step - loss: 0.0541 - accuracy: 0.4559\n",
      "Epoch 32/50\n",
      "14/14 [==============================] - 6s 415ms/step - loss: 0.0520 - accuracy: 0.4948\n",
      "Epoch 33/50\n",
      "14/14 [==============================] - 6s 396ms/step - loss: 0.0542 - accuracy: 0.4479\n",
      "Epoch 34/50\n",
      "14/14 [==============================] - 6s 395ms/step - loss: 0.8263 - accuracy: 0.3162\n",
      "Epoch 35/50\n",
      "14/14 [==============================] - 6s 450ms/step - loss: 2.3538 - accuracy: 0.2463\n",
      "Epoch 36/50\n",
      "14/14 [==============================] - 7s 490ms/step - loss: 3.0336 - accuracy: 0.3379\n",
      "Epoch 37/50\n",
      "14/14 [==============================] - 8s 553ms/step - loss: 2.7144 - accuracy: 0.3883\n",
      "Epoch 38/50\n",
      "14/14 [==============================] - 7s 503ms/step - loss: 2.2725 - accuracy: 0.3940\n",
      "Epoch 39/50\n",
      "14/14 [==============================] - 6s 398ms/step - loss: 1.9253 - accuracy: 0.4215\n",
      "Epoch 40/50\n",
      "14/14 [==============================] - 5s 344ms/step - loss: 1.6651 - accuracy: 0.4021\n",
      "Epoch 41/50\n",
      "14/14 [==============================] - 5s 345ms/step - loss: 1.4713 - accuracy: 0.4422\n",
      "Epoch 42/50\n",
      "14/14 [==============================] - 5s 343ms/step - loss: 1.3194 - accuracy: 0.4284\n",
      "Epoch 43/50\n",
      "14/14 [==============================] - 4s 277ms/step - loss: 1.1959 - accuracy: 0.4364\n",
      "Epoch 44/50\n",
      "14/14 [==============================] - 5s 320ms/step - loss: 1.0907 - accuracy: 0.4330\n",
      "Epoch 45/50\n",
      "14/14 [==============================] - 5s 366ms/step - loss: 1.0029 - accuracy: 0.4548\n",
      "Epoch 46/50\n",
      "14/14 [==============================] - 5s 392ms/step - loss: 0.9277 - accuracy: 0.4570\n",
      "Epoch 47/50\n",
      "14/14 [==============================] - 6s 428ms/step - loss: 0.8637 - accuracy: 0.4467\n",
      "Epoch 48/50\n",
      "14/14 [==============================] - 8s 534ms/step - loss: 0.8068 - accuracy: 0.4639\n",
      "Epoch 49/50\n",
      "14/14 [==============================] - 7s 472ms/step - loss: 0.7557 - accuracy: 0.4467\n",
      "Epoch 50/50\n",
      "14/14 [==============================] - 6s 448ms/step - loss: 0.7122 - accuracy: 0.4502\n",
      "1/1 [==============================] - 6s 6s/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[140], line 20\u001b[0m\n\u001b[1;32m     16\u001b[0m model \u001b[38;5;241m=\u001b[39m KerasRegressor(build_fn\u001b[38;5;241m=\u001b[39mcreate_model, random_state\u001b[38;5;241m=\u001b[39mseed, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m     18\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X, y)\n\u001b[0;32m---> 20\u001b[0m model_predictions, _ \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_next_days\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscalers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m error \u001b[38;5;241m=\u001b[39m custom_scoring_validation(validation, model_predictions[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m:])\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel number: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, seed number: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseend_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[137], line 18\u001b[0m, in \u001b[0;36mpredict_next_days\u001b[0;34m(ensemble, feature_dataset, scalers, n_steps, n_features, n_days_to_predict)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_days_to_predict):\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# Predecir 1 da posterior al ltimo da disponible en el dataset\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     last_sequence \u001b[38;5;241m=\u001b[39m future_dataset\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m-\u001b[39mn_steps:, :]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m1\u001b[39m, n_steps, n_features))\n\u001b[0;32m---> 18\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mensemble\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlast_sequence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# Agregar las predicciones sin desnormalizar a future_dataset\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     predicted_values_normalized \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(predictions, columns\u001b[38;5;241m=\u001b[39mfuture_dataset\u001b[38;5;241m.\u001b[39mcolumns)\n",
      "File \u001b[0;32m~/Desktop/Tesis-2024/myenv/lib/python3.8/site-packages/scikeras/wrappers.py:1048\u001b[0m, in \u001b[0;36mBaseWrapper.predict\u001b[0;34m(self, X, **kwargs)\u001b[0m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns predictions for the given test data.\u001b[39;00m\n\u001b[1;32m   1025\u001b[0m \n\u001b[1;32m   1026\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[38;5;124;03m    Predictions, of shape shape (n_samples,) or (n_samples, n_outputs).\u001b[39;00m\n\u001b[1;32m   1046\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# predict with Keras model\u001b[39;00m\n\u001b[0;32m-> 1048\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;66;03m# post process y\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_encoder_\u001b[38;5;241m.\u001b[39minverse_transform(y_pred)\n",
      "File \u001b[0;32m~/Desktop/Tesis-2024/myenv/lib/python3.8/site-packages/scikeras/wrappers.py:994\u001b[0m, in \u001b[0;36mBaseWrapper._predict_raw\u001b[0;34m(self, X, **kwargs)\u001b[0m\n\u001b[1;32m    990\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(\n\u001b[1;32m    991\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEstimator needs to be fit before `predict` \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcan be called\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    992\u001b[0m     )\n\u001b[1;32m    993\u001b[0m \u001b[38;5;66;03m# basic input checks\u001b[39;00m\n\u001b[0;32m--> 994\u001b[0m X, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    996\u001b[0m \u001b[38;5;66;03m# pre process X\u001b[39;00m\n\u001b[1;32m    997\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_encoder_\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/Desktop/Tesis-2024/myenv/lib/python3.8/site-packages/scikeras/wrappers.py:1723\u001b[0m, in \u001b[0;36mKerasRegressor._validate_data\u001b[0;34m(self, X, y, reset, y_numeric)\u001b[0m\n\u001b[1;32m   1716\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_data\u001b[39m(\n\u001b[1;32m   1717\u001b[0m     \u001b[38;5;28mself\u001b[39m, X\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, reset: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, y_numeric: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1718\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[np\u001b[38;5;241m.\u001b[39mndarray, Union[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28;01mNone\u001b[39;00m]]:\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1721\u001b[0m     \u001b[38;5;66;03m# when calling `_validate_data` which will force casting to numeric for\u001b[39;00m\n\u001b[1;32m   1722\u001b[0m     \u001b[38;5;66;03m# non-numeric data.\u001b[39;00m\n\u001b[0;32m-> 1723\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_numeric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Tesis-2024/myenv/lib/python3.8/site-packages/scikeras/wrappers.py:656\u001b[0m, in \u001b[0;36mBaseWrapper._validate_data\u001b[0;34m(self, X, y, reset, y_numeric)\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m isspmatrix(X):\n\u001b[1;32m    652\u001b[0m     \u001b[38;5;66;03m# TensorFlow does not support several of SciPy's sparse formats\u001b[39;00m\n\u001b[1;32m    653\u001b[0m     \u001b[38;5;66;03m# use SciPy to reformat here so at least the cost is known\u001b[39;00m\n\u001b[1;32m    654\u001b[0m     X \u001b[38;5;241m=\u001b[39m lil_matrix(X)  \u001b[38;5;66;03m# no-copy reformat\u001b[39;00m\n\u001b[0;32m--> 656\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_check_array_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_numeric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    662\u001b[0m X_dtype_ \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    663\u001b[0m X_shape_ \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/Desktop/Tesis-2024/myenv/lib/python3.8/site-packages/sklearn/utils/validation.py:921\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    915\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    916\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    917\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    918\u001b[0m         )\n\u001b[1;32m    920\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 921\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m            \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m            \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    929\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[0;32m~/Desktop/Tesis-2024/myenv/lib/python3.8/site-packages/sklearn/utils/validation.py:161\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    147\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m     )\n\u001b[0;32m--> 161\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN."
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "# Leer los hiperparmetros desde el archivo JSON\n",
    "with open('top_5_hyperparameters.json', 'r') as f:\n",
    "    top_hyperparameters = json.load(f)\n",
    "\n",
    "\n",
    "models = []\n",
    "best_seeds= {}\n",
    "prime_seeds = generate_prime_seeds(300)\n",
    "\n",
    "for mode_number, params in enumerate(top_hyperparameters['best_params']):\n",
    "    best_validation_errors = {}\n",
    "    \n",
    "    for seend_number, seed in enumerate(prime_seeds):\n",
    "        model = KerasRegressor(build_fn=create_model, random_state=seed, **params)\n",
    "        \n",
    "        model.fit(X, y)\n",
    "        \n",
    "        model_predictions, _ = predict_next_days(model, feature_dataset, scalers, n_steps, n_features, 5)\n",
    "\n",
    "        error = custom_scoring_validation(validation, model_predictions[-5:])\n",
    "        print(f\"model number: {mode_number}, seed number: {seend_number} error: {error}\")\n",
    "        \n",
    "        if seed not in best_validation_errors or error < best_validation_errors[seed]:\n",
    "            best_validation_errors[seed] = error\n",
    "    \n",
    "    best_seed_for_params = min(best_validation_errors, key=best_validation_errors.get)\n",
    "    best_seeds[str(params)] = best_seed_for_params\n",
    "    \n",
    "    model = KerasRegressor(build_fn=create_model, random_state=best_seed_for_params, **params)\n",
    "    model.fit(X, y)\n",
    "    models.append(model)\n",
    "\n",
    "\n",
    "ensemble = MultivariableVotingRegressor(models)\n",
    "ensemble.fit(X, y)\n",
    "\n",
    "with open('best_seeds.json', 'w') as f:\n",
    "    json.dump(best_seeds, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_dataset = feature_dataset\n",
    "\n",
    "future_dataset, predicted_values_desnormalized = predict_next_days(ensemble, feature_dataset, scalers, n_steps, n_features, n_days_to_predict)\n",
    "\n",
    "print(\"Valores predichos para los prximos {} das:\".format(n_days_to_predict))\n",
    "display(future_dataset.tail(n_days_to_predict + 1))\n",
    "\n",
    "print(\"Valores predichos desnormalizados para los prximos {} das:\".format(n_days_to_predict))\n",
    "display(predicted_values_desnormalized.tail(n_days_to_predict))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rearmado del modelo a partir de las semillas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('best_seeds.json', 'r') as f:\n",
    "    best_seeds = json.load(f)\n",
    "\n",
    "# 21 Crear y entrenar los modelos con los hiperparmetros y semillas guardados\n",
    "models = []\n",
    "for params_str, seed in best_seeds.items():\n",
    "    params = json.loads(params_str.replace(\"'\", \"\\\"\"))\n",
    "    model = KerasRegressor(build_fn=create_model, random_state=seed, **params)\n",
    "    model.fit(X, y)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Predecir 5 das en el futuro con los modelos entrenados\n",
    "ensemble = MultivariableVotingRegressor(models)\n",
    "ensemble.fit(X, y)\n",
    "future_dataset, predicted_values_desnormalized = predict_next_days(ensemble, feature_dataset, scalers, n_steps, n_features, 5)\n",
    "    \n",
    "display(predicted_values_desnormalized.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Light GBM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
