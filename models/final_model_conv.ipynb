{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo de este enfoque, sera generar un/os modelo/s para predecir los atributos del dia siguiente al ultimo disponible en el dataset. Aqui se aplicaran 2 enfoques:\n",
    "\n",
    "- Un modelo que prediga todas las variablse en simultaneo (con el objetivo de captar la interrelacion entre las mismas).\n",
    "- Un modelo que prediga solamente la variable target (incialmente se realizaran pruebas con la variable Close, y luego se procedera a usar la variable Tendencia).\n",
    "\n",
    "Una vez realiza la prediccion de los atributos del dia siguiente, se procedera a realizar la prediccion de la Tendencia/Close, se realimientara el dataset, y se procedere a predecir otro dia, repitiendo esto N veces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-09 08:10:14.410479: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Flatten, Conv1D, MaxPooling1D, BatchNormalization, Add\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pmdarima.arima import auto_arima\n",
    "from statsmodels.tsa.api import VAR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from neuralprophet import NeuralProphet\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
    "import datetime\n",
    "from prophet import Prophet\n",
    "from prophet.diagnostics import performance_metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.base import RegressorMixin\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from sklearn.metrics import make_scorer\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.base import clone\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from skopt import BayesSearchCV\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    'Open',\n",
    "    'High',\n",
    "    'Low',\n",
    "    'Close',\n",
    "    'Number of trades',\n",
    "    'Close_BTCUSDT',\n",
    "    'Volume_BTCUSDT',\n",
    "    'Number_of_trades_BTCUSDT',\n",
    "    'Close_ETHUSDT',\n",
    "    'Volume_ETHUSDT',\n",
    "    'Number_of_trades_ETHUSDT',\n",
    "    'Close_BNBUSDT',\n",
    "    'Volume_BNBUSDT',\n",
    "    'Number_of_trades_BNBUSDT',\n",
    "    'SMA_20',\n",
    "    'EMA_20',\n",
    "    'Upper_Band',\n",
    "    'Middle_Band',\n",
    "    'Lower_Band',\n",
    "    'RSI',\n",
    "    'buy_1000x_high_coinbase',\n",
    "    'sell_1000x_high_coinbase',\n",
    "    'total_trades_coinbase',\t\n",
    "    'Tweets_Utilizados',\n",
    "    'Tweets_Utilizados_coin',\n",
    "    'Tweets_Utilizados_referentes',\n",
    "    'Tweets_Utilizados_whale_alert',\n",
    "    'Buy_1000x_high',\n",
    "    'sell_1000x_high',\n",
    "    'total_trades_binance'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('/Users/mmarchetta/Desktop/Tesis-2024/data-visualization/final_dataset.csv')\n",
    "\n",
    "dataset['Open_time'] = pd.to_datetime(dataset['Open_time'])\n",
    "dates = dataset['Open_time'][:-5]\n",
    "\n",
    "# dataset.drop(['Sentimiento'], axis=1, inplace=True)\n",
    "# dataset.drop(['Sentimiento_coin'], axis=1, inplace=True)\n",
    "# dataset.drop(['Sentimiento_referentes'], axis=1, inplace=True)\n",
    "# dataset.drop(columns=['Open_time'], inplace=True)\n",
    "\n",
    "dataset = dataset.round(2) # Limitar los valores float a 2 decimales en todo el dataframe\n",
    "\n",
    "feature_dataset = dataset[columns]\n",
    "# feature_dataset.drop(['Tendencia'], axis=1, inplace=True)\n",
    "\n",
    "validation = feature_dataset[-5:]\n",
    "feature_dataset = feature_dataset[:-5]\n",
    "\n",
    "n_days_to_predict = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Number of trades</th>\n",
       "      <th>Close_BTCUSDT</th>\n",
       "      <th>Volume_BTCUSDT</th>\n",
       "      <th>Number_of_trades_BTCUSDT</th>\n",
       "      <th>Close_ETHUSDT</th>\n",
       "      <th>Volume_ETHUSDT</th>\n",
       "      <th>Number_of_trades_ETHUSDT</th>\n",
       "      <th>Close_BNBUSDT</th>\n",
       "      <th>Volume_BNBUSDT</th>\n",
       "      <th>Number_of_trades_BNBUSDT</th>\n",
       "      <th>SMA_20</th>\n",
       "      <th>EMA_20</th>\n",
       "      <th>Upper_Band</th>\n",
       "      <th>Middle_Band</th>\n",
       "      <th>Lower_Band</th>\n",
       "      <th>RSI</th>\n",
       "      <th>buy_1000x_high_coinbase</th>\n",
       "      <th>sell_1000x_high_coinbase</th>\n",
       "      <th>total_trades_coinbase</th>\n",
       "      <th>Tweets_Utilizados</th>\n",
       "      <th>Tweets_Utilizados_coin</th>\n",
       "      <th>Tweets_Utilizados_referentes</th>\n",
       "      <th>Tweets_Utilizados_whale_alert</th>\n",
       "      <th>Buy_1000x_high</th>\n",
       "      <th>sell_1000x_high</th>\n",
       "      <th>total_trades_binance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28.84</td>\n",
       "      <td>30.26</td>\n",
       "      <td>27.50</td>\n",
       "      <td>27.71</td>\n",
       "      <td>449178.0</td>\n",
       "      <td>42147.35</td>\n",
       "      <td>39776.84</td>\n",
       "      <td>1001487.0</td>\n",
       "      <td>2925.59</td>\n",
       "      <td>510130.73</td>\n",
       "      <td>1043885.0</td>\n",
       "      <td>335.5</td>\n",
       "      <td>956544.07</td>\n",
       "      <td>457187.0</td>\n",
       "      <td>31.85</td>\n",
       "      <td>30.83</td>\n",
       "      <td>38.30</td>\n",
       "      <td>31.85</td>\n",
       "      <td>25.39</td>\n",
       "      <td>44.11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>139</td>\n",
       "      <td>135</td>\n",
       "      <td>1.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>525.0</td>\n",
       "      <td>364.0</td>\n",
       "      <td>270000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27.72</td>\n",
       "      <td>28.38</td>\n",
       "      <td>26.14</td>\n",
       "      <td>26.31</td>\n",
       "      <td>362304.0</td>\n",
       "      <td>41026.54</td>\n",
       "      <td>43372.26</td>\n",
       "      <td>1045389.0</td>\n",
       "      <td>2804.91</td>\n",
       "      <td>511325.46</td>\n",
       "      <td>928494.0</td>\n",
       "      <td>333.0</td>\n",
       "      <td>922077.23</td>\n",
       "      <td>417006.0</td>\n",
       "      <td>31.77</td>\n",
       "      <td>30.40</td>\n",
       "      <td>38.44</td>\n",
       "      <td>31.77</td>\n",
       "      <td>25.11</td>\n",
       "      <td>41.83</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5273.0</td>\n",
       "      <td>93</td>\n",
       "      <td>122</td>\n",
       "      <td>2.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>472.0</td>\n",
       "      <td>331.0</td>\n",
       "      <td>204000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26.31</td>\n",
       "      <td>28.59</td>\n",
       "      <td>26.11</td>\n",
       "      <td>27.28</td>\n",
       "      <td>376232.0</td>\n",
       "      <td>41524.28</td>\n",
       "      <td>33511.53</td>\n",
       "      <td>884909.0</td>\n",
       "      <td>2850.45</td>\n",
       "      <td>411305.09</td>\n",
       "      <td>748804.0</td>\n",
       "      <td>367.7</td>\n",
       "      <td>1696420.04</td>\n",
       "      <td>653011.0</td>\n",
       "      <td>31.65</td>\n",
       "      <td>30.10</td>\n",
       "      <td>38.55</td>\n",
       "      <td>31.65</td>\n",
       "      <td>24.74</td>\n",
       "      <td>43.99</td>\n",
       "      <td>22.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>54144.0</td>\n",
       "      <td>112</td>\n",
       "      <td>145</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>495.0</td>\n",
       "      <td>216000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27.28</td>\n",
       "      <td>28.99</td>\n",
       "      <td>27.13</td>\n",
       "      <td>28.62</td>\n",
       "      <td>339737.0</td>\n",
       "      <td>43824.10</td>\n",
       "      <td>46381.23</td>\n",
       "      <td>1197815.0</td>\n",
       "      <td>3000.61</td>\n",
       "      <td>506896.76</td>\n",
       "      <td>992243.0</td>\n",
       "      <td>387.5</td>\n",
       "      <td>1163674.21</td>\n",
       "      <td>551245.0</td>\n",
       "      <td>31.62</td>\n",
       "      <td>29.96</td>\n",
       "      <td>38.56</td>\n",
       "      <td>31.62</td>\n",
       "      <td>24.67</td>\n",
       "      <td>46.92</td>\n",
       "      <td>15.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>39220.0</td>\n",
       "      <td>116</td>\n",
       "      <td>147</td>\n",
       "      <td>2.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>419.0</td>\n",
       "      <td>464.0</td>\n",
       "      <td>202000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28.61</td>\n",
       "      <td>32.33</td>\n",
       "      <td>28.50</td>\n",
       "      <td>31.94</td>\n",
       "      <td>735059.0</td>\n",
       "      <td>48141.61</td>\n",
       "      <td>66244.87</td>\n",
       "      <td>1771237.0</td>\n",
       "      <td>3309.91</td>\n",
       "      <td>648714.62</td>\n",
       "      <td>1446386.0</td>\n",
       "      <td>421.5</td>\n",
       "      <td>1440336.04</td>\n",
       "      <td>727854.0</td>\n",
       "      <td>31.64</td>\n",
       "      <td>30.15</td>\n",
       "      <td>38.58</td>\n",
       "      <td>31.64</td>\n",
       "      <td>24.69</td>\n",
       "      <td>53.42</td>\n",
       "      <td>24.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>63183.0</td>\n",
       "      <td>171</td>\n",
       "      <td>141</td>\n",
       "      <td>1.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>477.0</td>\n",
       "      <td>664.0</td>\n",
       "      <td>492000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Open   High    Low  Close  Number of trades  Close_BTCUSDT  \\\n",
       "0  28.84  30.26  27.50  27.71          449178.0       42147.35   \n",
       "1  27.72  28.38  26.14  26.31          362304.0       41026.54   \n",
       "2  26.31  28.59  26.11  27.28          376232.0       41524.28   \n",
       "3  27.28  28.99  27.13  28.62          339737.0       43824.10   \n",
       "4  28.61  32.33  28.50  31.94          735059.0       48141.61   \n",
       "\n",
       "   Volume_BTCUSDT  Number_of_trades_BTCUSDT  Close_ETHUSDT  Volume_ETHUSDT  \\\n",
       "0        39776.84                 1001487.0        2925.59       510130.73   \n",
       "1        43372.26                 1045389.0        2804.91       511325.46   \n",
       "2        33511.53                  884909.0        2850.45       411305.09   \n",
       "3        46381.23                 1197815.0        3000.61       506896.76   \n",
       "4        66244.87                 1771237.0        3309.91       648714.62   \n",
       "\n",
       "   Number_of_trades_ETHUSDT  Close_BNBUSDT  Volume_BNBUSDT  \\\n",
       "0                 1043885.0          335.5       956544.07   \n",
       "1                  928494.0          333.0       922077.23   \n",
       "2                  748804.0          367.7      1696420.04   \n",
       "3                  992243.0          387.5      1163674.21   \n",
       "4                 1446386.0          421.5      1440336.04   \n",
       "\n",
       "   Number_of_trades_BNBUSDT  SMA_20  EMA_20  Upper_Band  Middle_Band  \\\n",
       "0                  457187.0   31.85   30.83       38.30        31.85   \n",
       "1                  417006.0   31.77   30.40       38.44        31.77   \n",
       "2                  653011.0   31.65   30.10       38.55        31.65   \n",
       "3                  551245.0   31.62   29.96       38.56        31.62   \n",
       "4                  727854.0   31.64   30.15       38.58        31.64   \n",
       "\n",
       "   Lower_Band    RSI  buy_1000x_high_coinbase  sell_1000x_high_coinbase  \\\n",
       "0       25.39  44.11                      0.0                       0.0   \n",
       "1       25.11  41.83                      4.0                       2.0   \n",
       "2       24.74  43.99                     22.0                      40.0   \n",
       "3       24.67  46.92                     15.0                      23.0   \n",
       "4       24.69  53.42                     24.0                      35.0   \n",
       "\n",
       "   total_trades_coinbase  Tweets_Utilizados  Tweets_Utilizados_coin  \\\n",
       "0                    0.0                139                     135   \n",
       "1                 5273.0                 93                     122   \n",
       "2                54144.0                112                     145   \n",
       "3                39220.0                116                     147   \n",
       "4                63183.0                171                     141   \n",
       "\n",
       "   Tweets_Utilizados_referentes  Tweets_Utilizados_whale_alert  \\\n",
       "0                           1.0                           81.0   \n",
       "1                           2.0                           87.0   \n",
       "2                           0.0                           64.0   \n",
       "3                           2.0                           77.0   \n",
       "4                           1.0                           71.0   \n",
       "\n",
       "   Buy_1000x_high  sell_1000x_high  total_trades_binance  \n",
       "0           525.0            364.0              270000.0  \n",
       "1           472.0            331.0              204000.0  \n",
       "2           594.0            495.0              216000.0  \n",
       "3           419.0            464.0              202000.0  \n",
       "4           477.0            664.0              492000.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(903, 30)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Number of trades</th>\n",
       "      <th>Close_BTCUSDT</th>\n",
       "      <th>Volume_BTCUSDT</th>\n",
       "      <th>Number_of_trades_BTCUSDT</th>\n",
       "      <th>Close_ETHUSDT</th>\n",
       "      <th>Volume_ETHUSDT</th>\n",
       "      <th>Number_of_trades_ETHUSDT</th>\n",
       "      <th>Close_BNBUSDT</th>\n",
       "      <th>Volume_BNBUSDT</th>\n",
       "      <th>Number_of_trades_BNBUSDT</th>\n",
       "      <th>SMA_20</th>\n",
       "      <th>EMA_20</th>\n",
       "      <th>Upper_Band</th>\n",
       "      <th>Middle_Band</th>\n",
       "      <th>Lower_Band</th>\n",
       "      <th>RSI</th>\n",
       "      <th>buy_1000x_high_coinbase</th>\n",
       "      <th>sell_1000x_high_coinbase</th>\n",
       "      <th>total_trades_coinbase</th>\n",
       "      <th>Tweets_Utilizados</th>\n",
       "      <th>Tweets_Utilizados_coin</th>\n",
       "      <th>Tweets_Utilizados_referentes</th>\n",
       "      <th>Tweets_Utilizados_whale_alert</th>\n",
       "      <th>Buy_1000x_high</th>\n",
       "      <th>sell_1000x_high</th>\n",
       "      <th>total_trades_binance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>903</th>\n",
       "      <td>10.08</td>\n",
       "      <td>10.46</td>\n",
       "      <td>9.60</td>\n",
       "      <td>9.90</td>\n",
       "      <td>245319.0</td>\n",
       "      <td>67609.99</td>\n",
       "      <td>55691.08</td>\n",
       "      <td>2464515.0</td>\n",
       "      <td>3520.46</td>\n",
       "      <td>570901.29</td>\n",
       "      <td>1906387.0</td>\n",
       "      <td>555.4</td>\n",
       "      <td>2284301.81</td>\n",
       "      <td>994512.0</td>\n",
       "      <td>10.06</td>\n",
       "      <td>9.95</td>\n",
       "      <td>11.86</td>\n",
       "      <td>10.06</td>\n",
       "      <td>8.26</td>\n",
       "      <td>52.48</td>\n",
       "      <td>34.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>84706.0</td>\n",
       "      <td>696</td>\n",
       "      <td>471</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>154000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>9.90</td>\n",
       "      <td>9.99</td>\n",
       "      <td>8.60</td>\n",
       "      <td>8.77</td>\n",
       "      <td>341363.0</td>\n",
       "      <td>61937.40</td>\n",
       "      <td>101005.32</td>\n",
       "      <td>3593832.0</td>\n",
       "      <td>3158.64</td>\n",
       "      <td>1049629.69</td>\n",
       "      <td>2647385.0</td>\n",
       "      <td>507.7</td>\n",
       "      <td>2551361.51</td>\n",
       "      <td>1213572.0</td>\n",
       "      <td>10.08</td>\n",
       "      <td>9.84</td>\n",
       "      <td>11.81</td>\n",
       "      <td>10.08</td>\n",
       "      <td>8.35</td>\n",
       "      <td>42.93</td>\n",
       "      <td>120.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>135180.0</td>\n",
       "      <td>961</td>\n",
       "      <td>509</td>\n",
       "      <td>1.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>534.0</td>\n",
       "      <td>433.0</td>\n",
       "      <td>221000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>8.77</td>\n",
       "      <td>9.57</td>\n",
       "      <td>8.49</td>\n",
       "      <td>9.48</td>\n",
       "      <td>267797.0</td>\n",
       "      <td>67840.51</td>\n",
       "      <td>90420.59</td>\n",
       "      <td>3549793.0</td>\n",
       "      <td>3516.53</td>\n",
       "      <td>1207322.82</td>\n",
       "      <td>2987953.0</td>\n",
       "      <td>556.8</td>\n",
       "      <td>1425296.58</td>\n",
       "      <td>809335.0</td>\n",
       "      <td>10.14</td>\n",
       "      <td>9.80</td>\n",
       "      <td>11.68</td>\n",
       "      <td>10.14</td>\n",
       "      <td>8.60</td>\n",
       "      <td>49.21</td>\n",
       "      <td>185.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>112997.0</td>\n",
       "      <td>866</td>\n",
       "      <td>555</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>473.0</td>\n",
       "      <td>386.0</td>\n",
       "      <td>171000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>9.48</td>\n",
       "      <td>9.58</td>\n",
       "      <td>9.07</td>\n",
       "      <td>9.18</td>\n",
       "      <td>156774.0</td>\n",
       "      <td>65501.27</td>\n",
       "      <td>53357.48</td>\n",
       "      <td>2388390.0</td>\n",
       "      <td>3492.85</td>\n",
       "      <td>602755.21</td>\n",
       "      <td>1791989.0</td>\n",
       "      <td>553.8</td>\n",
       "      <td>953921.37</td>\n",
       "      <td>563996.0</td>\n",
       "      <td>10.17</td>\n",
       "      <td>9.74</td>\n",
       "      <td>11.63</td>\n",
       "      <td>10.17</td>\n",
       "      <td>8.71</td>\n",
       "      <td>46.85</td>\n",
       "      <td>64.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>66543.0</td>\n",
       "      <td>692</td>\n",
       "      <td>533</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>101000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>9.18</td>\n",
       "      <td>9.37</td>\n",
       "      <td>8.69</td>\n",
       "      <td>8.94</td>\n",
       "      <td>147578.0</td>\n",
       "      <td>63796.64</td>\n",
       "      <td>51482.38</td>\n",
       "      <td>2492881.0</td>\n",
       "      <td>3336.35</td>\n",
       "      <td>558848.89</td>\n",
       "      <td>1747756.0</td>\n",
       "      <td>553.8</td>\n",
       "      <td>1181298.51</td>\n",
       "      <td>712381.0</td>\n",
       "      <td>10.14</td>\n",
       "      <td>9.67</td>\n",
       "      <td>11.67</td>\n",
       "      <td>10.14</td>\n",
       "      <td>8.62</td>\n",
       "      <td>45.00</td>\n",
       "      <td>57.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>68616.0</td>\n",
       "      <td>681</td>\n",
       "      <td>546</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>92000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Open   High   Low  Close  Number of trades  Close_BTCUSDT  \\\n",
       "903  10.08  10.46  9.60   9.90          245319.0       67609.99   \n",
       "904   9.90   9.99  8.60   8.77          341363.0       61937.40   \n",
       "905   8.77   9.57  8.49   9.48          267797.0       67840.51   \n",
       "906   9.48   9.58  9.07   9.18          156774.0       65501.27   \n",
       "907   9.18   9.37  8.69   8.94          147578.0       63796.64   \n",
       "\n",
       "     Volume_BTCUSDT  Number_of_trades_BTCUSDT  Close_ETHUSDT  Volume_ETHUSDT  \\\n",
       "903        55691.08                 2464515.0        3520.46       570901.29   \n",
       "904       101005.32                 3593832.0        3158.64      1049629.69   \n",
       "905        90420.59                 3549793.0        3516.53      1207322.82   \n",
       "906        53357.48                 2388390.0        3492.85       602755.21   \n",
       "907        51482.38                 2492881.0        3336.35       558848.89   \n",
       "\n",
       "     Number_of_trades_ETHUSDT  Close_BNBUSDT  Volume_BNBUSDT  \\\n",
       "903                 1906387.0          555.4      2284301.81   \n",
       "904                 2647385.0          507.7      2551361.51   \n",
       "905                 2987953.0          556.8      1425296.58   \n",
       "906                 1791989.0          553.8       953921.37   \n",
       "907                 1747756.0          553.8      1181298.51   \n",
       "\n",
       "     Number_of_trades_BNBUSDT  SMA_20  EMA_20  Upper_Band  Middle_Band  \\\n",
       "903                  994512.0   10.06    9.95       11.86        10.06   \n",
       "904                 1213572.0   10.08    9.84       11.81        10.08   \n",
       "905                  809335.0   10.14    9.80       11.68        10.14   \n",
       "906                  563996.0   10.17    9.74       11.63        10.17   \n",
       "907                  712381.0   10.14    9.67       11.67        10.14   \n",
       "\n",
       "     Lower_Band    RSI  buy_1000x_high_coinbase  sell_1000x_high_coinbase  \\\n",
       "903        8.26  52.48                     34.0                      43.0   \n",
       "904        8.35  42.93                    120.0                     126.0   \n",
       "905        8.60  49.21                    185.0                     117.0   \n",
       "906        8.71  46.85                     64.0                      81.0   \n",
       "907        8.62  45.00                     57.0                      66.0   \n",
       "\n",
       "     total_trades_coinbase  Tweets_Utilizados  Tweets_Utilizados_coin  \\\n",
       "903                84706.0                696                     471   \n",
       "904               135180.0                961                     509   \n",
       "905               112997.0                866                     555   \n",
       "906                66543.0                692                     533   \n",
       "907                68616.0                681                     546   \n",
       "\n",
       "     Tweets_Utilizados_referentes  Tweets_Utilizados_whale_alert  \\\n",
       "903                           0.0                           43.0   \n",
       "904                           1.0                           56.0   \n",
       "905                           1.0                           40.0   \n",
       "906                           0.0                           24.0   \n",
       "907                           0.0                           41.0   \n",
       "\n",
       "     Buy_1000x_high  sell_1000x_high  total_trades_binance  \n",
       "903           343.0            228.0              154000.0  \n",
       "904           534.0            433.0              221000.0  \n",
       "905           473.0            386.0              171000.0  \n",
       "906           350.0            290.0              101000.0  \n",
       "907           252.0            206.0               92000.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(5, 30)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(feature_dataset.head())\n",
    "print(feature_dataset.shape)\n",
    "\n",
    "display(validation.head())\n",
    "display(validation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers = {}\n",
    "for col in feature_dataset.columns:\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    feature_dataset[col] = scaler.fit_transform(np.array(feature_dataset[col]).reshape(-1, 1))\n",
    "    scalers[col] = scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Number of trades</th>\n",
       "      <th>Close_BTCUSDT</th>\n",
       "      <th>Volume_BTCUSDT</th>\n",
       "      <th>Number_of_trades_BTCUSDT</th>\n",
       "      <th>Close_ETHUSDT</th>\n",
       "      <th>Volume_ETHUSDT</th>\n",
       "      <th>Number_of_trades_ETHUSDT</th>\n",
       "      <th>Close_BNBUSDT</th>\n",
       "      <th>Volume_BNBUSDT</th>\n",
       "      <th>Number_of_trades_BNBUSDT</th>\n",
       "      <th>SMA_20</th>\n",
       "      <th>EMA_20</th>\n",
       "      <th>Upper_Band</th>\n",
       "      <th>Middle_Band</th>\n",
       "      <th>Lower_Band</th>\n",
       "      <th>RSI</th>\n",
       "      <th>buy_1000x_high_coinbase</th>\n",
       "      <th>sell_1000x_high_coinbase</th>\n",
       "      <th>total_trades_coinbase</th>\n",
       "      <th>Tweets_Utilizados</th>\n",
       "      <th>Tweets_Utilizados_coin</th>\n",
       "      <th>Tweets_Utilizados_referentes</th>\n",
       "      <th>Tweets_Utilizados_whale_alert</th>\n",
       "      <th>Buy_1000x_high</th>\n",
       "      <th>sell_1000x_high</th>\n",
       "      <th>total_trades_binance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.501993</td>\n",
       "      <td>0.517208</td>\n",
       "      <td>0.492593</td>\n",
       "      <td>0.479569</td>\n",
       "      <td>0.168108</td>\n",
       "      <td>0.460212</td>\n",
       "      <td>0.040979</td>\n",
       "      <td>0.046312</td>\n",
       "      <td>0.506304</td>\n",
       "      <td>0.126579</td>\n",
       "      <td>0.285966</td>\n",
       "      <td>0.303395</td>\n",
       "      <td>0.197596</td>\n",
       "      <td>0.238897</td>\n",
       "      <td>0.633771</td>\n",
       "      <td>0.615034</td>\n",
       "      <td>0.648008</td>\n",
       "      <td>0.633771</td>\n",
       "      <td>0.602961</td>\n",
       "      <td>0.345342</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.123457</td>\n",
       "      <td>0.136662</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.536424</td>\n",
       "      <td>0.302018</td>\n",
       "      <td>0.212508</td>\n",
       "      <td>0.140182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.479673</td>\n",
       "      <td>0.480653</td>\n",
       "      <td>0.464609</td>\n",
       "      <td>0.451664</td>\n",
       "      <td>0.134757</td>\n",
       "      <td>0.440649</td>\n",
       "      <td>0.045762</td>\n",
       "      <td>0.049256</td>\n",
       "      <td>0.474653</td>\n",
       "      <td>0.126913</td>\n",
       "      <td>0.248880</td>\n",
       "      <td>0.297919</td>\n",
       "      <td>0.189790</td>\n",
       "      <td>0.215055</td>\n",
       "      <td>0.631960</td>\n",
       "      <td>0.605239</td>\n",
       "      <td>0.650664</td>\n",
       "      <td>0.631960</td>\n",
       "      <td>0.595284</td>\n",
       "      <td>0.307138</td>\n",
       "      <td>0.016878</td>\n",
       "      <td>0.007092</td>\n",
       "      <td>0.026035</td>\n",
       "      <td>0.079772</td>\n",
       "      <td>0.119580</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.576159</td>\n",
       "      <td>0.268600</td>\n",
       "      <td>0.191449</td>\n",
       "      <td>0.104869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.451574</td>\n",
       "      <td>0.484737</td>\n",
       "      <td>0.463992</td>\n",
       "      <td>0.470999</td>\n",
       "      <td>0.140104</td>\n",
       "      <td>0.449336</td>\n",
       "      <td>0.032645</td>\n",
       "      <td>0.038494</td>\n",
       "      <td>0.486597</td>\n",
       "      <td>0.098880</td>\n",
       "      <td>0.191129</td>\n",
       "      <td>0.373932</td>\n",
       "      <td>0.365156</td>\n",
       "      <td>0.355091</td>\n",
       "      <td>0.629244</td>\n",
       "      <td>0.598405</td>\n",
       "      <td>0.652751</td>\n",
       "      <td>0.629244</td>\n",
       "      <td>0.585138</td>\n",
       "      <td>0.343331</td>\n",
       "      <td>0.092827</td>\n",
       "      <td>0.141844</td>\n",
       "      <td>0.267329</td>\n",
       "      <td>0.097816</td>\n",
       "      <td>0.149803</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.423841</td>\n",
       "      <td>0.345523</td>\n",
       "      <td>0.296107</td>\n",
       "      <td>0.111289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.470905</td>\n",
       "      <td>0.492514</td>\n",
       "      <td>0.484979</td>\n",
       "      <td>0.497708</td>\n",
       "      <td>0.126093</td>\n",
       "      <td>0.489479</td>\n",
       "      <td>0.049765</td>\n",
       "      <td>0.059477</td>\n",
       "      <td>0.525979</td>\n",
       "      <td>0.125672</td>\n",
       "      <td>0.269369</td>\n",
       "      <td>0.417306</td>\n",
       "      <td>0.244504</td>\n",
       "      <td>0.294707</td>\n",
       "      <td>0.628565</td>\n",
       "      <td>0.595216</td>\n",
       "      <td>0.652941</td>\n",
       "      <td>0.628565</td>\n",
       "      <td>0.583219</td>\n",
       "      <td>0.392426</td>\n",
       "      <td>0.063291</td>\n",
       "      <td>0.081560</td>\n",
       "      <td>0.193644</td>\n",
       "      <td>0.101614</td>\n",
       "      <td>0.152431</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.509934</td>\n",
       "      <td>0.235183</td>\n",
       "      <td>0.276324</td>\n",
       "      <td>0.103799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.497409</td>\n",
       "      <td>0.557457</td>\n",
       "      <td>0.513169</td>\n",
       "      <td>0.563883</td>\n",
       "      <td>0.277858</td>\n",
       "      <td>0.564840</td>\n",
       "      <td>0.076188</td>\n",
       "      <td>0.097929</td>\n",
       "      <td>0.607100</td>\n",
       "      <td>0.165421</td>\n",
       "      <td>0.415326</td>\n",
       "      <td>0.491785</td>\n",
       "      <td>0.307160</td>\n",
       "      <td>0.399500</td>\n",
       "      <td>0.629018</td>\n",
       "      <td>0.599544</td>\n",
       "      <td>0.653321</td>\n",
       "      <td>0.629018</td>\n",
       "      <td>0.583767</td>\n",
       "      <td>0.501340</td>\n",
       "      <td>0.101266</td>\n",
       "      <td>0.124113</td>\n",
       "      <td>0.311958</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.144547</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.470199</td>\n",
       "      <td>0.271753</td>\n",
       "      <td>0.403957</td>\n",
       "      <td>0.258962</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Open      High       Low     Close  Number of trades  Close_BTCUSDT  \\\n",
       "0  0.501993  0.517208  0.492593  0.479569          0.168108       0.460212   \n",
       "1  0.479673  0.480653  0.464609  0.451664          0.134757       0.440649   \n",
       "2  0.451574  0.484737  0.463992  0.470999          0.140104       0.449336   \n",
       "3  0.470905  0.492514  0.484979  0.497708          0.126093       0.489479   \n",
       "4  0.497409  0.557457  0.513169  0.563883          0.277858       0.564840   \n",
       "\n",
       "   Volume_BTCUSDT  Number_of_trades_BTCUSDT  Close_ETHUSDT  Volume_ETHUSDT  \\\n",
       "0        0.040979                  0.046312       0.506304        0.126579   \n",
       "1        0.045762                  0.049256       0.474653        0.126913   \n",
       "2        0.032645                  0.038494       0.486597        0.098880   \n",
       "3        0.049765                  0.059477       0.525979        0.125672   \n",
       "4        0.076188                  0.097929       0.607100        0.165421   \n",
       "\n",
       "   Number_of_trades_ETHUSDT  Close_BNBUSDT  Volume_BNBUSDT  \\\n",
       "0                  0.285966       0.303395        0.197596   \n",
       "1                  0.248880       0.297919        0.189790   \n",
       "2                  0.191129       0.373932        0.365156   \n",
       "3                  0.269369       0.417306        0.244504   \n",
       "4                  0.415326       0.491785        0.307160   \n",
       "\n",
       "   Number_of_trades_BNBUSDT    SMA_20    EMA_20  Upper_Band  Middle_Band  \\\n",
       "0                  0.238897  0.633771  0.615034    0.648008     0.633771   \n",
       "1                  0.215055  0.631960  0.605239    0.650664     0.631960   \n",
       "2                  0.355091  0.629244  0.598405    0.652751     0.629244   \n",
       "3                  0.294707  0.628565  0.595216    0.652941     0.628565   \n",
       "4                  0.399500  0.629018  0.599544    0.653321     0.629018   \n",
       "\n",
       "   Lower_Band       RSI  buy_1000x_high_coinbase  sell_1000x_high_coinbase  \\\n",
       "0    0.602961  0.345342                 0.000000                  0.000000   \n",
       "1    0.595284  0.307138                 0.016878                  0.007092   \n",
       "2    0.585138  0.343331                 0.092827                  0.141844   \n",
       "3    0.583219  0.392426                 0.063291                  0.081560   \n",
       "4    0.583767  0.501340                 0.101266                  0.124113   \n",
       "\n",
       "   total_trades_coinbase  Tweets_Utilizados  Tweets_Utilizados_coin  \\\n",
       "0               0.000000           0.123457                0.136662   \n",
       "1               0.026035           0.079772                0.119580   \n",
       "2               0.267329           0.097816                0.149803   \n",
       "3               0.193644           0.101614                0.152431   \n",
       "4               0.311958           0.153846                0.144547   \n",
       "\n",
       "   Tweets_Utilizados_referentes  Tweets_Utilizados_whale_alert  \\\n",
       "0                      0.058824                       0.536424   \n",
       "1                      0.117647                       0.576159   \n",
       "2                      0.000000                       0.423841   \n",
       "3                      0.117647                       0.509934   \n",
       "4                      0.058824                       0.470199   \n",
       "\n",
       "   Buy_1000x_high  sell_1000x_high  total_trades_binance  \n",
       "0        0.302018         0.212508              0.140182  \n",
       "1        0.268600         0.191449              0.104869  \n",
       "2        0.345523         0.296107              0.111289  \n",
       "3        0.235183         0.276324              0.103799  \n",
       "4        0.271753         0.403957              0.258962  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(903, 30)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Number of trades</th>\n",
       "      <th>Close_BTCUSDT</th>\n",
       "      <th>Volume_BTCUSDT</th>\n",
       "      <th>Number_of_trades_BTCUSDT</th>\n",
       "      <th>Close_ETHUSDT</th>\n",
       "      <th>Volume_ETHUSDT</th>\n",
       "      <th>Number_of_trades_ETHUSDT</th>\n",
       "      <th>Close_BNBUSDT</th>\n",
       "      <th>Volume_BNBUSDT</th>\n",
       "      <th>Number_of_trades_BNBUSDT</th>\n",
       "      <th>SMA_20</th>\n",
       "      <th>EMA_20</th>\n",
       "      <th>Upper_Band</th>\n",
       "      <th>Middle_Band</th>\n",
       "      <th>Lower_Band</th>\n",
       "      <th>RSI</th>\n",
       "      <th>buy_1000x_high_coinbase</th>\n",
       "      <th>sell_1000x_high_coinbase</th>\n",
       "      <th>total_trades_coinbase</th>\n",
       "      <th>Tweets_Utilizados</th>\n",
       "      <th>Tweets_Utilizados_coin</th>\n",
       "      <th>Tweets_Utilizados_referentes</th>\n",
       "      <th>Tweets_Utilizados_whale_alert</th>\n",
       "      <th>Buy_1000x_high</th>\n",
       "      <th>sell_1000x_high</th>\n",
       "      <th>total_trades_binance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>903</th>\n",
       "      <td>10.08</td>\n",
       "      <td>10.46</td>\n",
       "      <td>9.60</td>\n",
       "      <td>9.90</td>\n",
       "      <td>245319.0</td>\n",
       "      <td>67609.99</td>\n",
       "      <td>55691.08</td>\n",
       "      <td>2464515.0</td>\n",
       "      <td>3520.46</td>\n",
       "      <td>570901.29</td>\n",
       "      <td>1906387.0</td>\n",
       "      <td>555.4</td>\n",
       "      <td>2284301.81</td>\n",
       "      <td>994512.0</td>\n",
       "      <td>10.06</td>\n",
       "      <td>9.95</td>\n",
       "      <td>11.86</td>\n",
       "      <td>10.06</td>\n",
       "      <td>8.26</td>\n",
       "      <td>52.48</td>\n",
       "      <td>34.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>84706.0</td>\n",
       "      <td>696</td>\n",
       "      <td>471</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>154000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>9.90</td>\n",
       "      <td>9.99</td>\n",
       "      <td>8.60</td>\n",
       "      <td>8.77</td>\n",
       "      <td>341363.0</td>\n",
       "      <td>61937.40</td>\n",
       "      <td>101005.32</td>\n",
       "      <td>3593832.0</td>\n",
       "      <td>3158.64</td>\n",
       "      <td>1049629.69</td>\n",
       "      <td>2647385.0</td>\n",
       "      <td>507.7</td>\n",
       "      <td>2551361.51</td>\n",
       "      <td>1213572.0</td>\n",
       "      <td>10.08</td>\n",
       "      <td>9.84</td>\n",
       "      <td>11.81</td>\n",
       "      <td>10.08</td>\n",
       "      <td>8.35</td>\n",
       "      <td>42.93</td>\n",
       "      <td>120.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>135180.0</td>\n",
       "      <td>961</td>\n",
       "      <td>509</td>\n",
       "      <td>1.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>534.0</td>\n",
       "      <td>433.0</td>\n",
       "      <td>221000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>8.77</td>\n",
       "      <td>9.57</td>\n",
       "      <td>8.49</td>\n",
       "      <td>9.48</td>\n",
       "      <td>267797.0</td>\n",
       "      <td>67840.51</td>\n",
       "      <td>90420.59</td>\n",
       "      <td>3549793.0</td>\n",
       "      <td>3516.53</td>\n",
       "      <td>1207322.82</td>\n",
       "      <td>2987953.0</td>\n",
       "      <td>556.8</td>\n",
       "      <td>1425296.58</td>\n",
       "      <td>809335.0</td>\n",
       "      <td>10.14</td>\n",
       "      <td>9.80</td>\n",
       "      <td>11.68</td>\n",
       "      <td>10.14</td>\n",
       "      <td>8.60</td>\n",
       "      <td>49.21</td>\n",
       "      <td>185.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>112997.0</td>\n",
       "      <td>866</td>\n",
       "      <td>555</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>473.0</td>\n",
       "      <td>386.0</td>\n",
       "      <td>171000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>9.48</td>\n",
       "      <td>9.58</td>\n",
       "      <td>9.07</td>\n",
       "      <td>9.18</td>\n",
       "      <td>156774.0</td>\n",
       "      <td>65501.27</td>\n",
       "      <td>53357.48</td>\n",
       "      <td>2388390.0</td>\n",
       "      <td>3492.85</td>\n",
       "      <td>602755.21</td>\n",
       "      <td>1791989.0</td>\n",
       "      <td>553.8</td>\n",
       "      <td>953921.37</td>\n",
       "      <td>563996.0</td>\n",
       "      <td>10.17</td>\n",
       "      <td>9.74</td>\n",
       "      <td>11.63</td>\n",
       "      <td>10.17</td>\n",
       "      <td>8.71</td>\n",
       "      <td>46.85</td>\n",
       "      <td>64.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>66543.0</td>\n",
       "      <td>692</td>\n",
       "      <td>533</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>101000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>9.18</td>\n",
       "      <td>9.37</td>\n",
       "      <td>8.69</td>\n",
       "      <td>8.94</td>\n",
       "      <td>147578.0</td>\n",
       "      <td>63796.64</td>\n",
       "      <td>51482.38</td>\n",
       "      <td>2492881.0</td>\n",
       "      <td>3336.35</td>\n",
       "      <td>558848.89</td>\n",
       "      <td>1747756.0</td>\n",
       "      <td>553.8</td>\n",
       "      <td>1181298.51</td>\n",
       "      <td>712381.0</td>\n",
       "      <td>10.14</td>\n",
       "      <td>9.67</td>\n",
       "      <td>11.67</td>\n",
       "      <td>10.14</td>\n",
       "      <td>8.62</td>\n",
       "      <td>45.00</td>\n",
       "      <td>57.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>68616.0</td>\n",
       "      <td>681</td>\n",
       "      <td>546</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>92000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Open   High   Low  Close  Number of trades  Close_BTCUSDT  \\\n",
       "903  10.08  10.46  9.60   9.90          245319.0       67609.99   \n",
       "904   9.90   9.99  8.60   8.77          341363.0       61937.40   \n",
       "905   8.77   9.57  8.49   9.48          267797.0       67840.51   \n",
       "906   9.48   9.58  9.07   9.18          156774.0       65501.27   \n",
       "907   9.18   9.37  8.69   8.94          147578.0       63796.64   \n",
       "\n",
       "     Volume_BTCUSDT  Number_of_trades_BTCUSDT  Close_ETHUSDT  Volume_ETHUSDT  \\\n",
       "903        55691.08                 2464515.0        3520.46       570901.29   \n",
       "904       101005.32                 3593832.0        3158.64      1049629.69   \n",
       "905        90420.59                 3549793.0        3516.53      1207322.82   \n",
       "906        53357.48                 2388390.0        3492.85       602755.21   \n",
       "907        51482.38                 2492881.0        3336.35       558848.89   \n",
       "\n",
       "     Number_of_trades_ETHUSDT  Close_BNBUSDT  Volume_BNBUSDT  \\\n",
       "903                 1906387.0          555.4      2284301.81   \n",
       "904                 2647385.0          507.7      2551361.51   \n",
       "905                 2987953.0          556.8      1425296.58   \n",
       "906                 1791989.0          553.8       953921.37   \n",
       "907                 1747756.0          553.8      1181298.51   \n",
       "\n",
       "     Number_of_trades_BNBUSDT  SMA_20  EMA_20  Upper_Band  Middle_Band  \\\n",
       "903                  994512.0   10.06    9.95       11.86        10.06   \n",
       "904                 1213572.0   10.08    9.84       11.81        10.08   \n",
       "905                  809335.0   10.14    9.80       11.68        10.14   \n",
       "906                  563996.0   10.17    9.74       11.63        10.17   \n",
       "907                  712381.0   10.14    9.67       11.67        10.14   \n",
       "\n",
       "     Lower_Band    RSI  buy_1000x_high_coinbase  sell_1000x_high_coinbase  \\\n",
       "903        8.26  52.48                     34.0                      43.0   \n",
       "904        8.35  42.93                    120.0                     126.0   \n",
       "905        8.60  49.21                    185.0                     117.0   \n",
       "906        8.71  46.85                     64.0                      81.0   \n",
       "907        8.62  45.00                     57.0                      66.0   \n",
       "\n",
       "     total_trades_coinbase  Tweets_Utilizados  Tweets_Utilizados_coin  \\\n",
       "903                84706.0                696                     471   \n",
       "904               135180.0                961                     509   \n",
       "905               112997.0                866                     555   \n",
       "906                66543.0                692                     533   \n",
       "907                68616.0                681                     546   \n",
       "\n",
       "     Tweets_Utilizados_referentes  Tweets_Utilizados_whale_alert  \\\n",
       "903                           0.0                           43.0   \n",
       "904                           1.0                           56.0   \n",
       "905                           1.0                           40.0   \n",
       "906                           0.0                           24.0   \n",
       "907                           0.0                           41.0   \n",
       "\n",
       "     Buy_1000x_high  sell_1000x_high  total_trades_binance  \n",
       "903           343.0            228.0              154000.0  \n",
       "904           534.0            433.0              221000.0  \n",
       "905           473.0            386.0              171000.0  \n",
       "906           350.0            290.0              101000.0  \n",
       "907           252.0            206.0               92000.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(5, 30)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(feature_dataset.head())\n",
    "print(feature_dataset.shape)\n",
    "\n",
    "display(validation.head())\n",
    "display(validation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparo el dataset para train: cada conjunto de entrenamiento, sera una seried de N dias previos, para predecir 1 dia siguiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, n_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - n_steps):\n",
    "        end_ix = i + n_steps\n",
    "        seq_x = data.iloc[i:end_ix, :].values\n",
    "        seq_y = data.iloc[end_ix, :].values\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "n_steps = 30  # Longitud de la secuencia de entrada\n",
    "n_features = feature_dataset.shape[1]  # Nmero de caractersticas\n",
    "\n",
    "# Crear las secuencias de entrada y salida\n",
    "X, y = create_sequences(feature_dataset, n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 30)\n",
      "(30,)\n",
      "(873, 30, 30)\n",
      "(873, 30)\n",
      "(903, 30)\n"
     ]
    }
   ],
   "source": [
    "print(X[0].shape) # Cada dato de entrenamiento, es un conjunto de 30 dias con sus 64 features\n",
    "print(y[0].shape) # El target de cada dato, son los 64 features del dia siguiente\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(feature_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "873\n",
      "873\n"
     ]
    }
   ],
   "source": [
    "print(len(X))\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtencion de los mejores hiperparametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_scoring(estimator, X, y):\n",
    "    y_pred = estimator.predict(X)\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    return -mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_scoring_validation(y, y_pred):\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    return -mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vmse(y_true, y_pred):\n",
    "    return K.mean(K.square(y_true - y_pred), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7febe9d46700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fec29919f70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "# Ultimos cambios:\n",
    "    # Se agrego early stopping\n",
    "    # Se agregaron los optimizers a la optimizacion bayesiana\n",
    "    # Se cambio la funcion de loss de mse a mae\n",
    "    # Se cambio la arquitectura de la red:\n",
    "        # dado que agrego batchNormalization, se omite dropout\n",
    "        # Cambio la cantidad de units (Ahora todas las capas tienen la misma cantidad)\n",
    "        # La profundida de la red es parte de la optimizacion\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def create_model(activation, units, dropout, learning_rate, l2_penalty, depth, optimizer='adam'):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(units, kernel_size=3, activation=activation, input_shape=(n_steps, n_features)))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    for _ in range(depth - 1):\n",
    "        model.add(Conv1D(units, kernel_size=3, activation=activation))\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "        model.add(Dropout(dropout))\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "    # model.add(Conv1D(units, kernel_size=3, activation=activation))\n",
    "    # model.add(MaxPooling1D(pool_size=2))\n",
    "    # model.add(Dropout(dropout))\n",
    "    \n",
    "    # model.add(Conv1D(units, kernel_size=3, activation=activation))\n",
    "    # model.add(MaxPooling1D(pool_size=2))\n",
    "    # model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units, activation=activation, kernel_regularizer=l2(l2_penalty)))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(units=n_features))\n",
    "    \n",
    "    # model.add(Conv1D(units, kernel_size=kernel_size, activation=activation, input_shape=(n_steps, n_features)))\n",
    "    # model.add(BatchNormalization())\n",
    "    \n",
    "    # for _ in range(depth - 1):\n",
    "    #     model.add(Conv1D(units, kernel_size=kernel_size, activation=activation))\n",
    "    #     model.add(BatchNormalization())\n",
    "    #     model.add(MaxPooling1D(pool_size=2))\n",
    "        \n",
    "    # model.add(Flatten())\n",
    "    # model.add(Dense(units, activation=activation, kernel_regularizer=l2(l2_penalty)))\n",
    "    # model.add(BatchNormalization())\n",
    "    \n",
    "    # # Capa de salida\n",
    "    # model.add(Dense(units=n_features))\n",
    "    \n",
    "    if optimizer == 'adam':\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == 'rmsprop':\n",
    "        optimizer = RMSprop(learning_rate=learning_rate)\n",
    "    elif optimizer == 'sgd':\n",
    "        optimizer = SGD(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=vmse, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "regressor = KerasRegressor(build_fn=create_model, verbose=0, activation='relu', units=50, dropout=0.2, learning_rate=0.1, l2_penalty=0.001, depth=2, optimizer='adam')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "cv = TimeSeriesSplit(n_splits=20).split(X)\n",
    "param_space = {\n",
    "    'depth': [2, 3],\n",
    "    # 'kernel_size': [3, 6, 9],\n",
    "    'activation': ['relu', 'tanh', 'swish', 'selu'],\n",
    "    'units': [64, 128, 256, 512],\n",
    "    'dropout': [0.1, 0.2, 0.3, 0.4],\n",
    "    'learning_rate': [0.01, 0.001, 0.0001],\n",
    "    'epochs': [10, 20, 30, 50, 100],\n",
    "    'batch_size': [32, 64, 128],\n",
    "    'optimizer': ['adam', 'rmsprop', 'sgd'],\n",
    "    'l2_penalty': [0.001, 0.01, 0.1]\n",
    "}\n",
    "\n",
    "bayes_search = BayesSearchCV(regressor, param_space, scoring=custom_scoring, cv=cv, verbose=0)\n",
    "bayes_result = bayes_search.fit(X, y, callbacks = [early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: -0.014945430524693826\n",
      "Best parameters: OrderedDict([('activation', 'tanh'), ('batch_size', 32), ('depth', 3), ('dropout', 0.4), ('epochs', 50), ('l2_penalty', 0.01), ('learning_rate', 0.001), ('optimizer', 'rmsprop'), ('units', 256)])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KerasRegressor(\n",
       "\tmodel=None\n",
       "\tbuild_fn=&lt;function create_model at 0x7fec388cc430&gt;\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=rmsprop\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=32\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=0\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=50\n",
       "\tactivation=tanh\n",
       "\tunits=256\n",
       "\tdropout=0.4\n",
       "\tlearning_rate=0.001\n",
       "\tl2_penalty=0.01\n",
       "\tdepth=3\n",
       ")</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KerasRegressor</label><div class=\"sk-toggleable__content\"><pre>KerasRegressor(\n",
       "\tmodel=None\n",
       "\tbuild_fn=&lt;function create_model at 0x7fec388cc430&gt;\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=rmsprop\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=32\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=0\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=50\n",
       "\tactivation=tanh\n",
       "\tunits=256\n",
       "\tdropout=0.4\n",
       "\tlearning_rate=0.001\n",
       "\tl2_penalty=0.01\n",
       "\tdepth=3\n",
       ")</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KerasRegressor(\n",
       "\tmodel=None\n",
       "\tbuild_fn=<function create_model at 0x7fec388cc430>\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=rmsprop\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=32\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=0\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=50\n",
       "\tactivation=tanh\n",
       "\tunits=256\n",
       "\tdropout=0.4\n",
       "\tlearning_rate=0.001\n",
       "\tl2_penalty=0.01\n",
       "\tdepth=3\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show best results\n",
    "print(\"Best score:\", bayes_result.best_score_)\n",
    "print(\"Best parameters:\", bayes_result.best_params_)\n",
    "\n",
    "# Entrenar el modelo con los mejores hiperparmetros\n",
    "best_model = bayes_result.best_estimator_\n",
    "best_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicciones con el mejor conjunto de hiper parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores predichos para los prximos 5 das:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Number of trades</th>\n",
       "      <th>Close_BTCUSDT</th>\n",
       "      <th>Volume_BTCUSDT</th>\n",
       "      <th>Number_of_trades_BTCUSDT</th>\n",
       "      <th>Close_ETHUSDT</th>\n",
       "      <th>Volume_ETHUSDT</th>\n",
       "      <th>Number_of_trades_ETHUSDT</th>\n",
       "      <th>Close_BNBUSDT</th>\n",
       "      <th>Volume_BNBUSDT</th>\n",
       "      <th>Number_of_trades_BNBUSDT</th>\n",
       "      <th>SMA_20</th>\n",
       "      <th>EMA_20</th>\n",
       "      <th>Upper_Band</th>\n",
       "      <th>Middle_Band</th>\n",
       "      <th>Lower_Band</th>\n",
       "      <th>RSI</th>\n",
       "      <th>buy_1000x_high_coinbase</th>\n",
       "      <th>sell_1000x_high_coinbase</th>\n",
       "      <th>total_trades_coinbase</th>\n",
       "      <th>Tweets_Utilizados</th>\n",
       "      <th>Tweets_Utilizados_coin</th>\n",
       "      <th>Tweets_Utilizados_referentes</th>\n",
       "      <th>Tweets_Utilizados_whale_alert</th>\n",
       "      <th>Buy_1000x_high</th>\n",
       "      <th>sell_1000x_high</th>\n",
       "      <th>total_trades_binance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>0.120167</td>\n",
       "      <td>0.128135</td>\n",
       "      <td>0.115844</td>\n",
       "      <td>0.128164</td>\n",
       "      <td>0.083843</td>\n",
       "      <td>0.918331</td>\n",
       "      <td>0.054236</td>\n",
       "      <td>0.143388</td>\n",
       "      <td>0.694908</td>\n",
       "      <td>0.128726</td>\n",
       "      <td>0.503699</td>\n",
       "      <td>0.820811</td>\n",
       "      <td>0.368893</td>\n",
       "      <td>0.443672</td>\n",
       "      <td>0.138751</td>\n",
       "      <td>0.139408</td>\n",
       "      <td>0.147628</td>\n",
       "      <td>0.138751</td>\n",
       "      <td>0.127228</td>\n",
       "      <td>0.515583</td>\n",
       "      <td>0.151899</td>\n",
       "      <td>0.170213</td>\n",
       "      <td>0.352479</td>\n",
       "      <td>0.649573</td>\n",
       "      <td>0.501971</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.139073</td>\n",
       "      <td>0.156999</td>\n",
       "      <td>0.156988</td>\n",
       "      <td>0.075976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>903</th>\n",
       "      <td>0.216350</td>\n",
       "      <td>0.220856</td>\n",
       "      <td>0.211053</td>\n",
       "      <td>0.214732</td>\n",
       "      <td>0.112964</td>\n",
       "      <td>0.744083</td>\n",
       "      <td>0.031945</td>\n",
       "      <td>0.094362</td>\n",
       "      <td>0.640497</td>\n",
       "      <td>0.107084</td>\n",
       "      <td>0.357486</td>\n",
       "      <td>0.517440</td>\n",
       "      <td>0.265600</td>\n",
       "      <td>0.319145</td>\n",
       "      <td>0.235087</td>\n",
       "      <td>0.233429</td>\n",
       "      <td>0.236812</td>\n",
       "      <td>0.231657</td>\n",
       "      <td>0.229314</td>\n",
       "      <td>0.699489</td>\n",
       "      <td>0.234968</td>\n",
       "      <td>0.178600</td>\n",
       "      <td>0.304918</td>\n",
       "      <td>0.229351</td>\n",
       "      <td>0.195754</td>\n",
       "      <td>-0.010150</td>\n",
       "      <td>0.258718</td>\n",
       "      <td>0.219295</td>\n",
       "      <td>0.204923</td>\n",
       "      <td>0.102679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>0.214328</td>\n",
       "      <td>0.218792</td>\n",
       "      <td>0.209039</td>\n",
       "      <td>0.212682</td>\n",
       "      <td>0.111027</td>\n",
       "      <td>0.738971</td>\n",
       "      <td>0.027178</td>\n",
       "      <td>0.087148</td>\n",
       "      <td>0.635627</td>\n",
       "      <td>0.107122</td>\n",
       "      <td>0.354030</td>\n",
       "      <td>0.511703</td>\n",
       "      <td>0.263615</td>\n",
       "      <td>0.315115</td>\n",
       "      <td>0.233745</td>\n",
       "      <td>0.232036</td>\n",
       "      <td>0.235348</td>\n",
       "      <td>0.230351</td>\n",
       "      <td>0.227855</td>\n",
       "      <td>0.687336</td>\n",
       "      <td>0.232499</td>\n",
       "      <td>0.176383</td>\n",
       "      <td>0.300742</td>\n",
       "      <td>0.228404</td>\n",
       "      <td>0.194411</td>\n",
       "      <td>-0.010622</td>\n",
       "      <td>0.256129</td>\n",
       "      <td>0.216171</td>\n",
       "      <td>0.202121</td>\n",
       "      <td>0.101571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>0.212247</td>\n",
       "      <td>0.216683</td>\n",
       "      <td>0.207221</td>\n",
       "      <td>0.210532</td>\n",
       "      <td>0.108251</td>\n",
       "      <td>0.731350</td>\n",
       "      <td>0.027154</td>\n",
       "      <td>0.085571</td>\n",
       "      <td>0.631571</td>\n",
       "      <td>0.106158</td>\n",
       "      <td>0.349996</td>\n",
       "      <td>0.511481</td>\n",
       "      <td>0.259843</td>\n",
       "      <td>0.310257</td>\n",
       "      <td>0.233107</td>\n",
       "      <td>0.231206</td>\n",
       "      <td>0.234637</td>\n",
       "      <td>0.229793</td>\n",
       "      <td>0.227220</td>\n",
       "      <td>0.668682</td>\n",
       "      <td>0.223106</td>\n",
       "      <td>0.168567</td>\n",
       "      <td>0.295021</td>\n",
       "      <td>0.226070</td>\n",
       "      <td>0.191471</td>\n",
       "      <td>-0.010513</td>\n",
       "      <td>0.253226</td>\n",
       "      <td>0.209816</td>\n",
       "      <td>0.196194</td>\n",
       "      <td>0.100164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>0.215067</td>\n",
       "      <td>0.219538</td>\n",
       "      <td>0.209976</td>\n",
       "      <td>0.213259</td>\n",
       "      <td>0.109169</td>\n",
       "      <td>0.730516</td>\n",
       "      <td>0.028306</td>\n",
       "      <td>0.086549</td>\n",
       "      <td>0.631552</td>\n",
       "      <td>0.106105</td>\n",
       "      <td>0.349718</td>\n",
       "      <td>0.511914</td>\n",
       "      <td>0.259653</td>\n",
       "      <td>0.311586</td>\n",
       "      <td>0.236259</td>\n",
       "      <td>0.234281</td>\n",
       "      <td>0.237696</td>\n",
       "      <td>0.233032</td>\n",
       "      <td>0.230266</td>\n",
       "      <td>0.667151</td>\n",
       "      <td>0.222224</td>\n",
       "      <td>0.168242</td>\n",
       "      <td>0.295925</td>\n",
       "      <td>0.225689</td>\n",
       "      <td>0.191646</td>\n",
       "      <td>-0.009903</td>\n",
       "      <td>0.254774</td>\n",
       "      <td>0.208854</td>\n",
       "      <td>0.195157</td>\n",
       "      <td>0.100893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>0.221527</td>\n",
       "      <td>0.226156</td>\n",
       "      <td>0.216614</td>\n",
       "      <td>0.219731</td>\n",
       "      <td>0.109972</td>\n",
       "      <td>0.729608</td>\n",
       "      <td>0.025555</td>\n",
       "      <td>0.081932</td>\n",
       "      <td>0.636190</td>\n",
       "      <td>0.103340</td>\n",
       "      <td>0.345416</td>\n",
       "      <td>0.518616</td>\n",
       "      <td>0.257550</td>\n",
       "      <td>0.311893</td>\n",
       "      <td>0.244986</td>\n",
       "      <td>0.242795</td>\n",
       "      <td>0.246227</td>\n",
       "      <td>0.241778</td>\n",
       "      <td>0.238888</td>\n",
       "      <td>0.652290</td>\n",
       "      <td>0.213998</td>\n",
       "      <td>0.161509</td>\n",
       "      <td>0.294880</td>\n",
       "      <td>0.223726</td>\n",
       "      <td>0.189790</td>\n",
       "      <td>-0.010177</td>\n",
       "      <td>0.256519</td>\n",
       "      <td>0.201002</td>\n",
       "      <td>0.188033</td>\n",
       "      <td>0.102510</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Open      High       Low     Close  Number of trades  Close_BTCUSDT  \\\n",
       "902  0.120167  0.128135  0.115844  0.128164          0.083843       0.918331   \n",
       "903  0.216350  0.220856  0.211053  0.214732          0.112964       0.744083   \n",
       "904  0.214328  0.218792  0.209039  0.212682          0.111027       0.738971   \n",
       "905  0.212247  0.216683  0.207221  0.210532          0.108251       0.731350   \n",
       "906  0.215067  0.219538  0.209976  0.213259          0.109169       0.730516   \n",
       "907  0.221527  0.226156  0.216614  0.219731          0.109972       0.729608   \n",
       "\n",
       "     Volume_BTCUSDT  Number_of_trades_BTCUSDT  Close_ETHUSDT  Volume_ETHUSDT  \\\n",
       "902        0.054236                  0.143388       0.694908        0.128726   \n",
       "903        0.031945                  0.094362       0.640497        0.107084   \n",
       "904        0.027178                  0.087148       0.635627        0.107122   \n",
       "905        0.027154                  0.085571       0.631571        0.106158   \n",
       "906        0.028306                  0.086549       0.631552        0.106105   \n",
       "907        0.025555                  0.081932       0.636190        0.103340   \n",
       "\n",
       "     Number_of_trades_ETHUSDT  Close_BNBUSDT  Volume_BNBUSDT  \\\n",
       "902                  0.503699       0.820811        0.368893   \n",
       "903                  0.357486       0.517440        0.265600   \n",
       "904                  0.354030       0.511703        0.263615   \n",
       "905                  0.349996       0.511481        0.259843   \n",
       "906                  0.349718       0.511914        0.259653   \n",
       "907                  0.345416       0.518616        0.257550   \n",
       "\n",
       "     Number_of_trades_BNBUSDT    SMA_20    EMA_20  Upper_Band  Middle_Band  \\\n",
       "902                  0.443672  0.138751  0.139408    0.147628     0.138751   \n",
       "903                  0.319145  0.235087  0.233429    0.236812     0.231657   \n",
       "904                  0.315115  0.233745  0.232036    0.235348     0.230351   \n",
       "905                  0.310257  0.233107  0.231206    0.234637     0.229793   \n",
       "906                  0.311586  0.236259  0.234281    0.237696     0.233032   \n",
       "907                  0.311893  0.244986  0.242795    0.246227     0.241778   \n",
       "\n",
       "     Lower_Band       RSI  buy_1000x_high_coinbase  sell_1000x_high_coinbase  \\\n",
       "902    0.127228  0.515583                 0.151899                  0.170213   \n",
       "903    0.229314  0.699489                 0.234968                  0.178600   \n",
       "904    0.227855  0.687336                 0.232499                  0.176383   \n",
       "905    0.227220  0.668682                 0.223106                  0.168567   \n",
       "906    0.230266  0.667151                 0.222224                  0.168242   \n",
       "907    0.238888  0.652290                 0.213998                  0.161509   \n",
       "\n",
       "     total_trades_coinbase  Tweets_Utilizados  Tweets_Utilizados_coin  \\\n",
       "902               0.352479           0.649573                0.501971   \n",
       "903               0.304918           0.229351                0.195754   \n",
       "904               0.300742           0.228404                0.194411   \n",
       "905               0.295021           0.226070                0.191471   \n",
       "906               0.295925           0.225689                0.191646   \n",
       "907               0.294880           0.223726                0.189790   \n",
       "\n",
       "     Tweets_Utilizados_referentes  Tweets_Utilizados_whale_alert  \\\n",
       "902                      0.000000                       0.139073   \n",
       "903                     -0.010150                       0.258718   \n",
       "904                     -0.010622                       0.256129   \n",
       "905                     -0.010513                       0.253226   \n",
       "906                     -0.009903                       0.254774   \n",
       "907                     -0.010177                       0.256519   \n",
       "\n",
       "     Buy_1000x_high  sell_1000x_high  total_trades_binance  \n",
       "902        0.156999         0.156988              0.075976  \n",
       "903        0.219295         0.204923              0.102679  \n",
       "904        0.216171         0.202121              0.101571  \n",
       "905        0.209816         0.196194              0.100164  \n",
       "906        0.208854         0.195157              0.100893  \n",
       "907        0.201002         0.188033              0.102510  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores predichos desnormalizados para los prximos 5 das:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Number of trades</th>\n",
       "      <th>Close_BTCUSDT</th>\n",
       "      <th>Volume_BTCUSDT</th>\n",
       "      <th>Number_of_trades_BTCUSDT</th>\n",
       "      <th>Close_ETHUSDT</th>\n",
       "      <th>Volume_ETHUSDT</th>\n",
       "      <th>Number_of_trades_ETHUSDT</th>\n",
       "      <th>Close_BNBUSDT</th>\n",
       "      <th>Volume_BNBUSDT</th>\n",
       "      <th>Number_of_trades_BNBUSDT</th>\n",
       "      <th>SMA_20</th>\n",
       "      <th>EMA_20</th>\n",
       "      <th>Upper_Band</th>\n",
       "      <th>Middle_Band</th>\n",
       "      <th>Lower_Band</th>\n",
       "      <th>RSI</th>\n",
       "      <th>buy_1000x_high_coinbase</th>\n",
       "      <th>sell_1000x_high_coinbase</th>\n",
       "      <th>total_trades_coinbase</th>\n",
       "      <th>Tweets_Utilizados</th>\n",
       "      <th>Tweets_Utilizados_coin</th>\n",
       "      <th>Tweets_Utilizados_referentes</th>\n",
       "      <th>Tweets_Utilizados_whale_alert</th>\n",
       "      <th>Buy_1000x_high</th>\n",
       "      <th>sell_1000x_high</th>\n",
       "      <th>total_trades_binance</th>\n",
       "      <th>Fecha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.506426</td>\n",
       "      <td>15.018613</td>\n",
       "      <td>13.817194</td>\n",
       "      <td>14.423111</td>\n",
       "      <td>305539.62500</td>\n",
       "      <td>58410.621094</td>\n",
       "      <td>32985.585938</td>\n",
       "      <td>1718050.000</td>\n",
       "      <td>3437.248779</td>\n",
       "      <td>440577.53125</td>\n",
       "      <td>1266416.875</td>\n",
       "      <td>433.211151</td>\n",
       "      <td>1256823.500</td>\n",
       "      <td>592430.9375</td>\n",
       "      <td>14.236122</td>\n",
       "      <td>14.077535</td>\n",
       "      <td>16.629971</td>\n",
       "      <td>14.084594</td>\n",
       "      <td>11.763090</td>\n",
       "      <td>65.245491</td>\n",
       "      <td>55.687397</td>\n",
       "      <td>50.365238</td>\n",
       "      <td>61757.210938</td>\n",
       "      <td>250.507095</td>\n",
       "      <td>179.969131</td>\n",
       "      <td>-0.172558</td>\n",
       "      <td>39.066448</td>\n",
       "      <td>393.802399</td>\n",
       "      <td>352.114471</td>\n",
       "      <td>199907.078125</td>\n",
       "      <td>2024-03-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.404958</td>\n",
       "      <td>14.912470</td>\n",
       "      <td>13.719303</td>\n",
       "      <td>14.320273</td>\n",
       "      <td>300493.12500</td>\n",
       "      <td>58117.792969</td>\n",
       "      <td>29402.314453</td>\n",
       "      <td>1610471.625</td>\n",
       "      <td>3418.681885</td>\n",
       "      <td>440711.46875</td>\n",
       "      <td>1255664.125</td>\n",
       "      <td>430.592621</td>\n",
       "      <td>1248058.375</td>\n",
       "      <td>585638.6875</td>\n",
       "      <td>14.176838</td>\n",
       "      <td>14.016373</td>\n",
       "      <td>16.552858</td>\n",
       "      <td>14.026902</td>\n",
       "      <td>11.709874</td>\n",
       "      <td>64.520233</td>\n",
       "      <td>55.102333</td>\n",
       "      <td>49.740128</td>\n",
       "      <td>60911.347656</td>\n",
       "      <td>249.509583</td>\n",
       "      <td>178.947006</td>\n",
       "      <td>-0.180570</td>\n",
       "      <td>38.675510</td>\n",
       "      <td>388.846832</td>\n",
       "      <td>347.723755</td>\n",
       "      <td>197836.781250</td>\n",
       "      <td>2024-03-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14.300566</td>\n",
       "      <td>14.803983</td>\n",
       "      <td>13.630942</td>\n",
       "      <td>14.212396</td>\n",
       "      <td>293261.09375</td>\n",
       "      <td>57681.125000</td>\n",
       "      <td>29384.275391</td>\n",
       "      <td>1586953.125</td>\n",
       "      <td>3403.215576</td>\n",
       "      <td>437273.37500</td>\n",
       "      <td>1243112.750</td>\n",
       "      <td>430.490967</td>\n",
       "      <td>1231401.875</td>\n",
       "      <td>577451.3125</td>\n",
       "      <td>14.148661</td>\n",
       "      <td>13.979933</td>\n",
       "      <td>16.515362</td>\n",
       "      <td>14.002253</td>\n",
       "      <td>11.686726</td>\n",
       "      <td>63.406956</td>\n",
       "      <td>52.876198</td>\n",
       "      <td>47.535912</td>\n",
       "      <td>59752.738281</td>\n",
       "      <td>247.051743</td>\n",
       "      <td>176.709747</td>\n",
       "      <td>-0.178716</td>\n",
       "      <td>38.237087</td>\n",
       "      <td>378.767517</td>\n",
       "      <td>338.435791</td>\n",
       "      <td>195205.921875</td>\n",
       "      <td>2024-03-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.442062</td>\n",
       "      <td>14.950838</td>\n",
       "      <td>13.764856</td>\n",
       "      <td>14.349216</td>\n",
       "      <td>295653.71875</td>\n",
       "      <td>57633.351562</td>\n",
       "      <td>30249.919922</td>\n",
       "      <td>1601540.375</td>\n",
       "      <td>3403.144287</td>\n",
       "      <td>437083.21875</td>\n",
       "      <td>1242247.000</td>\n",
       "      <td>430.688934</td>\n",
       "      <td>1230562.000</td>\n",
       "      <td>579691.4375</td>\n",
       "      <td>14.287920</td>\n",
       "      <td>14.114918</td>\n",
       "      <td>16.676605</td>\n",
       "      <td>14.145339</td>\n",
       "      <td>11.797796</td>\n",
       "      <td>63.315590</td>\n",
       "      <td>52.667156</td>\n",
       "      <td>47.444298</td>\n",
       "      <td>59935.675781</td>\n",
       "      <td>246.650360</td>\n",
       "      <td>176.842712</td>\n",
       "      <td>-0.168358</td>\n",
       "      <td>38.470821</td>\n",
       "      <td>377.241974</td>\n",
       "      <td>336.810974</td>\n",
       "      <td>196568.171875</td>\n",
       "      <td>2024-03-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14.766240</td>\n",
       "      <td>15.291197</td>\n",
       "      <td>14.087419</td>\n",
       "      <td>14.673927</td>\n",
       "      <td>297744.15625</td>\n",
       "      <td>57581.359375</td>\n",
       "      <td>28182.140625</td>\n",
       "      <td>1532676.375</td>\n",
       "      <td>3420.827637</td>\n",
       "      <td>427220.31250</td>\n",
       "      <td>1228861.000</td>\n",
       "      <td>433.748383</td>\n",
       "      <td>1221279.000</td>\n",
       "      <td>580207.7500</td>\n",
       "      <td>14.673493</td>\n",
       "      <td>14.488687</td>\n",
       "      <td>17.126171</td>\n",
       "      <td>14.531748</td>\n",
       "      <td>12.112259</td>\n",
       "      <td>62.428688</td>\n",
       "      <td>50.717510</td>\n",
       "      <td>45.545666</td>\n",
       "      <td>59724.125000</td>\n",
       "      <td>244.583847</td>\n",
       "      <td>175.430252</td>\n",
       "      <td>-0.173012</td>\n",
       "      <td>38.734337</td>\n",
       "      <td>364.789307</td>\n",
       "      <td>325.648376</td>\n",
       "      <td>199591.484375</td>\n",
       "      <td>2024-03-27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Open       High        Low      Close  Number of trades  \\\n",
       "0  14.506426  15.018613  13.817194  14.423111      305539.62500   \n",
       "1  14.404958  14.912470  13.719303  14.320273      300493.12500   \n",
       "2  14.300566  14.803983  13.630942  14.212396      293261.09375   \n",
       "3  14.442062  14.950838  13.764856  14.349216      295653.71875   \n",
       "4  14.766240  15.291197  14.087419  14.673927      297744.15625   \n",
       "\n",
       "   Close_BTCUSDT  Volume_BTCUSDT  Number_of_trades_BTCUSDT  Close_ETHUSDT  \\\n",
       "0   58410.621094    32985.585938               1718050.000    3437.248779   \n",
       "1   58117.792969    29402.314453               1610471.625    3418.681885   \n",
       "2   57681.125000    29384.275391               1586953.125    3403.215576   \n",
       "3   57633.351562    30249.919922               1601540.375    3403.144287   \n",
       "4   57581.359375    28182.140625               1532676.375    3420.827637   \n",
       "\n",
       "   Volume_ETHUSDT  Number_of_trades_ETHUSDT  Close_BNBUSDT  Volume_BNBUSDT  \\\n",
       "0    440577.53125               1266416.875     433.211151     1256823.500   \n",
       "1    440711.46875               1255664.125     430.592621     1248058.375   \n",
       "2    437273.37500               1243112.750     430.490967     1231401.875   \n",
       "3    437083.21875               1242247.000     430.688934     1230562.000   \n",
       "4    427220.31250               1228861.000     433.748383     1221279.000   \n",
       "\n",
       "   Number_of_trades_BNBUSDT     SMA_20     EMA_20  Upper_Band  Middle_Band  \\\n",
       "0               592430.9375  14.236122  14.077535   16.629971    14.084594   \n",
       "1               585638.6875  14.176838  14.016373   16.552858    14.026902   \n",
       "2               577451.3125  14.148661  13.979933   16.515362    14.002253   \n",
       "3               579691.4375  14.287920  14.114918   16.676605    14.145339   \n",
       "4               580207.7500  14.673493  14.488687   17.126171    14.531748   \n",
       "\n",
       "   Lower_Band        RSI  buy_1000x_high_coinbase  sell_1000x_high_coinbase  \\\n",
       "0   11.763090  65.245491                55.687397                 50.365238   \n",
       "1   11.709874  64.520233                55.102333                 49.740128   \n",
       "2   11.686726  63.406956                52.876198                 47.535912   \n",
       "3   11.797796  63.315590                52.667156                 47.444298   \n",
       "4   12.112259  62.428688                50.717510                 45.545666   \n",
       "\n",
       "   total_trades_coinbase  Tweets_Utilizados  Tweets_Utilizados_coin  \\\n",
       "0           61757.210938         250.507095              179.969131   \n",
       "1           60911.347656         249.509583              178.947006   \n",
       "2           59752.738281         247.051743              176.709747   \n",
       "3           59935.675781         246.650360              176.842712   \n",
       "4           59724.125000         244.583847              175.430252   \n",
       "\n",
       "   Tweets_Utilizados_referentes  Tweets_Utilizados_whale_alert  \\\n",
       "0                     -0.172558                      39.066448   \n",
       "1                     -0.180570                      38.675510   \n",
       "2                     -0.178716                      38.237087   \n",
       "3                     -0.168358                      38.470821   \n",
       "4                     -0.173012                      38.734337   \n",
       "\n",
       "   Buy_1000x_high  sell_1000x_high  total_trades_binance      Fecha  \n",
       "0      393.802399       352.114471         199907.078125 2024-03-23  \n",
       "1      388.846832       347.723755         197836.781250 2024-03-24  \n",
       "2      378.767517       338.435791         195205.921875 2024-03-25  \n",
       "3      377.241974       336.810974         196568.171875 2024-03-26  \n",
       "4      364.789307       325.648376         199591.484375 2024-03-27  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_days_to_predict = 5\n",
    "future_dataset = feature_dataset\n",
    "\n",
    "dataset = pd.read_csv('/Users/mmarchetta/Desktop/Tesis-2024/data-visualization/final_dataset.csv')\n",
    "dataset['Open_time'] = pd.to_datetime(dataset['Open_time'])\n",
    "dates = dataset['Open_time']\n",
    "\n",
    "# Crear un DataFrame vaco para almacenar las predicciones desnormalizadas\n",
    "predicted_values_desnormalized = pd.DataFrame(columns=future_dataset.columns)\n",
    "\n",
    "# Lista para almacenar las fechas de las predicciones\n",
    "predicted_dates = []\n",
    "\n",
    "for _ in range(n_days_to_predict):\n",
    "    # Predecir 1 da posterior al ltimo da disponible en el dataset\n",
    "    last_sequence = future_dataset.iloc[-n_steps:, :].values.reshape((1, n_steps, n_features))\n",
    "    predictions = best_model.predict(last_sequence)\n",
    "\n",
    "    # Agregar las predicciones sin desnormalizar a future_dataset\n",
    "    predicted_values_normalized = pd.DataFrame(predictions, columns=future_dataset.columns)\n",
    "    future_dataset = pd.concat([future_dataset, predicted_values_normalized], axis=0, ignore_index=True)\n",
    "\n",
    "    # Desnormalizar las predicciones y agregarlas al DataFrame de predicciones desnormalizadas\n",
    "    inverted_predictions = []\n",
    "    for i in range(len(future_dataset.columns)):\n",
    "        col = future_dataset.columns[i]\n",
    "        scaler = scalers[col]\n",
    "        prediction = predictions[:, i].reshape(-1, 1)\n",
    "        inverted_prediction = scaler.inverse_transform(prediction)\n",
    "        inverted_predictions.append(inverted_prediction)\n",
    "\n",
    "    # Calcular la fecha del prximo da\n",
    "    next_day_date = dates.iloc[-1] + pd.DateOffset(days=1)\n",
    "    predicted_dates.append(next_day_date)\n",
    "\n",
    "    # Actualizar la fecha del prximo da en el DataFrame principal\n",
    "    dates = dates.append(pd.Series([next_day_date], name='Fecha'))\n",
    "\n",
    "    # Crear un DataFrame con las predicciones desnormalizadas\n",
    "    predicted_values_desnormalized = pd.concat([predicted_values_desnormalized,\n",
    "                                                pd.DataFrame(np.concatenate(inverted_predictions, axis=1),\n",
    "                                                             columns=future_dataset.columns)], \n",
    "                                                ignore_index=True)\n",
    "\n",
    "# Agregar las fechas al DataFrame de predicciones desnormalizadas\n",
    "predicted_values_desnormalized['Fecha'] = predicted_dates\n",
    "\n",
    "print(\"Valores predichos para los prximos {} das:\".format(n_days_to_predict))\n",
    "display(future_dataset.tail(n_days_to_predict + 1))\n",
    "\n",
    "print(\"Valores predichos desnormalizados para los prximos {} das:\".format(n_days_to_predict))\n",
    "display(predicted_values_desnormalized.tail(n_days_to_predict))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardado de los mejores hiperparametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 mejores modelos:\n",
      "Modelo 1\n",
      "Hiperparmetros: OrderedDict([('activation', 'selu'), ('batch_size', 64), ('depth', 3), ('dropout', 0.1), ('epochs', 20), ('l2_penalty', 0.1), ('learning_rate', 0.001), ('optimizer', 'sgd'), ('units', 64)])\n",
      "Puntaje: -0.038629554049147294\n",
      "Modelo 2\n",
      "Hiperparmetros: OrderedDict([('activation', 'relu'), ('batch_size', 128), ('depth', 3), ('dropout', 0.3), ('epochs', 100), ('l2_penalty', 0.01), ('learning_rate', 0.01), ('optimizer', 'sgd'), ('units', 512)])\n",
      "Puntaje: -0.03132398189558662\n",
      "Modelo 3\n",
      "Hiperparmetros: OrderedDict([('activation', 'relu'), ('batch_size', 128), ('depth', 2), ('dropout', 0.2), ('epochs', 30), ('l2_penalty', 0.1), ('learning_rate', 0.0001), ('optimizer', 'rmsprop'), ('units', 128)])\n",
      "Puntaje: -0.12186648494558452\n",
      "Modelo 4\n",
      "Hiperparmetros: OrderedDict([('activation', 'relu'), ('batch_size', 32), ('depth', 2), ('dropout', 0.4), ('epochs', 10), ('l2_penalty', 0.001), ('learning_rate', 0.0001), ('optimizer', 'rmsprop'), ('units', 64)])\n",
      "Puntaje: -0.11410941666000154\n",
      "Modelo 5\n",
      "Hiperparmetros: OrderedDict([('activation', 'swish'), ('batch_size', 32), ('depth', 2), ('dropout', 0.4), ('epochs', 30), ('l2_penalty', 0.01), ('learning_rate', 0.001), ('optimizer', 'adam'), ('units', 128)])\n",
      "Puntaje: -0.022626062547342195\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Obtener los hiperparmetros y puntajes de los 5 mejores modelos\n",
    "top_n_models = 5\n",
    "best_params_list = []\n",
    "best_scores_list = []\n",
    "\n",
    "for i in range(min(top_n_models, len(bayes_search.cv_results_['params']))):\n",
    "    best_params_list.append(bayes_search.cv_results_['params'][i])\n",
    "    best_scores_list.append(bayes_search.cv_results_['mean_test_score'][i])\n",
    "\n",
    "# Guardar los hiperparmetros de los 5 mejores modelos en un archivo JSON\n",
    "with open('top_5_hyperparameters_conv.json', 'w') as f:\n",
    "    json.dump({'best_params': best_params_list, 'best_scores': best_scores_list}, f)\n",
    "\n",
    "# O imprimir los hiperparmetros\n",
    "print(\"Top 5 mejores modelos:\")\n",
    "for i in range(len(best_params_list)):\n",
    "    print(\"Modelo\", i+1)\n",
    "    print(\"Hiperparmetros:\", best_params_list[i])\n",
    "    print(\"Puntaje:\", best_scores_list[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Armado de un ensamble con los mejores 5 hiperparametros usando la mejor semilla en cada caso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prime_seeds(n):\n",
    "    seeds = []\n",
    "    num = 70001  # Comenzamos desde el primer nmero primo mayor que 70000\n",
    "    while len(seeds) < n:\n",
    "        is_prime = True\n",
    "        for i in range(2, int(num**0.5) + 1):\n",
    "            if num % i == 0:\n",
    "                is_prime = False\n",
    "                break\n",
    "        if is_prime:\n",
    "            seeds.append(num)\n",
    "        num += 1\n",
    "    return seeds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_days(ensemble, feature_dataset, scalers, n_steps, n_features, n_days_to_predict):\n",
    "    future_dataset = feature_dataset.copy()\n",
    "\n",
    "    # Leer el conjunto de datos original para obtener las fechas\n",
    "    dataset = pd.read_csv('/Users/mmarchetta/Desktop/Tesis-2024/data-visualization/final_dataset.csv')\n",
    "    dataset['Open_time'] = pd.to_datetime(dataset['Open_time'])\n",
    "    dates = dataset['Open_time'][:-n_days_to_predict]\n",
    "\n",
    "    # Crear un DataFrame vaco para almacenar las predicciones desnormalizadas\n",
    "    predicted_values_desnormalized = pd.DataFrame(columns=future_dataset.columns)\n",
    "\n",
    "    # Lista para almacenar las fechas de las predicciones\n",
    "    predicted_dates = []\n",
    "\n",
    "    for _ in range(n_days_to_predict):\n",
    "        # Predecir 1 da posterior al ltimo da disponible en el dataset\n",
    "        last_sequence = future_dataset.iloc[-n_steps:, :].values.reshape((1, n_steps, n_features))\n",
    "        predictions = ensemble.predict(last_sequence)\n",
    "\n",
    "        # Agregar las predicciones sin desnormalizar a future_dataset\n",
    "        predicted_values_normalized = pd.DataFrame(predictions, columns=future_dataset.columns)\n",
    "        future_dataset = pd.concat([future_dataset, predicted_values_normalized], axis=0, ignore_index=True)\n",
    "\n",
    "        # Desnormalizar las predicciones y agregarlas al DataFrame de predicciones desnormalizadas\n",
    "        inverted_predictions = []\n",
    "        for i in range(len(future_dataset.columns)):\n",
    "            col = future_dataset.columns[i]\n",
    "            scaler = scalers[col]\n",
    "            prediction = predictions[:, i].reshape(-1, 1)\n",
    "            inverted_prediction = scaler.inverse_transform(prediction)\n",
    "            inverted_predictions.append(inverted_prediction)\n",
    "\n",
    "        # Calcular la fecha del prximo da\n",
    "        next_day_date = dates.iloc[-1] + pd.DateOffset(days=1)\n",
    "        predicted_dates.append(next_day_date)\n",
    "\n",
    "        # Actualizar la fecha del prximo da en el DataFrame principal\n",
    "        dates = dates.append(pd.Series([next_day_date], name='Fecha'))\n",
    "\n",
    "        # Crear un DataFrame con las predicciones desnormalizadas\n",
    "        predicted_values_desnormalized = pd.concat([predicted_values_desnormalized,\n",
    "                                                    pd.DataFrame(np.concatenate(inverted_predictions, axis=1),\n",
    "                                                                 columns=future_dataset.columns)], \n",
    "                                                    ignore_index=True)\n",
    "\n",
    "    # Agregar las fechas al DataFrame de predicciones desnormalizadas\n",
    "    predicted_values_desnormalized['Fecha'] = predicted_dates\n",
    "\n",
    "    return future_dataset, predicted_values_desnormalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clase personalizada para hacer el ensamble, dado que sklearn no provee ninguna clase que permita hacer ensmble\n",
    "## de modelos re regresion multivariados\n",
    "class MultivariableVotingRegressor:\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for model in self.models:\n",
    "            model.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Hacer predicciones con cada modelo\n",
    "        predictions = [model.predict(X) for model in self.models]\n",
    "    \n",
    "        # Calcular el promedio de las predicciones\n",
    "        average_predictions = np.mean(predictions, axis=0)\n",
    "    \n",
    "        return average_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model number: 1, seed number: 222 error: -622831645509.9629\n",
      "model number: 1, seed number: 223 error: -622831641481.4478\n",
      "model number: 1, seed number: 224 error: -622831633742.571\n",
      "model number: 1, seed number: 225 error: -622831650830.6962\n",
      "model number: 1, seed number: 226 error: -622831609538.0438\n",
      "model number: 1, seed number: 227 error: -622831625760.3048\n",
      "model number: 1, seed number: 228 error: -622831629757.7859\n",
      "model number: 1, seed number: 229 error: -622831652072.9731\n",
      "model number: 1, seed number: 230 error: -622831629212.7938\n",
      "model number: 1, seed number: 231 error: -622831665008.1115\n",
      "model number: 1, seed number: 232 error: -622831657502.8982\n",
      "model number: 1, seed number: 233 error: -622831580407.6605\n",
      "model number: 1, seed number: 234 error: -622831653860.5012\n",
      "model number: 1, seed number: 235 error: -622831649453.6666\n",
      "model number: 1, seed number: 236 error: -622831615368.9042\n",
      "model number: 1, seed number: 237 error: -622831653945.9086\n",
      "model number: 1, seed number: 238 error: -622831627683.9606\n",
      "model number: 1, seed number: 239 error: -622831675544.1954\n",
      "model number: 1, seed number: 240 error: -622831651531.3771\n",
      "model number: 1, seed number: 241 error: -622831658416.5278\n",
      "model number: 1, seed number: 242 error: -622831665096.8147\n",
      "model number: 1, seed number: 243 error: -622831593497.1152\n",
      "model number: 1, seed number: 244 error: -622831642900.5706\n",
      "model number: 1, seed number: 245 error: -622831656432.1602\n",
      "model number: 1, seed number: 246 error: -622831648111.2856\n",
      "model number: 1, seed number: 247 error: -622831643475.0524\n",
      "model number: 1, seed number: 248 error: -622831620304.4503\n",
      "model number: 1, seed number: 249 error: -622831647578.1283\n",
      "model number: 1, seed number: 250 error: -622831630307.2192\n",
      "model number: 1, seed number: 251 error: -622831626625.5844\n",
      "model number: 1, seed number: 252 error: -622831660879.7332\n",
      "model number: 1, seed number: 253 error: -622831663814.0344\n",
      "model number: 1, seed number: 254 error: -622831631130.9214\n",
      "model number: 1, seed number: 255 error: -622831655200.3146\n",
      "model number: 1, seed number: 256 error: -622831626428.9467\n",
      "model number: 1, seed number: 257 error: -622831633395.5383\n",
      "model number: 1, seed number: 258 error: -622831655186.3849\n",
      "model number: 1, seed number: 259 error: -622831641566.0996\n",
      "model number: 1, seed number: 260 error: -622831639335.6317\n",
      "model number: 1, seed number: 261 error: -622831651873.5726\n",
      "model number: 1, seed number: 262 error: -622831659259.0265\n",
      "model number: 1, seed number: 263 error: -622831662228.2916\n",
      "model number: 1, seed number: 264 error: -622831652845.406\n",
      "model number: 1, seed number: 265 error: -622831608441.2854\n",
      "model number: 1, seed number: 266 error: -622831640778.718\n",
      "model number: 1, seed number: 267 error: -622831679271.8123\n",
      "model number: 1, seed number: 268 error: -622831655269.4875\n",
      "model number: 1, seed number: 269 error: -622831644975.0992\n",
      "model number: 1, seed number: 270 error: -622831661871.0769\n",
      "model number: 1, seed number: 271 error: -622831622367.7229\n",
      "model number: 1, seed number: 272 error: -622831660630.228\n",
      "model number: 1, seed number: 273 error: -622831657475.0808\n",
      "model number: 1, seed number: 274 error: -622831666852.5562\n",
      "model number: 1, seed number: 275 error: -622831643850.8031\n",
      "model number: 1, seed number: 276 error: -622831632771.2058\n",
      "model number: 1, seed number: 277 error: -622831620217.519\n",
      "model number: 1, seed number: 278 error: -622831663932.1655\n",
      "model number: 1, seed number: 279 error: -622831654580.9498\n",
      "model number: 1, seed number: 280 error: -622831644474.4799\n",
      "model number: 1, seed number: 281 error: -622831621533.3159\n",
      "model number: 1, seed number: 282 error: -622831659157.8563\n",
      "model number: 1, seed number: 283 error: -622831673612.9222\n",
      "model number: 1, seed number: 284 error: -622831641475.9681\n",
      "model number: 1, seed number: 285 error: -622831631119.9778\n",
      "model number: 1, seed number: 286 error: -622831654480.4827\n",
      "model number: 1, seed number: 287 error: -622831661443.8002\n",
      "model number: 1, seed number: 288 error: -622831668738.4672\n",
      "model number: 1, seed number: 289 error: -622831653996.0441\n",
      "model number: 1, seed number: 290 error: -622831643838.951\n",
      "model number: 1, seed number: 291 error: -622831653537.6034\n",
      "model number: 1, seed number: 292 error: -622831670556.2047\n",
      "model number: 1, seed number: 293 error: -622831667204.4272\n",
      "model number: 1, seed number: 294 error: -622831587260.0547\n",
      "model number: 1, seed number: 295 error: -622831676366.4789\n",
      "model number: 1, seed number: 296 error: -622831656154.2854\n",
      "model number: 1, seed number: 297 error: -622831628160.6409\n",
      "model number: 1, seed number: 298 error: -622831635039.1838\n",
      "model number: 1, seed number: 299 error: -622831661541.8505\n",
      "model number: 2, seed number: 0 error: -622831573912.8647\n",
      "model number: 2, seed number: 1 error: -622831671702.3811\n",
      "model number: 2, seed number: 2 error: -622831800114.1398\n",
      "model number: 2, seed number: 3 error: -622831777606.0836\n",
      "model number: 2, seed number: 4 error: -622831640790.9768\n",
      "model number: 2, seed number: 5 error: -622831638356.4128\n",
      "model number: 2, seed number: 6 error: -622831797835.3495\n",
      "model number: 2, seed number: 7 error: -622831817789.6715\n",
      "model number: 2, seed number: 8 error: -622831729265.5435\n",
      "model number: 2, seed number: 9 error: -622831640848.3372\n",
      "model number: 2, seed number: 10 error: -622831853145.2605\n",
      "model number: 2, seed number: 11 error: -622831696334.3204\n",
      "model number: 2, seed number: 12 error: -622831834061.6746\n",
      "model number: 2, seed number: 13 error: -622831743454.4626\n",
      "model number: 2, seed number: 14 error: -622831749734.7612\n",
      "model number: 2, seed number: 15 error: -622831653231.0667\n",
      "model number: 2, seed number: 16 error: -622831511252.5939\n",
      "model number: 2, seed number: 17 error: -622831760472.9727\n",
      "model number: 2, seed number: 18 error: -622831828127.9712\n",
      "model number: 2, seed number: 19 error: -622831590759.5193\n",
      "model number: 2, seed number: 20 error: -622831622442.5552\n",
      "model number: 2, seed number: 21 error: -622831733220.8873\n",
      "model number: 2, seed number: 22 error: -622831689289.1641\n",
      "model number: 2, seed number: 23 error: -622831674766.5319\n",
      "model number: 2, seed number: 24 error: -622831598599.8672\n",
      "model number: 2, seed number: 25 error: -622831641339.9944\n",
      "model number: 2, seed number: 26 error: -622831742841.0237\n",
      "model number: 2, seed number: 27 error: -622831772267.9398\n",
      "model number: 2, seed number: 28 error: -622831763433.2434\n",
      "model number: 2, seed number: 29 error: -622831692512.2676\n",
      "model number: 2, seed number: 30 error: -622831736352.9004\n",
      "model number: 2, seed number: 31 error: -622831734882.7433\n",
      "model number: 2, seed number: 32 error: -622831709088.0552\n",
      "model number: 2, seed number: 33 error: -622831752362.0608\n",
      "model number: 2, seed number: 34 error: -622831725209.7355\n",
      "model number: 2, seed number: 35 error: -622831726710.5542\n",
      "model number: 2, seed number: 36 error: -622831908485.5933\n",
      "model number: 2, seed number: 37 error: -622831890097.8624\n",
      "model number: 2, seed number: 38 error: -622831860952.6278\n",
      "model number: 2, seed number: 39 error: -622831609736.593\n",
      "model number: 2, seed number: 40 error: -622831696333.5038\n",
      "model number: 2, seed number: 41 error: -622831712296.4485\n",
      "model number: 2, seed number: 42 error: -622831736399.7421\n",
      "model number: 2, seed number: 43 error: -622831840857.2706\n",
      "model number: 2, seed number: 44 error: -622831734405.2488\n",
      "model number: 2, seed number: 45 error: -622831746216.2543\n",
      "model number: 2, seed number: 46 error: -622831512053.6993\n",
      "model number: 2, seed number: 47 error: -622831649210.1838\n",
      "model number: 2, seed number: 48 error: -622831817311.4089\n",
      "model number: 2, seed number: 49 error: -622831749247.4071\n",
      "model number: 2, seed number: 50 error: -622831822336.0779\n",
      "model number: 2, seed number: 51 error: -622831662629.2759\n",
      "model number: 2, seed number: 52 error: -622831579631.5277\n",
      "model number: 2, seed number: 53 error: -622831716690.487\n",
      "model number: 2, seed number: 54 error: -622831645025.7688\n",
      "model number: 2, seed number: 55 error: -622831701940.3948\n",
      "model number: 2, seed number: 56 error: -622831893562.1627\n",
      "model number: 2, seed number: 57 error: -622831803605.8813\n",
      "model number: 2, seed number: 58 error: -622831741394.3561\n",
      "model number: 2, seed number: 59 error: -622831760642.8976\n",
      "model number: 2, seed number: 60 error: -622831724425.1311\n",
      "model number: 2, seed number: 61 error: -622831750029.7532\n",
      "model number: 2, seed number: 62 error: -622831691829.6172\n",
      "model number: 2, seed number: 63 error: -622831725191.2648\n",
      "model number: 2, seed number: 64 error: -622831685884.5763\n",
      "model number: 2, seed number: 65 error: -622831641476.272\n",
      "model number: 2, seed number: 66 error: -622831704117.0002\n",
      "model number: 2, seed number: 67 error: -622831655863.8944\n",
      "model number: 2, seed number: 68 error: -622831727866.6962\n",
      "model number: 2, seed number: 69 error: -622831669776.7833\n",
      "model number: 2, seed number: 70 error: -622831811806.9924\n",
      "model number: 2, seed number: 71 error: -622831648886.502\n",
      "model number: 2, seed number: 72 error: -622831891499.4049\n",
      "model number: 2, seed number: 73 error: -622831719536.6852\n",
      "model number: 2, seed number: 74 error: -622831724428.5806\n",
      "model number: 2, seed number: 75 error: -622831503519.7485\n",
      "model number: 2, seed number: 76 error: -622831784173.047\n",
      "model number: 2, seed number: 77 error: -622831670194.9049\n",
      "model number: 2, seed number: 78 error: -622831878908.2839\n",
      "model number: 2, seed number: 79 error: -622831779341.1931\n",
      "model number: 2, seed number: 80 error: -622831723940.9408\n",
      "model number: 2, seed number: 81 error: -622831725638.825\n",
      "model number: 2, seed number: 82 error: -622831743637.5824\n",
      "model number: 2, seed number: 83 error: -622831579074.9397\n",
      "model number: 2, seed number: 84 error: -622831782696.0468\n",
      "model number: 2, seed number: 85 error: -622831686774.0231\n",
      "model number: 2, seed number: 86 error: -622831754409.7433\n",
      "model number: 2, seed number: 87 error: -622831723336.2847\n",
      "model number: 2, seed number: 88 error: -622831787334.302\n",
      "model number: 2, seed number: 89 error: -622831861390.8693\n",
      "model number: 2, seed number: 90 error: -622831626507.8899\n",
      "model number: 2, seed number: 91 error: -622831783335.6597\n",
      "model number: 2, seed number: 92 error: -622831788223.3162\n",
      "model number: 2, seed number: 93 error: -622831670915.7485\n",
      "model number: 2, seed number: 94 error: -622831697864.871\n",
      "model number: 2, seed number: 95 error: -622831734844.5737\n",
      "model number: 2, seed number: 96 error: -622831780168.2185\n",
      "model number: 2, seed number: 97 error: -622831840002.0366\n",
      "model number: 2, seed number: 98 error: -622831735248.6732\n",
      "model number: 2, seed number: 99 error: -622831732340.0706\n",
      "model number: 2, seed number: 100 error: -622831800833.6101\n",
      "model number: 2, seed number: 101 error: -622831673486.3516\n",
      "model number: 2, seed number: 102 error: -622831642930.8461\n",
      "model number: 2, seed number: 103 error: -622831713756.7855\n",
      "model number: 2, seed number: 104 error: -622831837024.282\n",
      "model number: 2, seed number: 105 error: -622831726759.2941\n",
      "model number: 2, seed number: 106 error: -622831782628.4801\n",
      "model number: 2, seed number: 107 error: -622831742020.9318\n",
      "model number: 2, seed number: 108 error: -622831465997.0565\n",
      "model number: 2, seed number: 109 error: -622831674658.4833\n",
      "model number: 2, seed number: 110 error: -622831598126.44\n",
      "model number: 2, seed number: 111 error: -622831737312.625\n",
      "model number: 2, seed number: 112 error: -622831790752.0435\n",
      "model number: 2, seed number: 113 error: -622831830398.1187\n",
      "model number: 2, seed number: 114 error: -622831581575.0552\n",
      "model number: 2, seed number: 115 error: -622831764185.4656\n",
      "model number: 2, seed number: 116 error: -622831706317.6842\n",
      "model number: 2, seed number: 117 error: -622831643043.6855\n",
      "model number: 2, seed number: 118 error: -622831825273.9005\n",
      "model number: 2, seed number: 119 error: -622831764255.7853\n",
      "model number: 2, seed number: 120 error: -622831766076.0194\n",
      "model number: 2, seed number: 121 error: -622831764666.2415\n",
      "model number: 2, seed number: 122 error: -622831657427.9386\n",
      "model number: 2, seed number: 123 error: -622831786259.6111\n",
      "model number: 2, seed number: 124 error: -622831711421.9991\n",
      "model number: 2, seed number: 125 error: -622831682975.0499\n",
      "model number: 2, seed number: 126 error: -622831672541.7449\n",
      "model number: 2, seed number: 127 error: -622831686416.6567\n",
      "model number: 2, seed number: 128 error: -622831723926.3048\n",
      "model number: 2, seed number: 129 error: -622831774398.5404\n",
      "model number: 2, seed number: 130 error: -622831723728.7198\n",
      "model number: 2, seed number: 131 error: -622831639274.5353\n",
      "model number: 2, seed number: 132 error: -622831520892.7454\n",
      "model number: 2, seed number: 133 error: -622831670612.1381\n",
      "model number: 2, seed number: 134 error: -622831779830.8394\n",
      "model number: 2, seed number: 135 error: -622831690103.6951\n",
      "model number: 2, seed number: 136 error: -622831863005.4264\n",
      "model number: 2, seed number: 137 error: -622831747052.5073\n",
      "model number: 2, seed number: 138 error: -622831719725.2083\n",
      "model number: 2, seed number: 139 error: -622831490654.3986\n",
      "model number: 2, seed number: 140 error: -622831798821.2858\n",
      "model number: 2, seed number: 141 error: -622831616458.1847\n",
      "model number: 2, seed number: 142 error: -622831751011.6583\n",
      "model number: 2, seed number: 143 error: -622831717061.4016\n",
      "model number: 2, seed number: 144 error: -622831705365.6136\n",
      "model number: 2, seed number: 145 error: -622831494401.7712\n",
      "model number: 2, seed number: 146 error: -622831824692.4861\n",
      "model number: 2, seed number: 147 error: -622831637618.7534\n",
      "model number: 2, seed number: 148 error: -622831663684.3656\n",
      "model number: 2, seed number: 149 error: -622831661145.8315\n",
      "model number: 2, seed number: 150 error: -622831642128.7262\n",
      "model number: 2, seed number: 151 error: -622831628456.2457\n",
      "model number: 2, seed number: 152 error: -622831801995.082\n",
      "model number: 2, seed number: 153 error: -622831735881.504\n",
      "model number: 2, seed number: 154 error: -622831660428.3132\n",
      "model number: 2, seed number: 155 error: -622831674926.497\n",
      "model number: 2, seed number: 156 error: -622831760745.5717\n",
      "model number: 2, seed number: 157 error: -622831700350.5173\n",
      "model number: 2, seed number: 158 error: -622831757429.9635\n",
      "model number: 2, seed number: 159 error: -622831794038.6195\n",
      "model number: 2, seed number: 160 error: -622831828959.2015\n",
      "model number: 2, seed number: 161 error: -622831690019.0571\n",
      "model number: 2, seed number: 162 error: -622831770453.1166\n",
      "model number: 2, seed number: 163 error: -622831579771.4485\n",
      "model number: 2, seed number: 164 error: -622831681471.3741\n",
      "model number: 2, seed number: 165 error: -622831716171.5121\n",
      "model number: 2, seed number: 166 error: -622831621732.7501\n",
      "model number: 2, seed number: 167 error: -622831783767.0006\n",
      "model number: 2, seed number: 168 error: -622831818241.6288\n",
      "model number: 2, seed number: 169 error: -622831731399.0955\n",
      "model number: 2, seed number: 170 error: -622831815499.7943\n",
      "model number: 2, seed number: 171 error: -622831821525.2968\n",
      "model number: 2, seed number: 172 error: -622831702552.7397\n",
      "model number: 2, seed number: 173 error: -622831818941.3639\n",
      "model number: 2, seed number: 174 error: -622831774432.0956\n",
      "model number: 2, seed number: 175 error: -622831691233.0636\n",
      "model number: 2, seed number: 176 error: -622831705803.2362\n",
      "model number: 2, seed number: 177 error: -622831639906.8038\n",
      "model number: 2, seed number: 178 error: -622831703632.077\n",
      "model number: 2, seed number: 179 error: -622831722403.0773\n",
      "model number: 2, seed number: 180 error: -622831597585.816\n",
      "model number: 2, seed number: 181 error: -622831765577.3881\n",
      "model number: 2, seed number: 182 error: -622831721741.4944\n",
      "model number: 2, seed number: 183 error: -622831498679.3708\n",
      "model number: 2, seed number: 184 error: -622831779479.1003\n",
      "model number: 2, seed number: 185 error: -622831727487.6106\n",
      "model number: 2, seed number: 186 error: -622831809403.3853\n",
      "model number: 2, seed number: 187 error: -622831803557.791\n",
      "model number: 2, seed number: 188 error: -622831610629.7308\n",
      "model number: 2, seed number: 189 error: -622831721580.0906\n",
      "model number: 2, seed number: 190 error: -622831721278.8705\n",
      "model number: 2, seed number: 191 error: -622831812006.8567\n",
      "model number: 2, seed number: 192 error: -622831786861.5975\n",
      "model number: 2, seed number: 193 error: -622831794357.9266\n",
      "model number: 2, seed number: 194 error: -622831843529.6588\n",
      "model number: 2, seed number: 195 error: -622831735432.9988\n",
      "model number: 2, seed number: 196 error: -622831602599.429\n",
      "model number: 2, seed number: 197 error: -622831893323.3778\n",
      "model number: 2, seed number: 198 error: -622831639680.8617\n",
      "model number: 2, seed number: 199 error: -622831807805.8597\n",
      "model number: 2, seed number: 200 error: -622831677420.9861\n",
      "model number: 2, seed number: 201 error: -622831655978.6918\n",
      "model number: 2, seed number: 202 error: -622831792114.3735\n",
      "model number: 2, seed number: 203 error: -622831698045.194\n",
      "model number: 2, seed number: 204 error: -622831774404.6953\n",
      "model number: 2, seed number: 205 error: -622831743797.1302\n",
      "model number: 2, seed number: 206 error: -622831772197.8262\n",
      "model number: 2, seed number: 207 error: -622831793220.705\n",
      "model number: 2, seed number: 208 error: -622831666733.1957\n",
      "model number: 2, seed number: 209 error: -622831784091.1743\n",
      "model number: 2, seed number: 210 error: -622831819046.1583\n",
      "model number: 2, seed number: 211 error: -622831663589.2759\n",
      "model number: 2, seed number: 212 error: -622831647894.1305\n",
      "model number: 2, seed number: 213 error: -622831687901.6564\n",
      "model number: 2, seed number: 214 error: -622831752035.3739\n",
      "model number: 2, seed number: 215 error: -622831695349.9183\n",
      "model number: 2, seed number: 216 error: -622831710158.5133\n",
      "model number: 2, seed number: 217 error: -622831701662.3947\n",
      "model number: 2, seed number: 218 error: -622831795313.0391\n",
      "model number: 2, seed number: 219 error: -622831684578.5535\n",
      "model number: 2, seed number: 220 error: -622831892391.9855\n",
      "model number: 2, seed number: 221 error: -622831675530.0636\n",
      "model number: 2, seed number: 222 error: -622831705003.1267\n",
      "model number: 2, seed number: 223 error: -622831833692.6992\n",
      "model number: 2, seed number: 224 error: -622831696023.7377\n",
      "model number: 2, seed number: 225 error: -622831758737.469\n",
      "model number: 2, seed number: 226 error: -622831741943.2877\n",
      "model number: 2, seed number: 227 error: -622831872591.2324\n",
      "model number: 2, seed number: 228 error: -622831655022.0571\n",
      "model number: 2, seed number: 229 error: -622831704900.3666\n",
      "model number: 2, seed number: 230 error: -622831702708.0695\n",
      "model number: 2, seed number: 231 error: -622831851988.0679\n",
      "model number: 2, seed number: 232 error: -622831730541.8452\n",
      "model number: 2, seed number: 233 error: -622831707199.2349\n",
      "model number: 2, seed number: 234 error: -622831613578.4423\n",
      "model number: 2, seed number: 235 error: -622831655243.9226\n",
      "model number: 2, seed number: 236 error: -622831882072.0084\n",
      "model number: 2, seed number: 237 error: -622831613978.0521\n",
      "model number: 2, seed number: 238 error: -622831655592.106\n",
      "model number: 2, seed number: 239 error: -622831623036.7848\n",
      "model number: 2, seed number: 240 error: -622831888395.0403\n",
      "model number: 2, seed number: 241 error: -622831808943.6298\n",
      "model number: 2, seed number: 242 error: -622831920668.4375\n",
      "model number: 2, seed number: 243 error: -622831772070.006\n",
      "model number: 2, seed number: 244 error: -622831698005.2183\n",
      "model number: 2, seed number: 245 error: -622831655593.0067\n",
      "model number: 2, seed number: 246 error: -622831678299.877\n",
      "model number: 2, seed number: 247 error: -622831902270.6215\n",
      "model number: 2, seed number: 248 error: -622831741528.0736\n",
      "model number: 2, seed number: 249 error: -622831745977.7345\n",
      "model number: 2, seed number: 250 error: -622831721315.807\n",
      "model number: 2, seed number: 251 error: -622831613108.9056\n",
      "model number: 2, seed number: 252 error: -622831714173.7493\n",
      "model number: 2, seed number: 253 error: -622831553609.1001\n",
      "model number: 2, seed number: 254 error: -622831542480.6742\n",
      "model number: 2, seed number: 255 error: -622831814399.7587\n",
      "model number: 2, seed number: 256 error: -622831629404.8239\n",
      "model number: 2, seed number: 257 error: -622831725269.273\n",
      "model number: 2, seed number: 258 error: -622831738805.8925\n",
      "model number: 2, seed number: 259 error: -622831706469.5286\n",
      "model number: 2, seed number: 260 error: -622831735154.7045\n",
      "model number: 2, seed number: 261 error: -622831798043.6871\n",
      "model number: 2, seed number: 262 error: -622831661248.3342\n",
      "model number: 2, seed number: 263 error: -622831554805.0242\n",
      "model number: 2, seed number: 264 error: -622831823390.6411\n",
      "model number: 2, seed number: 265 error: -622831812558.4844\n",
      "model number: 2, seed number: 266 error: -622831761045.8496\n",
      "model number: 2, seed number: 267 error: -622831633092.8617\n",
      "model number: 2, seed number: 268 error: -622831661474.832\n",
      "model number: 2, seed number: 269 error: -622831789819.8832\n",
      "model number: 2, seed number: 270 error: -622831738009.3179\n",
      "model number: 2, seed number: 271 error: -622831670694.2985\n",
      "model number: 2, seed number: 272 error: -622831791114.4796\n",
      "model number: 2, seed number: 273 error: -622831699819.2205\n",
      "model number: 2, seed number: 274 error: -622831696957.6791\n",
      "model number: 2, seed number: 275 error: -622831633940.7765\n",
      "model number: 2, seed number: 276 error: -622831680818.7252\n",
      "model number: 2, seed number: 277 error: -622831767893.3082\n",
      "model number: 2, seed number: 278 error: -622831695637.254\n",
      "model number: 2, seed number: 279 error: -622831734352.3605\n",
      "model number: 2, seed number: 280 error: -622831794909.9058\n",
      "model number: 2, seed number: 281 error: -622831730016.2229\n",
      "model number: 2, seed number: 282 error: -622831853088.394\n",
      "model number: 2, seed number: 283 error: -622831875980.1018\n",
      "model number: 2, seed number: 284 error: -622831744191.0096\n",
      "model number: 2, seed number: 285 error: -622831840350.4447\n",
      "model number: 2, seed number: 286 error: -622831832259.9988\n",
      "model number: 2, seed number: 287 error: -622831890373.6814\n",
      "model number: 2, seed number: 288 error: -622831806903.7047\n",
      "model number: 2, seed number: 289 error: -622831772859.4219\n",
      "model number: 2, seed number: 290 error: -622831724288.5779\n",
      "model number: 2, seed number: 291 error: -622831736604.4055\n",
      "model number: 2, seed number: 292 error: -622831847037.9784\n",
      "model number: 2, seed number: 293 error: -622831779782.438\n",
      "model number: 2, seed number: 294 error: -622831669501.3833\n",
      "model number: 2, seed number: 295 error: -622831641462.3956\n",
      "model number: 2, seed number: 296 error: -622831904981.5608\n",
      "model number: 2, seed number: 297 error: -622831741359.6058\n",
      "model number: 2, seed number: 298 error: -622831665857.3279\n",
      "model number: 2, seed number: 299 error: -622831887967.6017\n",
      "model number: 3, seed number: 0 error: -622831737138.7214\n",
      "model number: 3, seed number: 1 error: -622831610578.8453\n",
      "model number: 3, seed number: 2 error: -622831699721.888\n",
      "model number: 3, seed number: 3 error: -622831789183.5724\n",
      "model number: 3, seed number: 4 error: -622831593532.8444\n",
      "model number: 3, seed number: 5 error: -622831631237.8187\n",
      "model number: 3, seed number: 6 error: -622831636822.0577\n",
      "model number: 3, seed number: 7 error: -622831857482.5345\n",
      "model number: 3, seed number: 8 error: -622831817950.082\n",
      "model number: 3, seed number: 9 error: -622831765977.2122\n",
      "model number: 3, seed number: 10 error: -622831751147.0183\n",
      "model number: 3, seed number: 11 error: -622831688997.444\n",
      "model number: 3, seed number: 12 error: -622831893435.5952\n",
      "model number: 3, seed number: 13 error: -622831848490.8456\n",
      "model number: 3, seed number: 14 error: -622831806400.352\n",
      "model number: 3, seed number: 15 error: -622831774500.8284\n",
      "model number: 3, seed number: 16 error: -622831665301.5095\n",
      "model number: 3, seed number: 17 error: -622831750732.5328\n",
      "model number: 3, seed number: 18 error: -622831668674.0437\n",
      "model number: 3, seed number: 19 error: -622831788538.3722\n",
      "model number: 3, seed number: 20 error: -622831727004.4421\n",
      "model number: 3, seed number: 21 error: -622831677855.7164\n",
      "model number: 3, seed number: 22 error: -622831673277.8665\n",
      "model number: 3, seed number: 23 error: -622831760888.8257\n",
      "model number: 3, seed number: 24 error: -622831724733.72\n",
      "model number: 3, seed number: 25 error: -622831702102.2712\n",
      "model number: 3, seed number: 26 error: -622831766875.5758\n",
      "model number: 3, seed number: 27 error: -622831742639.1588\n",
      "model number: 3, seed number: 28 error: -622831721520.05\n",
      "model number: 3, seed number: 29 error: -622831679251.9342\n",
      "model number: 3, seed number: 30 error: -622831659153.3605\n",
      "model number: 3, seed number: 31 error: -622831736502.3489\n",
      "model number: 3, seed number: 32 error: -622831637881.7245\n",
      "model number: 3, seed number: 33 error: -622831726934.9592\n",
      "model number: 3, seed number: 34 error: -622831533919.0009\n",
      "model number: 3, seed number: 35 error: -622831808551.283\n",
      "model number: 3, seed number: 36 error: -622831612067.0941\n",
      "model number: 3, seed number: 37 error: -622831698930.5428\n",
      "model number: 3, seed number: 38 error: -622831774019.908\n",
      "model number: 3, seed number: 39 error: -622831736852.3136\n",
      "model number: 3, seed number: 40 error: -622831632278.196\n",
      "model number: 3, seed number: 41 error: -622831661971.2095\n",
      "model number: 3, seed number: 42 error: -622831786930.3856\n",
      "model number: 3, seed number: 43 error: -622831994520.0951\n",
      "model number: 3, seed number: 44 error: -622831864465.2899\n",
      "model number: 3, seed number: 45 error: -622831699033.6405\n",
      "model number: 3, seed number: 46 error: -622831762862.4767\n",
      "model number: 3, seed number: 47 error: -622831484342.078\n",
      "model number: 3, seed number: 48 error: -622831629879.7898\n",
      "model number: 3, seed number: 49 error: -622831835874.1094\n",
      "model number: 3, seed number: 50 error: -622831694225.9001\n",
      "model number: 3, seed number: 51 error: -622831784284.5833\n",
      "model number: 3, seed number: 52 error: -622831717024.7153\n",
      "model number: 3, seed number: 53 error: -622831679681.8308\n",
      "model number: 3, seed number: 54 error: -622831793096.0482\n",
      "model number: 3, seed number: 55 error: -622831770925.8201\n",
      "model number: 3, seed number: 56 error: -622831939489.3405\n",
      "model number: 3, seed number: 57 error: -622831691319.4474\n",
      "model number: 3, seed number: 58 error: -622831668217.2216\n",
      "model number: 3, seed number: 59 error: -622831552262.4797\n",
      "model number: 3, seed number: 60 error: -622831675737.5491\n",
      "model number: 3, seed number: 61 error: -622831761784.787\n",
      "model number: 3, seed number: 62 error: -622831648177.8511\n",
      "model number: 3, seed number: 63 error: -622831685440.8743\n",
      "model number: 3, seed number: 64 error: -622831859044.3972\n",
      "model number: 3, seed number: 65 error: -622831661531.9606\n",
      "model number: 3, seed number: 66 error: -622831666226.148\n",
      "model number: 3, seed number: 67 error: -622831689257.727\n",
      "model number: 3, seed number: 68 error: -622831814056.8044\n",
      "model number: 3, seed number: 69 error: -622831618192.738\n",
      "model number: 3, seed number: 70 error: -622831764962.0787\n",
      "model number: 3, seed number: 71 error: -622831746345.6927\n",
      "model number: 3, seed number: 72 error: -622831740639.3353\n",
      "model number: 3, seed number: 73 error: -622831815537.0298\n",
      "model number: 3, seed number: 74 error: -622831714052.8047\n",
      "model number: 3, seed number: 75 error: -622831800635.2733\n",
      "model number: 3, seed number: 76 error: -622831631439.5197\n",
      "model number: 3, seed number: 77 error: -622831600718.3041\n",
      "model number: 3, seed number: 78 error: -622831605322.1366\n",
      "model number: 3, seed number: 79 error: -622831858087.2911\n",
      "model number: 3, seed number: 80 error: -622831660407.4476\n",
      "model number: 3, seed number: 81 error: -622831854766.9\n",
      "model number: 3, seed number: 82 error: -622831526047.9489\n",
      "model number: 3, seed number: 83 error: -622831649054.3308\n",
      "model number: 3, seed number: 84 error: -622831817738.9572\n",
      "model number: 3, seed number: 85 error: -622831632400.6399\n",
      "model number: 3, seed number: 86 error: -622831766570.2983\n",
      "model number: 3, seed number: 87 error: -622831895271.6628\n",
      "model number: 3, seed number: 88 error: -622831792844.2891\n",
      "model number: 3, seed number: 89 error: -622831694257.966\n",
      "model number: 3, seed number: 90 error: -622831690404.0359\n",
      "model number: 3, seed number: 91 error: -622831753282.0992\n",
      "model number: 3, seed number: 92 error: -622831843639.2399\n",
      "model number: 3, seed number: 93 error: -622831670286.8873\n",
      "model number: 3, seed number: 94 error: -622831597131.2562\n",
      "model number: 3, seed number: 95 error: -622831934390.1718\n",
      "model number: 3, seed number: 96 error: -622831828520.5802\n",
      "model number: 3, seed number: 97 error: -622831644739.6843\n",
      "model number: 3, seed number: 98 error: -622831801722.8463\n",
      "model number: 3, seed number: 99 error: -622831786565.0521\n",
      "model number: 3, seed number: 100 error: -622831684439.457\n",
      "model number: 3, seed number: 101 error: -622831579775.7067\n",
      "model number: 3, seed number: 102 error: -622831762949.8441\n",
      "model number: 3, seed number: 103 error: -622831770768.5901\n",
      "model number: 3, seed number: 104 error: -622831695179.9136\n",
      "model number: 3, seed number: 105 error: -622831749597.2866\n",
      "model number: 3, seed number: 106 error: -622831560076.9175\n",
      "model number: 3, seed number: 107 error: -622831865904.5483\n",
      "model number: 3, seed number: 108 error: -622831618655.0134\n",
      "model number: 3, seed number: 109 error: -622831663437.7518\n",
      "model number: 3, seed number: 110 error: -622831526527.3713\n",
      "model number: 3, seed number: 111 error: -622831727001.658\n",
      "model number: 3, seed number: 112 error: -622831783233.2926\n",
      "model number: 3, seed number: 113 error: -622831778033.0242\n",
      "model number: 3, seed number: 114 error: -622831753051.6608\n",
      "model number: 3, seed number: 115 error: -622831775596.7406\n",
      "model number: 3, seed number: 116 error: -622831697790.0553\n",
      "model number: 3, seed number: 117 error: -622831674943.9768\n",
      "model number: 3, seed number: 118 error: -622831958431.0891\n",
      "model number: 3, seed number: 119 error: -622831781473.8795\n",
      "model number: 3, seed number: 120 error: -622831886554.5985\n",
      "model number: 3, seed number: 121 error: -622831803947.0286\n",
      "model number: 3, seed number: 122 error: -622831726324.2771\n",
      "model number: 3, seed number: 123 error: -622831823649.3259\n",
      "model number: 3, seed number: 124 error: -622831818532.0966\n",
      "model number: 3, seed number: 125 error: -622831838607.3949\n",
      "model number: 3, seed number: 126 error: -622831612409.4176\n",
      "model number: 3, seed number: 127 error: -622831786772.8596\n",
      "model number: 3, seed number: 128 error: -622831611365.5499\n",
      "model number: 3, seed number: 129 error: -622831757247.4507\n",
      "model number: 3, seed number: 130 error: -622831885153.2299\n",
      "model number: 3, seed number: 131 error: -622831568185.2765\n",
      "model number: 3, seed number: 132 error: -622831701312.6783\n",
      "model number: 3, seed number: 133 error: -622831710256.5708\n",
      "model number: 3, seed number: 134 error: -622831785038.8191\n",
      "model number: 3, seed number: 135 error: -622831726097.8014\n",
      "model number: 3, seed number: 136 error: -622831751846.899\n",
      "model number: 3, seed number: 137 error: -622831776699.5216\n",
      "model number: 3, seed number: 138 error: -622831719339.2604\n",
      "model number: 3, seed number: 139 error: -622831648279.6272\n",
      "model number: 3, seed number: 140 error: -622831753933.175\n",
      "model number: 3, seed number: 141 error: -622831732846.2593\n",
      "model number: 3, seed number: 142 error: -622831722324.7631\n",
      "model number: 3, seed number: 143 error: -622831672112.6268\n",
      "model number: 3, seed number: 144 error: -622831776586.871\n",
      "model number: 3, seed number: 145 error: -622831676107.0377\n",
      "model number: 3, seed number: 146 error: -622831680878.7186\n",
      "model number: 3, seed number: 147 error: -622831692452.592\n",
      "model number: 3, seed number: 148 error: -622831655808.648\n",
      "model number: 3, seed number: 149 error: -622831722515.5837\n",
      "model number: 3, seed number: 150 error: -622831708142.6206\n",
      "model number: 3, seed number: 151 error: -622831772582.2662\n",
      "model number: 3, seed number: 152 error: -622831792511.3502\n",
      "model number: 3, seed number: 153 error: -622831728679.0281\n",
      "model number: 3, seed number: 154 error: -622831792808.2843\n",
      "model number: 3, seed number: 155 error: -622831810991.7684\n",
      "model number: 3, seed number: 156 error: -622831749253.0431\n",
      "model number: 3, seed number: 157 error: -622831719052.0782\n",
      "model number: 3, seed number: 158 error: -622831807702.4879\n",
      "model number: 3, seed number: 159 error: -622831883990.8468\n",
      "model number: 3, seed number: 160 error: -622831666469.0657\n",
      "model number: 3, seed number: 161 error: -622831795944.5265\n",
      "model number: 3, seed number: 162 error: -622831739603.8551\n",
      "model number: 3, seed number: 163 error: -622831661297.3738\n",
      "model number: 3, seed number: 164 error: -622831661753.9448\n",
      "model number: 3, seed number: 165 error: -622831632166.8517\n",
      "model number: 3, seed number: 166 error: -622831814680.7587\n",
      "model number: 3, seed number: 167 error: -622831772362.55\n",
      "model number: 3, seed number: 168 error: -622831776284.0543\n",
      "model number: 3, seed number: 169 error: -622831678172.6167\n",
      "model number: 3, seed number: 170 error: -622831594415.1135\n",
      "model number: 3, seed number: 171 error: -622831823754.2098\n",
      "model number: 3, seed number: 172 error: -622831741149.2389\n",
      "model number: 3, seed number: 173 error: -622831829501.7981\n",
      "model number: 3, seed number: 174 error: -622831673538.6036\n",
      "model number: 3, seed number: 175 error: -622831668981.9132\n",
      "model number: 3, seed number: 176 error: -622831766955.4604\n",
      "model number: 3, seed number: 177 error: -622831721202.2162\n",
      "model number: 3, seed number: 178 error: -622831654224.0599\n",
      "model number: 3, seed number: 179 error: -622831908177.3772\n",
      "model number: 3, seed number: 180 error: -622831699956.7312\n",
      "model number: 3, seed number: 181 error: -622831664054.311\n",
      "model number: 3, seed number: 182 error: -622831748270.3055\n",
      "model number: 3, seed number: 183 error: -622831787340.2843\n",
      "model number: 3, seed number: 184 error: -622831779204.3138\n",
      "model number: 3, seed number: 185 error: -622831700580.4247\n",
      "model number: 3, seed number: 186 error: -622831760229.8359\n",
      "model number: 3, seed number: 187 error: -622831705957.2029\n",
      "model number: 3, seed number: 188 error: -622831617328.1504\n",
      "model number: 3, seed number: 189 error: -622831755976.7848\n",
      "model number: 3, seed number: 190 error: -622831753011.8868\n",
      "model number: 3, seed number: 191 error: -622831822879.6534\n",
      "model number: 3, seed number: 192 error: -622831876113.1007\n",
      "model number: 3, seed number: 193 error: -622831741348.9583\n",
      "model number: 3, seed number: 194 error: -622831459276.811\n",
      "model number: 3, seed number: 195 error: -622831748041.3173\n",
      "model number: 3, seed number: 196 error: -622831696598.0348\n",
      "model number: 3, seed number: 197 error: -622831670553.031\n",
      "model number: 3, seed number: 198 error: -622831654764.4285\n",
      "model number: 3, seed number: 199 error: -622831781278.866\n",
      "model number: 3, seed number: 200 error: -622831698722.9772\n",
      "model number: 3, seed number: 201 error: -622831657718.0159\n",
      "model number: 3, seed number: 202 error: -622831729779.2056\n",
      "model number: 3, seed number: 203 error: -622831799083.7444\n",
      "model number: 3, seed number: 204 error: -622831785259.2683\n",
      "model number: 3, seed number: 205 error: -622831530154.2511\n",
      "model number: 3, seed number: 206 error: -622831793480.8651\n",
      "model number: 3, seed number: 207 error: -622831865171.4391\n",
      "model number: 3, seed number: 208 error: -622831783216.8065\n",
      "model number: 3, seed number: 209 error: -622831799141.6945\n",
      "model number: 3, seed number: 210 error: -622831759474.5519\n",
      "model number: 3, seed number: 211 error: -622831773363.0792\n",
      "model number: 3, seed number: 212 error: -622831660094.0416\n",
      "model number: 3, seed number: 213 error: -622831610479.4995\n",
      "model number: 3, seed number: 214 error: -622831704308.8798\n",
      "model number: 3, seed number: 215 error: -622831836873.2955\n",
      "model number: 3, seed number: 216 error: -622831755671.9044\n",
      "model number: 3, seed number: 217 error: -622831771049.512\n",
      "model number: 3, seed number: 218 error: -622831863117.0753\n",
      "model number: 3, seed number: 219 error: -622831846332.4535\n",
      "model number: 3, seed number: 220 error: -622831871925.923\n",
      "model number: 3, seed number: 221 error: -622831669091.6151\n",
      "model number: 3, seed number: 222 error: -622831581872.6395\n",
      "model number: 3, seed number: 223 error: -622831683132.9718\n",
      "model number: 3, seed number: 224 error: -622831749837.4305\n",
      "model number: 3, seed number: 225 error: -622831751540.5217\n",
      "model number: 3, seed number: 226 error: -622831708626.814\n",
      "model number: 3, seed number: 227 error: -622831800894.4692\n",
      "model number: 3, seed number: 228 error: -622831730010.5344\n",
      "model number: 3, seed number: 229 error: -622831862426.0266\n",
      "model number: 3, seed number: 230 error: -622831681491.7318\n",
      "model number: 3, seed number: 231 error: -622831748292.6486\n",
      "model number: 3, seed number: 232 error: -622831824599.2812\n",
      "model number: 3, seed number: 233 error: -622831835028.823\n",
      "model number: 3, seed number: 234 error: -622831593763.8414\n",
      "model number: 3, seed number: 235 error: -622831658242.9865\n",
      "model number: 3, seed number: 236 error: -622831661147.8302\n",
      "model number: 3, seed number: 237 error: -622831679184.2699\n",
      "model number: 3, seed number: 238 error: -622831707089.4242\n",
      "model number: 3, seed number: 239 error: -622831872246.0981\n",
      "model number: 3, seed number: 240 error: -622831748305.2384\n",
      "model number: 3, seed number: 241 error: -622831787379.3398\n",
      "model number: 3, seed number: 242 error: -622831785429.7395\n",
      "model number: 3, seed number: 243 error: -622831799291.9763\n",
      "model number: 3, seed number: 244 error: -622831619069.0979\n",
      "model number: 3, seed number: 245 error: -622831836820.1742\n",
      "model number: 3, seed number: 246 error: -622831729810.9438\n",
      "model number: 3, seed number: 247 error: -622831681866.1815\n",
      "model number: 3, seed number: 248 error: -622831765820.4115\n",
      "model number: 3, seed number: 249 error: -622831964703.6636\n",
      "model number: 3, seed number: 250 error: -622831870160.8519\n",
      "model number: 3, seed number: 251 error: -622831696434.1412\n",
      "model number: 3, seed number: 252 error: -622831704113.8661\n",
      "model number: 3, seed number: 253 error: -622831777151.8177\n",
      "model number: 3, seed number: 254 error: -622831734085.0433\n",
      "model number: 3, seed number: 255 error: -622831628511.6191\n",
      "model number: 3, seed number: 256 error: -622831814486.471\n",
      "model number: 3, seed number: 257 error: -622831917474.3773\n",
      "model number: 3, seed number: 258 error: -622831660666.5573\n",
      "model number: 3, seed number: 259 error: -622831888886.8151\n",
      "model number: 3, seed number: 260 error: -622831735861.093\n",
      "model number: 3, seed number: 261 error: -622831672245.7026\n",
      "model number: 3, seed number: 262 error: -622831761847.7655\n",
      "model number: 3, seed number: 263 error: -622831645093.8572\n",
      "model number: 3, seed number: 264 error: -622831815605.1877\n",
      "model number: 3, seed number: 265 error: -622831794730.8373\n",
      "model number: 3, seed number: 266 error: -622831896202.892\n",
      "model number: 3, seed number: 267 error: -622831812456.8794\n",
      "model number: 3, seed number: 268 error: -622831743967.832\n",
      "model number: 3, seed number: 269 error: -622831656976.7313\n",
      "model number: 3, seed number: 270 error: -622831778958.6484\n",
      "model number: 3, seed number: 271 error: -622831712162.9238\n",
      "model number: 3, seed number: 272 error: -622831737272.284\n",
      "model number: 3, seed number: 273 error: -622831754072.6967\n",
      "model number: 3, seed number: 274 error: -622831665366.1187\n",
      "model number: 3, seed number: 275 error: -622831758730.7214\n",
      "model number: 3, seed number: 276 error: -622831767485.3054\n",
      "model number: 3, seed number: 277 error: -622831818288.0482\n",
      "model number: 3, seed number: 278 error: -622831646256.6509\n",
      "model number: 3, seed number: 279 error: -622831707089.3798\n",
      "model number: 3, seed number: 280 error: -622831775292.3179\n",
      "model number: 3, seed number: 281 error: -622831681623.7513\n",
      "model number: 3, seed number: 282 error: -622831753860.6271\n",
      "model number: 3, seed number: 283 error: -622831681805.5125\n",
      "model number: 3, seed number: 284 error: -622831803117.0813\n",
      "model number: 3, seed number: 285 error: -622831817887.5803\n",
      "model number: 3, seed number: 286 error: -622831731950.3334\n",
      "model number: 3, seed number: 287 error: -622831687595.2361\n",
      "model number: 3, seed number: 288 error: -622831730166.0688\n",
      "model number: 3, seed number: 289 error: -622831724857.5841\n",
      "model number: 3, seed number: 290 error: -622831564222.171\n",
      "model number: 3, seed number: 291 error: -622831784142.1597\n",
      "model number: 3, seed number: 292 error: -622831694778.3326\n",
      "model number: 3, seed number: 293 error: -622831722934.8151\n",
      "model number: 3, seed number: 294 error: -622831769962.9406\n",
      "model number: 3, seed number: 295 error: -622831823724.6814\n",
      "model number: 3, seed number: 296 error: -622831666458.0354\n",
      "model number: 3, seed number: 297 error: -622831867863.211\n",
      "model number: 3, seed number: 298 error: -622831561634.1337\n",
      "model number: 3, seed number: 299 error: -622831707166.5542\n",
      "model number: 4, seed number: 0 error: -622831643135.7406\n",
      "model number: 4, seed number: 1 error: -622831651547.2264\n",
      "model number: 4, seed number: 2 error: -622831638381.5822\n",
      "model number: 4, seed number: 3 error: -622831640737.3854\n",
      "model number: 4, seed number: 4 error: -622831621902.6866\n",
      "model number: 4, seed number: 5 error: -622831636471.1454\n",
      "model number: 4, seed number: 6 error: -622831643992.6295\n",
      "model number: 4, seed number: 7 error: -622831628928.059\n",
      "model number: 4, seed number: 8 error: -622831646265.1602\n",
      "model number: 4, seed number: 9 error: -622831638853.7858\n",
      "model number: 4, seed number: 10 error: -622831649168.5807\n",
      "model number: 4, seed number: 11 error: -622831637092.3116\n",
      "model number: 4, seed number: 12 error: -622831636559.0643\n",
      "model number: 4, seed number: 13 error: -622831620874.7169\n",
      "model number: 4, seed number: 14 error: -622831639946.2733\n",
      "model number: 4, seed number: 15 error: -622831621326.1857\n",
      "model number: 4, seed number: 16 error: -622831622519.0875\n",
      "model number: 4, seed number: 17 error: -622831633425.8715\n",
      "model number: 4, seed number: 18 error: -622831631166.953\n",
      "model number: 4, seed number: 19 error: -622831626191.4457\n",
      "model number: 4, seed number: 20 error: -622831628485.1965\n",
      "model number: 4, seed number: 21 error: -622831638175.2727\n",
      "model number: 4, seed number: 22 error: -622831631597.5868\n",
      "model number: 4, seed number: 23 error: -622831638899.1482\n",
      "model number: 4, seed number: 24 error: -622831640277.8337\n",
      "model number: 4, seed number: 25 error: -622831626373.0111\n",
      "model number: 4, seed number: 26 error: -622831637385.8273\n",
      "model number: 4, seed number: 27 error: -622831634283.4023\n",
      "model number: 4, seed number: 28 error: -622831637912.0881\n",
      "model number: 4, seed number: 29 error: -622831638233.7065\n",
      "model number: 4, seed number: 30 error: -622831624440.3114\n",
      "model number: 4, seed number: 31 error: -622831628301.9407\n",
      "model number: 4, seed number: 32 error: -622831632449.043\n",
      "model number: 4, seed number: 33 error: -622831638930.0936\n",
      "model number: 4, seed number: 34 error: -622831634360.4282\n",
      "model number: 4, seed number: 35 error: -622831647896.0875\n",
      "model number: 4, seed number: 36 error: -622831629247.6305\n",
      "model number: 4, seed number: 37 error: -622831632060.6277\n",
      "model number: 4, seed number: 38 error: -622831645987.3889\n",
      "model number: 4, seed number: 39 error: -622831639144.0493\n",
      "model number: 4, seed number: 40 error: -622831634885.3146\n",
      "model number: 4, seed number: 41 error: -622831640037.1838\n",
      "model number: 4, seed number: 42 error: -622831655857.8864\n",
      "model number: 4, seed number: 43 error: -622831644593.0758\n",
      "model number: 4, seed number: 44 error: -622831656543.5125\n",
      "model number: 4, seed number: 45 error: -622831630755.6301\n",
      "model number: 4, seed number: 46 error: -622831641309.4154\n",
      "model number: 4, seed number: 47 error: -622831650307.1664\n",
      "model number: 4, seed number: 48 error: -622831644866.8934\n",
      "model number: 4, seed number: 49 error: -622831623847.0449\n",
      "model number: 4, seed number: 50 error: -622831639816.0316\n",
      "model number: 4, seed number: 51 error: -622831638634.2167\n",
      "model number: 4, seed number: 52 error: -622831633676.1833\n",
      "model number: 4, seed number: 53 error: -622831636710.0999\n",
      "model number: 4, seed number: 54 error: -622831647820.8016\n",
      "model number: 4, seed number: 55 error: -622831640261.3378\n",
      "model number: 4, seed number: 56 error: -622831637052.372\n",
      "model number: 4, seed number: 57 error: -622831627100.894\n",
      "model number: 4, seed number: 58 error: -622831632495.653\n",
      "model number: 4, seed number: 59 error: -622831632660.2418\n",
      "model number: 4, seed number: 60 error: -622831634059.8683\n",
      "model number: 4, seed number: 61 error: -622831638856.7368\n",
      "model number: 4, seed number: 62 error: -622831611573.7173\n",
      "model number: 4, seed number: 63 error: -622831635341.4586\n",
      "model number: 4, seed number: 64 error: -622831630592.5723\n",
      "model number: 4, seed number: 65 error: -622831636876.8043\n",
      "model number: 4, seed number: 66 error: -622831651359.416\n",
      "model number: 4, seed number: 67 error: -622831642983.9403\n",
      "model number: 4, seed number: 68 error: -622831643860.0212\n",
      "model number: 4, seed number: 69 error: -622831633556.4052\n",
      "model number: 4, seed number: 70 error: -622831648956.5886\n",
      "model number: 4, seed number: 71 error: -622831637830.9698\n",
      "model number: 4, seed number: 72 error: -622831641437.1772\n",
      "model number: 4, seed number: 73 error: -622831640432.1698\n",
      "model number: 4, seed number: 74 error: -622831639752.7872\n",
      "model number: 4, seed number: 75 error: -622831628373.6821\n",
      "model number: 4, seed number: 76 error: -622831649725.1552\n",
      "model number: 4, seed number: 77 error: -622831626564.4972\n",
      "model number: 4, seed number: 78 error: -622831644076.0658\n",
      "model number: 4, seed number: 79 error: -622831635981.0945\n",
      "model number: 4, seed number: 80 error: -622831649496.2792\n",
      "model number: 4, seed number: 81 error: -622831643605.4429\n",
      "model number: 4, seed number: 82 error: -622831615556.1002\n",
      "model number: 4, seed number: 83 error: -622831627374.3594\n",
      "model number: 4, seed number: 84 error: -622831644659.2358\n",
      "model number: 4, seed number: 85 error: -622831647347.547\n",
      "model number: 4, seed number: 86 error: -622831648619.0496\n",
      "model number: 4, seed number: 87 error: -622831649004.9313\n",
      "model number: 4, seed number: 88 error: -622831647808.1432\n",
      "model number: 4, seed number: 89 error: -622831642201.4276\n",
      "model number: 4, seed number: 90 error: -622831628078.4183\n",
      "model number: 4, seed number: 91 error: -622831644190.1722\n",
      "model number: 4, seed number: 92 error: -622831617298.8901\n",
      "model number: 4, seed number: 93 error: -622831624213.647\n",
      "model number: 4, seed number: 94 error: -622831638487.0168\n",
      "model number: 4, seed number: 95 error: -622831628318.8942\n",
      "model number: 4, seed number: 96 error: -622831629678.231\n",
      "model number: 4, seed number: 97 error: -622831639255.3792\n",
      "model number: 4, seed number: 98 error: -622831640049.1659\n",
      "model number: 4, seed number: 99 error: -622831648489.9058\n",
      "model number: 4, seed number: 100 error: -622831630396.3031\n",
      "model number: 4, seed number: 101 error: -622831640695.7863\n",
      "model number: 4, seed number: 102 error: -622831649541.3032\n",
      "model number: 4, seed number: 103 error: -622831645017.099\n",
      "model number: 4, seed number: 104 error: -622831631808.1562\n",
      "model number: 4, seed number: 105 error: -622831644705.2509\n",
      "model number: 4, seed number: 106 error: -622831647900.1637\n",
      "model number: 4, seed number: 107 error: -622831629638.6808\n",
      "model number: 4, seed number: 108 error: -622831630307.9657\n",
      "model number: 4, seed number: 109 error: -622831640625.9214\n",
      "model number: 4, seed number: 110 error: -622831625904.9154\n",
      "model number: 4, seed number: 111 error: -622831647627.8773\n",
      "model number: 4, seed number: 112 error: -622831637592.2577\n",
      "model number: 4, seed number: 113 error: -622831636327.7805\n",
      "model number: 4, seed number: 114 error: -622831632134.1432\n",
      "model number: 4, seed number: 115 error: -622831639296.057\n",
      "model number: 4, seed number: 116 error: -622831621373.9513\n",
      "model number: 4, seed number: 117 error: -622831647561.4824\n",
      "model number: 4, seed number: 118 error: -622831637494.7855\n",
      "model number: 4, seed number: 119 error: -622831630983.6793\n",
      "model number: 4, seed number: 120 error: -622831637268.0876\n",
      "model number: 4, seed number: 121 error: -622831624766.3997\n",
      "model number: 4, seed number: 122 error: -622831627002.0665\n",
      "model number: 4, seed number: 123 error: -622831623702.1699\n",
      "model number: 4, seed number: 124 error: -622831628909.6323\n",
      "model number: 4, seed number: 125 error: -622831635894.6759\n",
      "model number: 4, seed number: 126 error: -622831623535.1339\n",
      "model number: 4, seed number: 127 error: -622831636447.6863\n",
      "model number: 4, seed number: 128 error: -622831636074.5736\n",
      "model number: 4, seed number: 129 error: -622831651444.9447\n",
      "model number: 4, seed number: 130 error: -622831627433.4254\n",
      "model number: 4, seed number: 131 error: -622831641859.0942\n",
      "model number: 4, seed number: 132 error: -622831655300.7183\n",
      "model number: 4, seed number: 133 error: -622831639882.901\n",
      "model number: 4, seed number: 134 error: -622831646412.6656\n",
      "model number: 4, seed number: 135 error: -622831625912.8961\n",
      "model number: 4, seed number: 136 error: -622831651242.5898\n",
      "model number: 4, seed number: 137 error: -622831626613.7643\n",
      "model number: 4, seed number: 138 error: -622831635333.9465\n",
      "model number: 4, seed number: 139 error: -622831630994.3181\n",
      "model number: 4, seed number: 140 error: -622831642223.8439\n",
      "model number: 4, seed number: 141 error: -622831640366.7556\n",
      "model number: 4, seed number: 142 error: -622831642972.9219\n",
      "model number: 4, seed number: 143 error: -622831633087.3887\n",
      "model number: 4, seed number: 144 error: -622831624468.0277\n",
      "model number: 4, seed number: 145 error: -622831628769.9232\n",
      "model number: 4, seed number: 146 error: -622831634582.9841\n",
      "model number: 4, seed number: 147 error: -622831636500.6898\n",
      "model number: 4, seed number: 148 error: -622831633799.4272\n",
      "model number: 4, seed number: 149 error: -622831629395.1534\n",
      "model number: 4, seed number: 150 error: -622831639469.362\n",
      "model number: 4, seed number: 151 error: -622831628387.1019\n",
      "model number: 4, seed number: 152 error: -622831627026.3639\n",
      "model number: 4, seed number: 153 error: -622831641921.5089\n",
      "model number: 4, seed number: 154 error: -622831622406.0206\n",
      "model number: 4, seed number: 155 error: -622831616450.858\n",
      "model number: 4, seed number: 156 error: -622831642216.4507\n",
      "model number: 4, seed number: 157 error: -622831631831.317\n",
      "model number: 4, seed number: 158 error: -622831629580.1011\n",
      "model number: 4, seed number: 159 error: -622831639866.4568\n",
      "model number: 4, seed number: 160 error: -622831648239.1323\n",
      "model number: 4, seed number: 161 error: -622831626398.4818\n",
      "model number: 4, seed number: 162 error: -622831612095.1984\n",
      "model number: 4, seed number: 163 error: -622831637581.3785\n",
      "model number: 4, seed number: 164 error: -622831624669.0275\n",
      "model number: 4, seed number: 165 error: -622831636719.7509\n",
      "model number: 4, seed number: 166 error: -622831640796.765\n",
      "model number: 4, seed number: 167 error: -622831618470.5018\n",
      "model number: 4, seed number: 168 error: -622831628655.6399\n",
      "model number: 4, seed number: 169 error: -622831637810.721\n",
      "model number: 4, seed number: 170 error: -622831631121.7767\n",
      "model number: 4, seed number: 171 error: -622831635471.8064\n",
      "model number: 4, seed number: 172 error: -622831618796.2975\n",
      "model number: 4, seed number: 173 error: -622831640439.7156\n",
      "model number: 4, seed number: 174 error: -622831639680.8177\n",
      "model number: 4, seed number: 175 error: -622831643312.4403\n",
      "model number: 4, seed number: 176 error: -622831615024.1422\n",
      "model number: 4, seed number: 177 error: -622831646696.7252\n",
      "model number: 4, seed number: 178 error: -622831636971.4984\n",
      "model number: 4, seed number: 179 error: -622831631236.6309\n",
      "model number: 4, seed number: 180 error: -622831629772.2795\n",
      "model number: 4, seed number: 181 error: -622831642151.2628\n",
      "model number: 4, seed number: 182 error: -622831652177.682\n",
      "model number: 4, seed number: 183 error: -622831609869.5739\n",
      "model number: 4, seed number: 184 error: -622831630729.2048\n",
      "model number: 4, seed number: 185 error: -622831630696.2832\n",
      "model number: 4, seed number: 186 error: -622831634709.9186\n",
      "model number: 4, seed number: 187 error: -622831637139.1443\n",
      "model number: 4, seed number: 188 error: -622831641579.6927\n",
      "model number: 4, seed number: 189 error: -622831633198.7877\n",
      "model number: 4, seed number: 190 error: -622831622859.8884\n",
      "model number: 4, seed number: 191 error: -622831616287.913\n",
      "model number: 4, seed number: 192 error: -622831641479.6224\n",
      "model number: 4, seed number: 193 error: -622831649122.5364\n",
      "model number: 4, seed number: 194 error: -622831630998.8165\n",
      "model number: 4, seed number: 195 error: -622831635229.213\n",
      "model number: 4, seed number: 196 error: -622831630703.3796\n",
      "model number: 4, seed number: 197 error: -622831631219.2198\n",
      "model number: 4, seed number: 198 error: -622831630781.5588\n",
      "model number: 4, seed number: 199 error: -622831643658.6919\n",
      "model number: 4, seed number: 200 error: -622831636897.781\n",
      "model number: 4, seed number: 201 error: -622831640206.2008\n",
      "model number: 4, seed number: 202 error: -622831636067.5844\n",
      "model number: 4, seed number: 203 error: -622831638338.9918\n",
      "model number: 4, seed number: 204 error: -622831630855.5343\n",
      "model number: 4, seed number: 205 error: -622831636998.2356\n",
      "model number: 4, seed number: 206 error: -622831639646.5309\n",
      "model number: 4, seed number: 207 error: -622831625659.1995\n",
      "model number: 4, seed number: 208 error: -622831629777.1968\n",
      "model number: 4, seed number: 209 error: -622831637132.5153\n",
      "model number: 4, seed number: 210 error: -622831632640.0424\n",
      "model number: 4, seed number: 211 error: -622831622087.1505\n",
      "model number: 4, seed number: 212 error: -622831626228.2747\n",
      "model number: 4, seed number: 213 error: -622831642686.4684\n",
      "model number: 4, seed number: 214 error: -622831636290.7516\n",
      "model number: 4, seed number: 215 error: -622831636766.7786\n",
      "model number: 4, seed number: 216 error: -622831624522.8481\n",
      "model number: 4, seed number: 217 error: -622831629948.2732\n",
      "model number: 4, seed number: 218 error: -622831627999.6343\n",
      "model number: 4, seed number: 219 error: -622831634994.4536\n",
      "model number: 4, seed number: 220 error: -622831635237.3809\n",
      "model number: 4, seed number: 221 error: -622831646764.1671\n",
      "model number: 4, seed number: 222 error: -622831636486.488\n",
      "model number: 4, seed number: 223 error: -622831630801.0464\n",
      "model number: 4, seed number: 224 error: -622831637717.5691\n",
      "model number: 4, seed number: 225 error: -622831620506.8075\n",
      "model number: 4, seed number: 226 error: -622831626448.158\n",
      "model number: 4, seed number: 227 error: -622831654517.8746\n",
      "model number: 4, seed number: 228 error: -622831643501.4889\n",
      "model number: 4, seed number: 229 error: -622831626706.1554\n",
      "model number: 4, seed number: 230 error: -622831629892.0168\n",
      "model number: 4, seed number: 231 error: -622831631193.1469\n",
      "model number: 4, seed number: 232 error: -622831632525.3804\n",
      "model number: 4, seed number: 233 error: -622831637255.481\n",
      "model number: 4, seed number: 234 error: -622831633589.1344\n",
      "model number: 4, seed number: 235 error: -622831635167.3203\n",
      "model number: 4, seed number: 236 error: -622831637436.5354\n",
      "model number: 4, seed number: 237 error: -622831635964.019\n",
      "model number: 4, seed number: 238 error: -622831638417.7758\n",
      "model number: 4, seed number: 239 error: -622831648954.9934\n",
      "model number: 4, seed number: 240 error: -622831634664.682\n",
      "model number: 4, seed number: 241 error: -622831630476.3372\n",
      "model number: 4, seed number: 242 error: -622831621913.8296\n",
      "model number: 4, seed number: 243 error: -622831633128.2428\n",
      "model number: 4, seed number: 244 error: -622831666453.6317\n",
      "model number: 4, seed number: 245 error: -622831632082.0979\n",
      "model number: 4, seed number: 246 error: -622831632436.872\n",
      "model number: 4, seed number: 247 error: -622831635548.2977\n",
      "model number: 4, seed number: 248 error: -622831625391.1069\n",
      "model number: 4, seed number: 249 error: -622831622161.759\n",
      "model number: 4, seed number: 250 error: -622831636632.1274\n",
      "model number: 4, seed number: 251 error: -622831634029.9125\n",
      "model number: 4, seed number: 252 error: -622831637466.5409\n",
      "model number: 4, seed number: 253 error: -622831645226.1202\n",
      "model number: 4, seed number: 254 error: -622831625491.101\n",
      "model number: 4, seed number: 255 error: -622831641439.2996\n",
      "model number: 4, seed number: 256 error: -622831636030.776\n",
      "model number: 4, seed number: 257 error: -622831634585.522\n",
      "model number: 4, seed number: 258 error: -622831646126.4995\n",
      "model number: 4, seed number: 259 error: -622831640345.7153\n",
      "model number: 4, seed number: 260 error: -622831637338.8722\n",
      "model number: 4, seed number: 261 error: -622831637512.9519\n",
      "model number: 4, seed number: 262 error: -622831639466.3826\n",
      "model number: 4, seed number: 263 error: -622831636037.6942\n",
      "model number: 4, seed number: 264 error: -622831640946.4512\n",
      "model number: 4, seed number: 265 error: -622831623461.0035\n",
      "model number: 4, seed number: 266 error: -622831644867.3129\n",
      "model number: 4, seed number: 267 error: -622831630488.2916\n",
      "model number: 4, seed number: 268 error: -622831645468.6305\n",
      "model number: 4, seed number: 269 error: -622831635648.4904\n",
      "model number: 4, seed number: 270 error: -622831641043.3796\n",
      "model number: 4, seed number: 271 error: -622831637187.0383\n",
      "model number: 4, seed number: 272 error: -622831637468.427\n",
      "model number: 4, seed number: 273 error: -622831640017.0269\n",
      "model number: 4, seed number: 274 error: -622831644762.6393\n",
      "model number: 4, seed number: 275 error: -622831648014.8374\n",
      "model number: 4, seed number: 276 error: -622831639778.0317\n",
      "model number: 4, seed number: 277 error: -622831629307.538\n",
      "model number: 4, seed number: 278 error: -622831632054.8599\n",
      "model number: 4, seed number: 279 error: -622831635712.4681\n",
      "model number: 4, seed number: 280 error: -622831633045.725\n",
      "model number: 4, seed number: 281 error: -622831649401.8546\n",
      "model number: 4, seed number: 282 error: -622831629049.6713\n",
      "model number: 4, seed number: 283 error: -622831645480.5887\n",
      "model number: 4, seed number: 284 error: -622831637739.2042\n",
      "model number: 4, seed number: 285 error: -622831646759.0729\n",
      "model number: 4, seed number: 286 error: -622831655265.3458\n",
      "model number: 4, seed number: 287 error: -622831629868.368\n",
      "model number: 4, seed number: 288 error: -622831625572.5101\n",
      "model number: 4, seed number: 289 error: -622831629533.9283\n",
      "model number: 4, seed number: 290 error: -622831633112.7917\n",
      "model number: 4, seed number: 291 error: -622831640922.5295\n",
      "model number: 4, seed number: 292 error: -622831636096.2103\n",
      "model number: 4, seed number: 293 error: -622831630814.3186\n",
      "model number: 4, seed number: 294 error: -622831643548.1915\n",
      "model number: 4, seed number: 295 error: -622831646026.9412\n",
      "model number: 4, seed number: 296 error: -622831629581.6046\n",
      "model number: 4, seed number: 297 error: -622831639082.7328\n",
      "model number: 4, seed number: 298 error: -622831631799.5823\n",
      "model number: 4, seed number: 299 error: -622831629560.3312\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "# Leer los hiperparmetros desde el archivo JSON\n",
    "with open('top_5_hyperparameters_conv.json', 'r') as f:\n",
    "    top_hyperparameters = json.load(f)\n",
    "\n",
    "\n",
    "models = []\n",
    "best_seeds= {}\n",
    "prime_seeds = generate_prime_seeds(300)\n",
    "\n",
    "for mode_number, params in enumerate(top_hyperparameters['best_params']):\n",
    "    best_validation_errors = {}\n",
    "    \n",
    "    for seend_number, seed in enumerate(prime_seeds):\n",
    "        model = KerasRegressor(build_fn=create_model, random_state=seed, verbose=0, **params)\n",
    "        \n",
    "        model.fit(X, y)\n",
    "        \n",
    "        model_predictions, _ = predict_next_days(model, feature_dataset, scalers, n_steps, n_features, 5)\n",
    "\n",
    "        error = custom_scoring_validation(validation, model_predictions[-5:])\n",
    "        print(f\"model number: {mode_number}, seed number: {seend_number} error: {error}\")\n",
    "        \n",
    "        if seed not in best_validation_errors or error < best_validation_errors[seed]:\n",
    "            best_validation_errors[seed] = error\n",
    "    \n",
    "    best_seed_for_params = min(best_validation_errors, key=best_validation_errors.get)\n",
    "    best_seeds[str(params)] = best_seed_for_params\n",
    "    \n",
    "    model = KerasRegressor(build_fn=create_model, random_state=best_seed_for_params, verbose=0, **params)\n",
    "    model.fit(X, y)\n",
    "    models.append(model)\n",
    "\n",
    "\n",
    "ensemble = MultivariableVotingRegressor(models)\n",
    "ensemble.fit(X, y)\n",
    "\n",
    "with open('best_seeds_conv.json', 'w') as f:\n",
    "    json.dump(best_seeds, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores predichos para los prximos 5 das:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Number of trades</th>\n",
       "      <th>Close_BTCUSDT</th>\n",
       "      <th>Volume_BTCUSDT</th>\n",
       "      <th>Number_of_trades_BTCUSDT</th>\n",
       "      <th>Close_ETHUSDT</th>\n",
       "      <th>Volume_ETHUSDT</th>\n",
       "      <th>Number_of_trades_ETHUSDT</th>\n",
       "      <th>Close_BNBUSDT</th>\n",
       "      <th>Volume_BNBUSDT</th>\n",
       "      <th>Number_of_trades_BNBUSDT</th>\n",
       "      <th>SMA_20</th>\n",
       "      <th>EMA_20</th>\n",
       "      <th>Upper_Band</th>\n",
       "      <th>Middle_Band</th>\n",
       "      <th>Lower_Band</th>\n",
       "      <th>RSI</th>\n",
       "      <th>buy_1000x_high_coinbase</th>\n",
       "      <th>sell_1000x_high_coinbase</th>\n",
       "      <th>total_trades_coinbase</th>\n",
       "      <th>Tweets_Utilizados</th>\n",
       "      <th>Tweets_Utilizados_coin</th>\n",
       "      <th>Tweets_Utilizados_referentes</th>\n",
       "      <th>Tweets_Utilizados_whale_alert</th>\n",
       "      <th>Buy_1000x_high</th>\n",
       "      <th>sell_1000x_high</th>\n",
       "      <th>total_trades_binance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>0.120167</td>\n",
       "      <td>0.128135</td>\n",
       "      <td>0.115844</td>\n",
       "      <td>0.128164</td>\n",
       "      <td>0.083843</td>\n",
       "      <td>0.918331</td>\n",
       "      <td>0.054236</td>\n",
       "      <td>0.143388</td>\n",
       "      <td>0.694908</td>\n",
       "      <td>0.128726</td>\n",
       "      <td>0.503699</td>\n",
       "      <td>0.820811</td>\n",
       "      <td>0.368893</td>\n",
       "      <td>0.443672</td>\n",
       "      <td>0.138751</td>\n",
       "      <td>0.139408</td>\n",
       "      <td>0.147628</td>\n",
       "      <td>0.138751</td>\n",
       "      <td>0.127228</td>\n",
       "      <td>0.515583</td>\n",
       "      <td>0.151899</td>\n",
       "      <td>0.170213</td>\n",
       "      <td>0.352479</td>\n",
       "      <td>0.649573</td>\n",
       "      <td>0.501971</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.139073</td>\n",
       "      <td>0.156999</td>\n",
       "      <td>0.156988</td>\n",
       "      <td>0.075976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>903</th>\n",
       "      <td>0.058166</td>\n",
       "      <td>0.071053</td>\n",
       "      <td>0.093088</td>\n",
       "      <td>0.209103</td>\n",
       "      <td>0.185831</td>\n",
       "      <td>0.464913</td>\n",
       "      <td>0.177570</td>\n",
       "      <td>-0.265770</td>\n",
       "      <td>0.453063</td>\n",
       "      <td>0.272907</td>\n",
       "      <td>-0.046448</td>\n",
       "      <td>0.128281</td>\n",
       "      <td>0.044855</td>\n",
       "      <td>0.040945</td>\n",
       "      <td>0.214738</td>\n",
       "      <td>0.054068</td>\n",
       "      <td>0.019642</td>\n",
       "      <td>0.249467</td>\n",
       "      <td>0.253264</td>\n",
       "      <td>0.384640</td>\n",
       "      <td>0.117167</td>\n",
       "      <td>0.131928</td>\n",
       "      <td>0.099171</td>\n",
       "      <td>0.148628</td>\n",
       "      <td>0.198215</td>\n",
       "      <td>-0.080837</td>\n",
       "      <td>0.057983</td>\n",
       "      <td>0.256941</td>\n",
       "      <td>0.086346</td>\n",
       "      <td>-0.031942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>0.053865</td>\n",
       "      <td>0.059676</td>\n",
       "      <td>0.107928</td>\n",
       "      <td>0.199356</td>\n",
       "      <td>0.184252</td>\n",
       "      <td>0.469203</td>\n",
       "      <td>0.210182</td>\n",
       "      <td>-0.245169</td>\n",
       "      <td>0.449417</td>\n",
       "      <td>0.268409</td>\n",
       "      <td>-0.053798</td>\n",
       "      <td>0.115805</td>\n",
       "      <td>0.066775</td>\n",
       "      <td>0.029250</td>\n",
       "      <td>0.218476</td>\n",
       "      <td>0.032941</td>\n",
       "      <td>0.009195</td>\n",
       "      <td>0.222476</td>\n",
       "      <td>0.266412</td>\n",
       "      <td>0.398104</td>\n",
       "      <td>0.117548</td>\n",
       "      <td>0.121060</td>\n",
       "      <td>0.119892</td>\n",
       "      <td>0.119854</td>\n",
       "      <td>0.189500</td>\n",
       "      <td>-0.074527</td>\n",
       "      <td>0.063870</td>\n",
       "      <td>0.241400</td>\n",
       "      <td>0.077387</td>\n",
       "      <td>0.013364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>0.058332</td>\n",
       "      <td>0.096123</td>\n",
       "      <td>0.121772</td>\n",
       "      <td>0.178222</td>\n",
       "      <td>0.158552</td>\n",
       "      <td>0.475065</td>\n",
       "      <td>0.232054</td>\n",
       "      <td>-0.236756</td>\n",
       "      <td>0.430063</td>\n",
       "      <td>0.290036</td>\n",
       "      <td>-0.062651</td>\n",
       "      <td>0.131211</td>\n",
       "      <td>0.051065</td>\n",
       "      <td>0.004363</td>\n",
       "      <td>0.227632</td>\n",
       "      <td>0.011619</td>\n",
       "      <td>-0.009679</td>\n",
       "      <td>0.229419</td>\n",
       "      <td>0.275967</td>\n",
       "      <td>0.381162</td>\n",
       "      <td>0.142746</td>\n",
       "      <td>0.123641</td>\n",
       "      <td>0.122658</td>\n",
       "      <td>0.132233</td>\n",
       "      <td>0.202292</td>\n",
       "      <td>-0.073110</td>\n",
       "      <td>0.074081</td>\n",
       "      <td>0.242383</td>\n",
       "      <td>0.066761</td>\n",
       "      <td>0.047663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>0.039550</td>\n",
       "      <td>0.119542</td>\n",
       "      <td>0.142280</td>\n",
       "      <td>0.177967</td>\n",
       "      <td>0.162377</td>\n",
       "      <td>0.460846</td>\n",
       "      <td>0.237408</td>\n",
       "      <td>-0.216928</td>\n",
       "      <td>0.441667</td>\n",
       "      <td>0.277357</td>\n",
       "      <td>-0.064674</td>\n",
       "      <td>0.119679</td>\n",
       "      <td>0.022562</td>\n",
       "      <td>-0.005907</td>\n",
       "      <td>0.201207</td>\n",
       "      <td>-0.026751</td>\n",
       "      <td>-0.026805</td>\n",
       "      <td>0.199530</td>\n",
       "      <td>0.294347</td>\n",
       "      <td>0.382179</td>\n",
       "      <td>0.140136</td>\n",
       "      <td>0.104527</td>\n",
       "      <td>0.153506</td>\n",
       "      <td>0.104307</td>\n",
       "      <td>0.190286</td>\n",
       "      <td>-0.105188</td>\n",
       "      <td>0.088579</td>\n",
       "      <td>0.263886</td>\n",
       "      <td>0.064236</td>\n",
       "      <td>0.034172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>0.032229</td>\n",
       "      <td>0.133685</td>\n",
       "      <td>0.169269</td>\n",
       "      <td>0.173267</td>\n",
       "      <td>0.170423</td>\n",
       "      <td>0.472926</td>\n",
       "      <td>0.219638</td>\n",
       "      <td>-0.244406</td>\n",
       "      <td>0.424583</td>\n",
       "      <td>0.276831</td>\n",
       "      <td>-0.071239</td>\n",
       "      <td>0.109653</td>\n",
       "      <td>0.022438</td>\n",
       "      <td>-0.017584</td>\n",
       "      <td>0.234318</td>\n",
       "      <td>-0.011293</td>\n",
       "      <td>-0.032044</td>\n",
       "      <td>0.180447</td>\n",
       "      <td>0.307913</td>\n",
       "      <td>0.380740</td>\n",
       "      <td>0.134958</td>\n",
       "      <td>0.144291</td>\n",
       "      <td>0.126089</td>\n",
       "      <td>0.095902</td>\n",
       "      <td>0.197616</td>\n",
       "      <td>-0.091180</td>\n",
       "      <td>0.064725</td>\n",
       "      <td>0.260005</td>\n",
       "      <td>0.063938</td>\n",
       "      <td>0.021819</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Open      High       Low     Close  Number of trades  Close_BTCUSDT  \\\n",
       "902  0.120167  0.128135  0.115844  0.128164          0.083843       0.918331   \n",
       "903  0.058166  0.071053  0.093088  0.209103          0.185831       0.464913   \n",
       "904  0.053865  0.059676  0.107928  0.199356          0.184252       0.469203   \n",
       "905  0.058332  0.096123  0.121772  0.178222          0.158552       0.475065   \n",
       "906  0.039550  0.119542  0.142280  0.177967          0.162377       0.460846   \n",
       "907  0.032229  0.133685  0.169269  0.173267          0.170423       0.472926   \n",
       "\n",
       "     Volume_BTCUSDT  Number_of_trades_BTCUSDT  Close_ETHUSDT  Volume_ETHUSDT  \\\n",
       "902        0.054236                  0.143388       0.694908        0.128726   \n",
       "903        0.177570                 -0.265770       0.453063        0.272907   \n",
       "904        0.210182                 -0.245169       0.449417        0.268409   \n",
       "905        0.232054                 -0.236756       0.430063        0.290036   \n",
       "906        0.237408                 -0.216928       0.441667        0.277357   \n",
       "907        0.219638                 -0.244406       0.424583        0.276831   \n",
       "\n",
       "     Number_of_trades_ETHUSDT  Close_BNBUSDT  Volume_BNBUSDT  \\\n",
       "902                  0.503699       0.820811        0.368893   \n",
       "903                 -0.046448       0.128281        0.044855   \n",
       "904                 -0.053798       0.115805        0.066775   \n",
       "905                 -0.062651       0.131211        0.051065   \n",
       "906                 -0.064674       0.119679        0.022562   \n",
       "907                 -0.071239       0.109653        0.022438   \n",
       "\n",
       "     Number_of_trades_BNBUSDT    SMA_20    EMA_20  Upper_Band  Middle_Band  \\\n",
       "902                  0.443672  0.138751  0.139408    0.147628     0.138751   \n",
       "903                  0.040945  0.214738  0.054068    0.019642     0.249467   \n",
       "904                  0.029250  0.218476  0.032941    0.009195     0.222476   \n",
       "905                  0.004363  0.227632  0.011619   -0.009679     0.229419   \n",
       "906                 -0.005907  0.201207 -0.026751   -0.026805     0.199530   \n",
       "907                 -0.017584  0.234318 -0.011293   -0.032044     0.180447   \n",
       "\n",
       "     Lower_Band       RSI  buy_1000x_high_coinbase  sell_1000x_high_coinbase  \\\n",
       "902    0.127228  0.515583                 0.151899                  0.170213   \n",
       "903    0.253264  0.384640                 0.117167                  0.131928   \n",
       "904    0.266412  0.398104                 0.117548                  0.121060   \n",
       "905    0.275967  0.381162                 0.142746                  0.123641   \n",
       "906    0.294347  0.382179                 0.140136                  0.104527   \n",
       "907    0.307913  0.380740                 0.134958                  0.144291   \n",
       "\n",
       "     total_trades_coinbase  Tweets_Utilizados  Tweets_Utilizados_coin  \\\n",
       "902               0.352479           0.649573                0.501971   \n",
       "903               0.099171           0.148628                0.198215   \n",
       "904               0.119892           0.119854                0.189500   \n",
       "905               0.122658           0.132233                0.202292   \n",
       "906               0.153506           0.104307                0.190286   \n",
       "907               0.126089           0.095902                0.197616   \n",
       "\n",
       "     Tweets_Utilizados_referentes  Tweets_Utilizados_whale_alert  \\\n",
       "902                      0.000000                       0.139073   \n",
       "903                     -0.080837                       0.057983   \n",
       "904                     -0.074527                       0.063870   \n",
       "905                     -0.073110                       0.074081   \n",
       "906                     -0.105188                       0.088579   \n",
       "907                     -0.091180                       0.064725   \n",
       "\n",
       "     Buy_1000x_high  sell_1000x_high  total_trades_binance  \n",
       "902        0.156999         0.156988              0.075976  \n",
       "903        0.256941         0.086346             -0.031942  \n",
       "904        0.241400         0.077387              0.013364  \n",
       "905        0.242383         0.066761              0.047663  \n",
       "906        0.263886         0.064236              0.034172  \n",
       "907        0.260005         0.063938              0.021819  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores predichos desnormalizados para los prximos 5 das:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Number of trades</th>\n",
       "      <th>Close_BTCUSDT</th>\n",
       "      <th>Volume_BTCUSDT</th>\n",
       "      <th>Number_of_trades_BTCUSDT</th>\n",
       "      <th>Close_ETHUSDT</th>\n",
       "      <th>Volume_ETHUSDT</th>\n",
       "      <th>Number_of_trades_ETHUSDT</th>\n",
       "      <th>Close_BNBUSDT</th>\n",
       "      <th>Volume_BNBUSDT</th>\n",
       "      <th>Number_of_trades_BNBUSDT</th>\n",
       "      <th>SMA_20</th>\n",
       "      <th>EMA_20</th>\n",
       "      <th>Upper_Band</th>\n",
       "      <th>Middle_Band</th>\n",
       "      <th>Lower_Band</th>\n",
       "      <th>RSI</th>\n",
       "      <th>buy_1000x_high_coinbase</th>\n",
       "      <th>sell_1000x_high_coinbase</th>\n",
       "      <th>total_trades_coinbase</th>\n",
       "      <th>Tweets_Utilizados</th>\n",
       "      <th>Tweets_Utilizados_coin</th>\n",
       "      <th>Tweets_Utilizados_referentes</th>\n",
       "      <th>Tweets_Utilizados_whale_alert</th>\n",
       "      <th>Buy_1000x_high</th>\n",
       "      <th>sell_1000x_high</th>\n",
       "      <th>total_trades_binance</th>\n",
       "      <th>Fecha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.568762</td>\n",
       "      <td>7.314273</td>\n",
       "      <td>8.084068</td>\n",
       "      <td>14.140716</td>\n",
       "      <td>495344.40625</td>\n",
       "      <td>42416.695312</td>\n",
       "      <td>142456.609375</td>\n",
       "      <td>-3652509.00</td>\n",
       "      <td>2722.593018</td>\n",
       "      <td>1.032206e+06</td>\n",
       "      <td>9588.093750</td>\n",
       "      <td>255.560089</td>\n",
       "      <td>282103.78125</td>\n",
       "      <td>123575.445312</td>\n",
       "      <td>13.337129</td>\n",
       "      <td>6.203595</td>\n",
       "      <td>5.185119</td>\n",
       "      <td>14.871467</td>\n",
       "      <td>12.636547</td>\n",
       "      <td>46.455318</td>\n",
       "      <td>27.768692</td>\n",
       "      <td>37.203564</td>\n",
       "      <td>20085.880859</td>\n",
       "      <td>165.505447</td>\n",
       "      <td>181.841339</td>\n",
       "      <td>-1.374227</td>\n",
       "      <td>8.755414</td>\n",
       "      <td>453.509003</td>\n",
       "      <td>166.304886</td>\n",
       "      <td>-51699.308594</td>\n",
       "      <td>2024-03-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.352944</td>\n",
       "      <td>6.729131</td>\n",
       "      <td>8.805325</td>\n",
       "      <td>13.651693</td>\n",
       "      <td>491231.65625</td>\n",
       "      <td>42662.476562</td>\n",
       "      <td>166972.312500</td>\n",
       "      <td>-3345285.25</td>\n",
       "      <td>2708.688477</td>\n",
       "      <td>1.016160e+06</td>\n",
       "      <td>-13281.995117</td>\n",
       "      <td>249.865051</td>\n",
       "      <td>378897.12500</td>\n",
       "      <td>103865.914062</td>\n",
       "      <td>13.502258</td>\n",
       "      <td>5.276132</td>\n",
       "      <td>4.634593</td>\n",
       "      <td>13.678970</td>\n",
       "      <td>13.116061</td>\n",
       "      <td>47.258823</td>\n",
       "      <td>27.858992</td>\n",
       "      <td>34.138977</td>\n",
       "      <td>24282.658203</td>\n",
       "      <td>135.206604</td>\n",
       "      <td>175.209488</td>\n",
       "      <td>-1.266967</td>\n",
       "      <td>9.644383</td>\n",
       "      <td>428.860809</td>\n",
       "      <td>152.266159</td>\n",
       "      <td>32977.546875</td>\n",
       "      <td>2024-03-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.577085</td>\n",
       "      <td>8.603601</td>\n",
       "      <td>9.478132</td>\n",
       "      <td>12.591394</td>\n",
       "      <td>424287.28125</td>\n",
       "      <td>42998.308594</td>\n",
       "      <td>183414.578125</td>\n",
       "      <td>-3219832.75</td>\n",
       "      <td>2634.895752</td>\n",
       "      <td>1.093318e+06</td>\n",
       "      <td>-40826.648438</td>\n",
       "      <td>256.898010</td>\n",
       "      <td>309525.28125</td>\n",
       "      <td>61923.730469</td>\n",
       "      <td>13.906770</td>\n",
       "      <td>4.340058</td>\n",
       "      <td>3.639909</td>\n",
       "      <td>13.985720</td>\n",
       "      <td>13.464526</td>\n",
       "      <td>46.247765</td>\n",
       "      <td>33.830688</td>\n",
       "      <td>34.866688</td>\n",
       "      <td>24842.693359</td>\n",
       "      <td>148.241348</td>\n",
       "      <td>184.944519</td>\n",
       "      <td>-1.242866</td>\n",
       "      <td>11.186186</td>\n",
       "      <td>430.419220</td>\n",
       "      <td>135.614014</td>\n",
       "      <td>97081.687500</td>\n",
       "      <td>2024-03-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.634637</td>\n",
       "      <td>9.808044</td>\n",
       "      <td>10.474800</td>\n",
       "      <td>12.578601</td>\n",
       "      <td>434249.28125</td>\n",
       "      <td>42183.648438</td>\n",
       "      <td>187439.062500</td>\n",
       "      <td>-2924145.50</td>\n",
       "      <td>2679.139404</td>\n",
       "      <td>1.048083e+06</td>\n",
       "      <td>-47120.707031</td>\n",
       "      <td>251.633316</td>\n",
       "      <td>183667.21875</td>\n",
       "      <td>44615.621094</td>\n",
       "      <td>12.739328</td>\n",
       "      <td>2.655619</td>\n",
       "      <td>2.737377</td>\n",
       "      <td>12.665237</td>\n",
       "      <td>14.134830</td>\n",
       "      <td>46.308434</td>\n",
       "      <td>33.212223</td>\n",
       "      <td>29.476658</td>\n",
       "      <td>31090.611328</td>\n",
       "      <td>118.835098</td>\n",
       "      <td>175.807266</td>\n",
       "      <td>-1.788198</td>\n",
       "      <td>13.375440</td>\n",
       "      <td>464.523315</td>\n",
       "      <td>131.658295</td>\n",
       "      <td>71866.585938</td>\n",
       "      <td>2024-03-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.267235</td>\n",
       "      <td>10.535395</td>\n",
       "      <td>11.786478</td>\n",
       "      <td>12.342828</td>\n",
       "      <td>455209.37500</td>\n",
       "      <td>42875.769531</td>\n",
       "      <td>174081.140625</td>\n",
       "      <td>-3333910.75</td>\n",
       "      <td>2614.000977</td>\n",
       "      <td>1.046208e+06</td>\n",
       "      <td>-67547.328125</td>\n",
       "      <td>247.056595</td>\n",
       "      <td>183121.46875</td>\n",
       "      <td>24934.687500</td>\n",
       "      <td>14.202191</td>\n",
       "      <td>3.334253</td>\n",
       "      <td>2.461271</td>\n",
       "      <td>11.822153</td>\n",
       "      <td>14.629593</td>\n",
       "      <td>46.222549</td>\n",
       "      <td>31.985155</td>\n",
       "      <td>40.690067</td>\n",
       "      <td>25537.763672</td>\n",
       "      <td>109.984833</td>\n",
       "      <td>181.385696</td>\n",
       "      <td>-1.550068</td>\n",
       "      <td>9.773538</td>\n",
       "      <td>458.367767</td>\n",
       "      <td>131.190948</td>\n",
       "      <td>48780.425781</td>\n",
       "      <td>2024-03-22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Open       High        Low      Close  Number of trades  Close_BTCUSDT  \\\n",
       "0  6.568762   7.314273   8.084068  14.140716      495344.40625   42416.695312   \n",
       "1  6.352944   6.729131   8.805325  13.651693      491231.65625   42662.476562   \n",
       "2  6.577085   8.603601   9.478132  12.591394      424287.28125   42998.308594   \n",
       "3  5.634637   9.808044  10.474800  12.578601      434249.28125   42183.648438   \n",
       "4  5.267235  10.535395  11.786478  12.342828      455209.37500   42875.769531   \n",
       "\n",
       "   Volume_BTCUSDT  Number_of_trades_BTCUSDT  Close_ETHUSDT  Volume_ETHUSDT  \\\n",
       "0   142456.609375               -3652509.00    2722.593018    1.032206e+06   \n",
       "1   166972.312500               -3345285.25    2708.688477    1.016160e+06   \n",
       "2   183414.578125               -3219832.75    2634.895752    1.093318e+06   \n",
       "3   187439.062500               -2924145.50    2679.139404    1.048083e+06   \n",
       "4   174081.140625               -3333910.75    2614.000977    1.046208e+06   \n",
       "\n",
       "   Number_of_trades_ETHUSDT  Close_BNBUSDT  Volume_BNBUSDT  \\\n",
       "0               9588.093750     255.560089    282103.78125   \n",
       "1             -13281.995117     249.865051    378897.12500   \n",
       "2             -40826.648438     256.898010    309525.28125   \n",
       "3             -47120.707031     251.633316    183667.21875   \n",
       "4             -67547.328125     247.056595    183121.46875   \n",
       "\n",
       "   Number_of_trades_BNBUSDT     SMA_20    EMA_20  Upper_Band  Middle_Band  \\\n",
       "0             123575.445312  13.337129  6.203595    5.185119    14.871467   \n",
       "1             103865.914062  13.502258  5.276132    4.634593    13.678970   \n",
       "2              61923.730469  13.906770  4.340058    3.639909    13.985720   \n",
       "3              44615.621094  12.739328  2.655619    2.737377    12.665237   \n",
       "4              24934.687500  14.202191  3.334253    2.461271    11.822153   \n",
       "\n",
       "   Lower_Band        RSI  buy_1000x_high_coinbase  sell_1000x_high_coinbase  \\\n",
       "0   12.636547  46.455318                27.768692                 37.203564   \n",
       "1   13.116061  47.258823                27.858992                 34.138977   \n",
       "2   13.464526  46.247765                33.830688                 34.866688   \n",
       "3   14.134830  46.308434                33.212223                 29.476658   \n",
       "4   14.629593  46.222549                31.985155                 40.690067   \n",
       "\n",
       "   total_trades_coinbase  Tweets_Utilizados  Tweets_Utilizados_coin  \\\n",
       "0           20085.880859         165.505447              181.841339   \n",
       "1           24282.658203         135.206604              175.209488   \n",
       "2           24842.693359         148.241348              184.944519   \n",
       "3           31090.611328         118.835098              175.807266   \n",
       "4           25537.763672         109.984833              181.385696   \n",
       "\n",
       "   Tweets_Utilizados_referentes  Tweets_Utilizados_whale_alert  \\\n",
       "0                     -1.374227                       8.755414   \n",
       "1                     -1.266967                       9.644383   \n",
       "2                     -1.242866                      11.186186   \n",
       "3                     -1.788198                      13.375440   \n",
       "4                     -1.550068                       9.773538   \n",
       "\n",
       "   Buy_1000x_high  sell_1000x_high  total_trades_binance      Fecha  \n",
       "0      453.509003       166.304886         -51699.308594 2024-03-18  \n",
       "1      428.860809       152.266159          32977.546875 2024-03-19  \n",
       "2      430.419220       135.614014          97081.687500 2024-03-20  \n",
       "3      464.523315       131.658295          71866.585938 2024-03-21  \n",
       "4      458.367767       131.190948          48780.425781 2024-03-22  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "future_dataset = feature_dataset\n",
    "\n",
    "future_dataset, predicted_values_desnormalized = predict_next_days(ensemble, feature_dataset, scalers, n_steps, n_features, n_days_to_predict)\n",
    "\n",
    "print(\"Valores predichos para los prximos {} das:\".format(n_days_to_predict))\n",
    "display(future_dataset.tail(n_days_to_predict + 1))\n",
    "\n",
    "print(\"Valores predichos desnormalizados para los prximos {} das:\".format(n_days_to_predict))\n",
    "display(predicted_values_desnormalized.tail(n_days_to_predict))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rearmado del modelo a partir de las semillas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "14/14 [==============================] - 5s 17ms/step - loss: 8.4303 - accuracy: 0.0435\n",
      "Epoch 2/20\n",
      "14/14 [==============================] - 0s 16ms/step - loss: 6.3861 - accuracy: 0.0607\n",
      "Epoch 3/20\n",
      "14/14 [==============================] - 0s 12ms/step - loss: 4.9430 - accuracy: 0.0687\n",
      "Epoch 4/20\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 3.7974 - accuracy: 0.0745\n",
      "Epoch 5/20\n",
      "14/14 [==============================] - 0s 13ms/step - loss: 2.9156 - accuracy: 0.1042\n",
      "Epoch 6/20\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 2.2294 - accuracy: 0.1065\n",
      "Epoch 7/20\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 1.6980 - accuracy: 0.1294\n",
      "Epoch 8/20\n",
      "14/14 [==============================] - 0s 15ms/step - loss: 1.2883 - accuracy: 0.1363\n",
      "Epoch 9/20\n",
      "14/14 [==============================] - 0s 12ms/step - loss: 0.9759 - accuracy: 0.1558\n",
      "Epoch 10/20\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 0.7365 - accuracy: 0.1523\n",
      "Epoch 11/20\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 0.5620 - accuracy: 0.1684\n",
      "Epoch 12/20\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 0.4278 - accuracy: 0.2016\n",
      "Epoch 13/20\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 0.3239 - accuracy: 0.1993\n",
      "Epoch 14/20\n",
      "14/14 [==============================] - 0s 17ms/step - loss: 0.2517 - accuracy: 0.2016\n",
      "Epoch 15/20\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 0.1964 - accuracy: 0.2153\n",
      "Epoch 16/20\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 0.1594 - accuracy: 0.2199\n",
      "Epoch 17/20\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 0.1291 - accuracy: 0.2188\n",
      "Epoch 18/20\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 0.1064 - accuracy: 0.2428\n",
      "Epoch 19/20\n",
      "14/14 [==============================] - 0s 15ms/step - loss: 0.0921 - accuracy: 0.2211\n",
      "Epoch 20/20\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 0.0857 - accuracy: 0.2291\n",
      "Epoch 1/100\n",
      "7/7 [==============================] - 3s 98ms/step - loss: 8.6992 - accuracy: 0.0573\n",
      "Epoch 2/100\n",
      "7/7 [==============================] - 1s 98ms/step - loss: 5.8084 - accuracy: 0.0550\n",
      "Epoch 3/100\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 3.2598 - accuracy: 0.0699\n",
      "Epoch 4/100\n",
      "7/7 [==============================] - 1s 103ms/step - loss: 1.6121 - accuracy: 0.0790\n",
      "Epoch 5/100\n",
      "7/7 [==============================] - 1s 116ms/step - loss: 0.9703 - accuracy: 0.1019\n",
      "Epoch 6/100\n",
      "7/7 [==============================] - 1s 100ms/step - loss: 0.7718 - accuracy: 0.1191\n",
      "Epoch 7/100\n",
      "7/7 [==============================] - 1s 109ms/step - loss: 0.5894 - accuracy: 0.1168\n",
      "Epoch 8/100\n",
      "7/7 [==============================] - 1s 119ms/step - loss: 0.3969 - accuracy: 0.1432\n",
      "Epoch 9/100\n",
      "7/7 [==============================] - 1s 103ms/step - loss: 0.3092 - accuracy: 0.1993\n",
      "Epoch 10/100\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 0.2719 - accuracy: 0.1959\n",
      "Epoch 11/100\n",
      "7/7 [==============================] - 1s 98ms/step - loss: 0.2265 - accuracy: 0.1913\n",
      "Epoch 12/100\n",
      "7/7 [==============================] - 1s 114ms/step - loss: 0.1956 - accuracy: 0.2039\n",
      "Epoch 13/100\n",
      "7/7 [==============================] - 1s 167ms/step - loss: 0.1665 - accuracy: 0.2394\n",
      "Epoch 14/100\n",
      "7/7 [==============================] - 1s 164ms/step - loss: 0.1341 - accuracy: 0.2463\n",
      "Epoch 15/100\n",
      "7/7 [==============================] - 1s 184ms/step - loss: 0.1129 - accuracy: 0.2291\n",
      "Epoch 16/100\n",
      "7/7 [==============================] - 1s 164ms/step - loss: 0.1013 - accuracy: 0.2612\n",
      "Epoch 17/100\n",
      "7/7 [==============================] - 1s 162ms/step - loss: 0.0993 - accuracy: 0.2910\n",
      "Epoch 18/100\n",
      "7/7 [==============================] - 1s 184ms/step - loss: 0.0799 - accuracy: 0.3207\n",
      "Epoch 19/100\n",
      "7/7 [==============================] - 1s 165ms/step - loss: 0.0706 - accuracy: 0.3242\n",
      "Epoch 20/100\n",
      "7/7 [==============================] - 1s 166ms/step - loss: 0.0685 - accuracy: 0.3265\n",
      "Epoch 21/100\n",
      "7/7 [==============================] - 1s 192ms/step - loss: 0.0608 - accuracy: 0.3139\n",
      "Epoch 22/100\n",
      "7/7 [==============================] - 1s 158ms/step - loss: 0.0567 - accuracy: 0.3196\n",
      "Epoch 23/100\n",
      "7/7 [==============================] - 1s 155ms/step - loss: 0.0538 - accuracy: 0.3459\n",
      "Epoch 24/100\n",
      "7/7 [==============================] - 1s 182ms/step - loss: 0.0539 - accuracy: 0.3505\n",
      "Epoch 25/100\n",
      "7/7 [==============================] - 1s 164ms/step - loss: 0.0465 - accuracy: 0.3780\n",
      "Epoch 26/100\n",
      "7/7 [==============================] - 1s 177ms/step - loss: 0.0419 - accuracy: 0.3814\n",
      "Epoch 27/100\n",
      "7/7 [==============================] - 1s 175ms/step - loss: 0.0457 - accuracy: 0.3780\n",
      "Epoch 28/100\n",
      "7/7 [==============================] - 1s 158ms/step - loss: 0.0414 - accuracy: 0.3711\n",
      "Epoch 29/100\n",
      "7/7 [==============================] - 1s 188ms/step - loss: 0.0456 - accuracy: 0.3792\n",
      "Epoch 30/100\n",
      "7/7 [==============================] - 1s 193ms/step - loss: 0.0536 - accuracy: 0.3746\n",
      "Epoch 31/100\n",
      "7/7 [==============================] - 1s 163ms/step - loss: 0.0569 - accuracy: 0.3688\n",
      "Epoch 32/100\n",
      "7/7 [==============================] - 1s 168ms/step - loss: 0.0498 - accuracy: 0.3837\n",
      "Epoch 33/100\n",
      "7/7 [==============================] - 1s 181ms/step - loss: 0.0518 - accuracy: 0.4078\n",
      "Epoch 34/100\n",
      "7/7 [==============================] - 1s 182ms/step - loss: 0.0503 - accuracy: 0.4399\n",
      "Epoch 35/100\n",
      "7/7 [==============================] - 1s 177ms/step - loss: 0.0439 - accuracy: 0.4204\n",
      "Epoch 36/100\n",
      "7/7 [==============================] - 1s 215ms/step - loss: 0.0485 - accuracy: 0.4032\n",
      "Epoch 37/100\n",
      "7/7 [==============================] - 1s 171ms/step - loss: 0.0381 - accuracy: 0.4192\n",
      "Epoch 38/100\n",
      "7/7 [==============================] - 1s 146ms/step - loss: 0.0429 - accuracy: 0.4284\n",
      "Epoch 39/100\n",
      "7/7 [==============================] - 1s 178ms/step - loss: 0.0398 - accuracy: 0.4078\n",
      "Epoch 40/100\n",
      "7/7 [==============================] - 1s 153ms/step - loss: 0.0451 - accuracy: 0.4147\n",
      "Epoch 41/100\n",
      "7/7 [==============================] - 1s 147ms/step - loss: 0.0445 - accuracy: 0.4250\n",
      "Epoch 42/100\n",
      "7/7 [==============================] - 1s 179ms/step - loss: 0.0501 - accuracy: 0.3975\n",
      "Epoch 43/100\n",
      "7/7 [==============================] - 1s 177ms/step - loss: 0.0487 - accuracy: 0.3952\n",
      "Epoch 44/100\n",
      "7/7 [==============================] - 1s 183ms/step - loss: 0.0430 - accuracy: 0.4032\n",
      "Epoch 45/100\n",
      "7/7 [==============================] - 1s 169ms/step - loss: 0.0472 - accuracy: 0.4135\n",
      "Epoch 46/100\n",
      "7/7 [==============================] - 1s 177ms/step - loss: 0.0446 - accuracy: 0.4296\n",
      "Epoch 47/100\n",
      "7/7 [==============================] - 1s 162ms/step - loss: 0.0520 - accuracy: 0.4250\n",
      "Epoch 48/100\n",
      "7/7 [==============================] - 1s 160ms/step - loss: 0.0597 - accuracy: 0.4089\n",
      "Epoch 49/100\n",
      "7/7 [==============================] - 1s 183ms/step - loss: 0.0556 - accuracy: 0.3986\n",
      "Epoch 50/100\n",
      "7/7 [==============================] - 1s 185ms/step - loss: 0.0623 - accuracy: 0.3906\n",
      "Epoch 51/100\n",
      "7/7 [==============================] - 1s 168ms/step - loss: 0.0624 - accuracy: 0.4101\n",
      "Epoch 52/100\n",
      "7/7 [==============================] - 1s 149ms/step - loss: 0.0626 - accuracy: 0.4124\n",
      "Epoch 53/100\n",
      "7/7 [==============================] - 1s 161ms/step - loss: 0.0541 - accuracy: 0.4021\n",
      "Epoch 54/100\n",
      "7/7 [==============================] - 1s 175ms/step - loss: 0.0609 - accuracy: 0.4032\n",
      "Epoch 55/100\n",
      "7/7 [==============================] - 1s 176ms/step - loss: 0.0602 - accuracy: 0.4147\n",
      "Epoch 56/100\n",
      "7/7 [==============================] - 1s 186ms/step - loss: 0.0578 - accuracy: 0.3872\n",
      "Epoch 57/100\n",
      "7/7 [==============================] - 1s 168ms/step - loss: 0.0704 - accuracy: 0.3929\n",
      "Epoch 58/100\n",
      "7/7 [==============================] - 1s 181ms/step - loss: 0.0609 - accuracy: 0.4376\n",
      "Epoch 59/100\n",
      "7/7 [==============================] - 1s 214ms/step - loss: 0.0634 - accuracy: 0.3929\n",
      "Epoch 60/100\n",
      "7/7 [==============================] - 1s 172ms/step - loss: 0.0618 - accuracy: 0.4170\n",
      "Epoch 61/100\n",
      "7/7 [==============================] - 1s 178ms/step - loss: 0.0537 - accuracy: 0.4021\n",
      "Epoch 62/100\n",
      "7/7 [==============================] - 1s 204ms/step - loss: 0.0545 - accuracy: 0.4021\n",
      "Epoch 63/100\n",
      "7/7 [==============================] - 1s 172ms/step - loss: 0.0617 - accuracy: 0.4078\n",
      "Epoch 64/100\n",
      "7/7 [==============================] - 1s 181ms/step - loss: 0.0495 - accuracy: 0.4215\n",
      "Epoch 65/100\n",
      "7/7 [==============================] - 1s 201ms/step - loss: 0.0481 - accuracy: 0.4078\n",
      "Epoch 66/100\n",
      "7/7 [==============================] - 1s 177ms/step - loss: 0.0502 - accuracy: 0.4009\n",
      "Epoch 67/100\n",
      "7/7 [==============================] - 1s 191ms/step - loss: 0.0513 - accuracy: 0.4204\n",
      "Epoch 68/100\n",
      "7/7 [==============================] - 1s 192ms/step - loss: 0.0550 - accuracy: 0.4101\n",
      "Epoch 69/100\n",
      "7/7 [==============================] - 1s 179ms/step - loss: 0.0444 - accuracy: 0.4170\n",
      "Epoch 70/100\n",
      "7/7 [==============================] - 1s 170ms/step - loss: 0.0519 - accuracy: 0.4238\n",
      "Epoch 71/100\n",
      "7/7 [==============================] - 1s 194ms/step - loss: 0.0665 - accuracy: 0.4009\n",
      "Epoch 72/100\n",
      "7/7 [==============================] - 1s 179ms/step - loss: 0.0744 - accuracy: 0.3952\n",
      "Epoch 73/100\n",
      "7/7 [==============================] - 1s 171ms/step - loss: 0.0700 - accuracy: 0.3895\n",
      "Epoch 74/100\n",
      "7/7 [==============================] - 1s 175ms/step - loss: 0.0830 - accuracy: 0.4238\n",
      "Epoch 75/100\n",
      "7/7 [==============================] - 1s 185ms/step - loss: 0.0702 - accuracy: 0.4009\n",
      "Epoch 76/100\n",
      "7/7 [==============================] - 1s 204ms/step - loss: 0.0937 - accuracy: 0.4066\n",
      "Epoch 77/100\n",
      "7/7 [==============================] - 1s 188ms/step - loss: 0.0821 - accuracy: 0.4170\n",
      "Epoch 78/100\n",
      "7/7 [==============================] - 1s 187ms/step - loss: 0.0784 - accuracy: 0.3711\n",
      "Epoch 79/100\n",
      "7/7 [==============================] - 1s 184ms/step - loss: 0.0778 - accuracy: 0.4089\n",
      "Epoch 80/100\n",
      "7/7 [==============================] - 1s 181ms/step - loss: 0.0888 - accuracy: 0.3986\n",
      "Epoch 81/100\n",
      "7/7 [==============================] - 1s 169ms/step - loss: 0.0753 - accuracy: 0.4181\n",
      "Epoch 82/100\n",
      "7/7 [==============================] - 1s 209ms/step - loss: 0.1041 - accuracy: 0.3906\n",
      "Epoch 83/100\n",
      "7/7 [==============================] - 1s 165ms/step - loss: 0.1165 - accuracy: 0.3654\n",
      "Epoch 84/100\n",
      "7/7 [==============================] - 1s 165ms/step - loss: 0.0881 - accuracy: 0.3986\n",
      "Epoch 85/100\n",
      "7/7 [==============================] - 2s 186ms/step - loss: 0.0949 - accuracy: 0.3940\n",
      "Epoch 86/100\n",
      "7/7 [==============================] - 1s 164ms/step - loss: 0.1079 - accuracy: 0.4032\n",
      "Epoch 87/100\n",
      "7/7 [==============================] - 1s 204ms/step - loss: 0.0739 - accuracy: 0.4078\n",
      "Epoch 88/100\n",
      "7/7 [==============================] - 1s 194ms/step - loss: 0.0771 - accuracy: 0.3711\n",
      "Epoch 89/100\n",
      "7/7 [==============================] - 1s 179ms/step - loss: 0.0825 - accuracy: 0.4101\n",
      "Epoch 90/100\n",
      "7/7 [==============================] - 1s 186ms/step - loss: 0.0749 - accuracy: 0.3654\n",
      "Epoch 91/100\n",
      "7/7 [==============================] - 1s 164ms/step - loss: 0.0833 - accuracy: 0.4089\n",
      "Epoch 92/100\n",
      "7/7 [==============================] - 1s 199ms/step - loss: 0.0781 - accuracy: 0.4032\n",
      "Epoch 93/100\n",
      "7/7 [==============================] - 1s 198ms/step - loss: 0.0846 - accuracy: 0.4009\n",
      "Epoch 94/100\n",
      "7/7 [==============================] - 1s 183ms/step - loss: 0.0766 - accuracy: 0.4101\n",
      "Epoch 95/100\n",
      "7/7 [==============================] - 1s 185ms/step - loss: 0.0750 - accuracy: 0.3734\n",
      "Epoch 96/100\n",
      "7/7 [==============================] - 1s 212ms/step - loss: 0.0794 - accuracy: 0.4181\n",
      "Epoch 97/100\n",
      "7/7 [==============================] - 1s 192ms/step - loss: 0.0729 - accuracy: 0.4101\n",
      "Epoch 98/100\n",
      "7/7 [==============================] - 1s 192ms/step - loss: 0.0720 - accuracy: 0.4227\n",
      "Epoch 99/100\n",
      "7/7 [==============================] - 1s 222ms/step - loss: 0.0739 - accuracy: 0.3998\n",
      "Epoch 100/100\n",
      "7/7 [==============================] - 1s 184ms/step - loss: 0.0622 - accuracy: 0.4009\n",
      "Epoch 1/30\n",
      "7/7 [==============================] - 6s 46ms/step - loss: 23.3344 - accuracy: 0.0424\n",
      "Epoch 2/30\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 22.7773 - accuracy: 0.0470\n",
      "Epoch 3/30\n",
      "7/7 [==============================] - 0s 42ms/step - loss: 22.2269 - accuracy: 0.0355\n",
      "Epoch 4/30\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 21.7098 - accuracy: 0.0412\n",
      "Epoch 5/30\n",
      "7/7 [==============================] - 0s 65ms/step - loss: 21.2230 - accuracy: 0.0447\n",
      "Epoch 6/30\n",
      "7/7 [==============================] - 0s 59ms/step - loss: 20.7029 - accuracy: 0.0344\n",
      "Epoch 7/30\n",
      "7/7 [==============================] - 0s 40ms/step - loss: 20.2063 - accuracy: 0.0493\n",
      "Epoch 8/30\n",
      "7/7 [==============================] - 0s 60ms/step - loss: 19.7301 - accuracy: 0.0321\n",
      "Epoch 9/30\n",
      "7/7 [==============================] - 0s 53ms/step - loss: 19.2542 - accuracy: 0.0504\n",
      "Epoch 10/30\n",
      "7/7 [==============================] - 0s 58ms/step - loss: 18.8158 - accuracy: 0.0561\n",
      "Epoch 11/30\n",
      "7/7 [==============================] - 0s 36ms/step - loss: 18.3277 - accuracy: 0.0435\n",
      "Epoch 12/30\n",
      "7/7 [==============================] - 0s 45ms/step - loss: 17.9342 - accuracy: 0.0493\n",
      "Epoch 13/30\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 17.4783 - accuracy: 0.0401\n",
      "Epoch 14/30\n",
      "7/7 [==============================] - 0s 42ms/step - loss: 17.0483 - accuracy: 0.0378\n",
      "Epoch 15/30\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 16.6342 - accuracy: 0.0458\n",
      "Epoch 16/30\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 16.2384 - accuracy: 0.0344\n",
      "Epoch 17/30\n",
      "7/7 [==============================] - 0s 47ms/step - loss: 15.8283 - accuracy: 0.0561\n",
      "Epoch 18/30\n",
      "7/7 [==============================] - 0s 54ms/step - loss: 15.4213 - accuracy: 0.0458\n",
      "Epoch 19/30\n",
      "7/7 [==============================] - 0s 39ms/step - loss: 15.0433 - accuracy: 0.0355\n",
      "Epoch 20/30\n",
      "7/7 [==============================] - 0s 36ms/step - loss: 14.6844 - accuracy: 0.0515\n",
      "Epoch 21/30\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 14.3145 - accuracy: 0.0550\n",
      "Epoch 22/30\n",
      "7/7 [==============================] - 0s 34ms/step - loss: 13.9833 - accuracy: 0.0481\n",
      "Epoch 23/30\n",
      "7/7 [==============================] - 0s 41ms/step - loss: 13.6141 - accuracy: 0.0493\n",
      "Epoch 24/30\n",
      "7/7 [==============================] - 0s 40ms/step - loss: 13.2602 - accuracy: 0.0573\n",
      "Epoch 25/30\n",
      "7/7 [==============================] - 0s 42ms/step - loss: 12.9405 - accuracy: 0.0389\n",
      "Epoch 26/30\n",
      "7/7 [==============================] - 0s 34ms/step - loss: 12.6191 - accuracy: 0.0493\n",
      "Epoch 27/30\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 12.2944 - accuracy: 0.0389\n",
      "Epoch 28/30\n",
      "7/7 [==============================] - 0s 45ms/step - loss: 11.9830 - accuracy: 0.0538\n",
      "Epoch 29/30\n",
      "7/7 [==============================] - 0s 56ms/step - loss: 11.6696 - accuracy: 0.0596\n",
      "Epoch 30/30\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 11.3750 - accuracy: 0.0527\n",
      "Epoch 1/10\n",
      "28/28 [==============================] - 6s 23ms/step - loss: 1.4819 - accuracy: 0.0298\n",
      "Epoch 2/10\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 1.4457 - accuracy: 0.0286\n",
      "Epoch 3/10\n",
      "28/28 [==============================] - 0s 17ms/step - loss: 1.4067 - accuracy: 0.0378\n",
      "Epoch 4/10\n",
      "28/28 [==============================] - 0s 17ms/step - loss: 1.3180 - accuracy: 0.0367\n",
      "Epoch 5/10\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 1.3005 - accuracy: 0.0241\n",
      "Epoch 6/10\n",
      "28/28 [==============================] - 0s 16ms/step - loss: 1.2735 - accuracy: 0.0378\n",
      "Epoch 7/10\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 1.2239 - accuracy: 0.0389\n",
      "Epoch 8/10\n",
      "28/28 [==============================] - 0s 17ms/step - loss: 1.1830 - accuracy: 0.0447\n",
      "Epoch 9/10\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 1.1654 - accuracy: 0.0447\n",
      "Epoch 10/10\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.1360 - accuracy: 0.0458\n",
      "Epoch 1/30\n",
      "28/28 [==============================] - 6s 19ms/step - loss: 3.3456 - accuracy: 0.0401\n",
      "Epoch 2/30\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 2.5802 - accuracy: 0.0344\n",
      "Epoch 3/30\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 2.0301 - accuracy: 0.0504\n",
      "Epoch 4/30\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.6224 - accuracy: 0.0722\n",
      "Epoch 5/30\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2736 - accuracy: 0.0951\n",
      "Epoch 6/30\n",
      "28/28 [==============================] - 1s 30ms/step - loss: 0.9940 - accuracy: 0.0928\n",
      "Epoch 7/30\n",
      "28/28 [==============================] - 1s 29ms/step - loss: 0.7791 - accuracy: 0.0997\n",
      "Epoch 8/30\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.6059 - accuracy: 0.1203\n",
      "Epoch 9/30\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.4714 - accuracy: 0.1420\n",
      "Epoch 10/30\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.3687 - accuracy: 0.1455\n",
      "Epoch 11/30\n",
      "28/28 [==============================] - 0s 18ms/step - loss: 0.2915 - accuracy: 0.1661\n",
      "Epoch 12/30\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 0.2241 - accuracy: 0.1810\n",
      "Epoch 13/30\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.1815 - accuracy: 0.1856\n",
      "Epoch 14/30\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 0.1483 - accuracy: 0.2016\n",
      "Epoch 15/30\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.1200 - accuracy: 0.2325\n",
      "Epoch 16/30\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 0.1031 - accuracy: 0.2497\n",
      "Epoch 17/30\n",
      "28/28 [==============================] - 0s 16ms/step - loss: 0.0853 - accuracy: 0.2279\n",
      "Epoch 18/30\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.0762 - accuracy: 0.2772\n",
      "Epoch 19/30\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 0.0669 - accuracy: 0.2692\n",
      "Epoch 20/30\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.0588 - accuracy: 0.2772\n",
      "Epoch 21/30\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 0.0517 - accuracy: 0.3093\n",
      "Epoch 22/30\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0448 - accuracy: 0.3036\n",
      "Epoch 23/30\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.0408 - accuracy: 0.3425\n",
      "Epoch 24/30\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 0.0349 - accuracy: 0.3585\n",
      "Epoch 25/30\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 0.0310 - accuracy: 0.3517\n",
      "Epoch 26/30\n",
      "28/28 [==============================] - 0s 16ms/step - loss: 0.0291 - accuracy: 0.3643\n",
      "Epoch 27/30\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 0.0264 - accuracy: 0.3780\n",
      "Epoch 28/30\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 0.0244 - accuracy: 0.3975\n",
      "Epoch 29/30\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.0221 - accuracy: 0.3940\n",
      "Epoch 30/30\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 0.0203 - accuracy: 0.3940\n"
     ]
    }
   ],
   "source": [
    "with open('best_seeds_conv.json', 'r') as f:\n",
    "    best_seeds = json.load(f)\n",
    "\n",
    "# 21 Crear y entrenar los modelos con los hiperparmetros y semillas guardados\n",
    "models = []\n",
    "for params_str, seed in best_seeds.items():\n",
    "    params = json.loads(params_str.replace(\"'\", \"\\\"\"))\n",
    "    model = KerasRegressor(build_fn=create_model, random_state=seed, **params)\n",
    "    model.fit(X, y)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "14/14 [==============================] - 3s 12ms/step - loss: 8.4303 - accuracy: 0.0435\n",
      "Epoch 2/20\n",
      "14/14 [==============================] - 0s 16ms/step - loss: 6.3861 - accuracy: 0.0607\n",
      "Epoch 3/20\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 4.9430 - accuracy: 0.0687\n",
      "Epoch 4/20\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 3.7974 - accuracy: 0.0745\n",
      "Epoch 5/20\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 2.9156 - accuracy: 0.1042\n",
      "Epoch 6/20\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 2.2294 - accuracy: 0.1065\n",
      "Epoch 7/20\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 1.6980 - accuracy: 0.1294\n",
      "Epoch 8/20\n",
      "14/14 [==============================] - 0s 13ms/step - loss: 1.2883 - accuracy: 0.1363\n",
      "Epoch 9/20\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 0.9759 - accuracy: 0.1558\n",
      "Epoch 10/20\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 0.7365 - accuracy: 0.1523\n",
      "Epoch 11/20\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 0.5620 - accuracy: 0.1684\n",
      "Epoch 12/20\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 0.4278 - accuracy: 0.2016\n",
      "Epoch 13/20\n",
      "14/14 [==============================] - 0s 13ms/step - loss: 0.3239 - accuracy: 0.1993\n",
      "Epoch 14/20\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 0.2517 - accuracy: 0.2016\n",
      "Epoch 15/20\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 0.1964 - accuracy: 0.2153\n",
      "Epoch 16/20\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 0.1594 - accuracy: 0.2199\n",
      "Epoch 17/20\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 0.1291 - accuracy: 0.2188\n",
      "Epoch 18/20\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 0.1064 - accuracy: 0.2428\n",
      "Epoch 19/20\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 0.0921 - accuracy: 0.2211\n",
      "Epoch 20/20\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 0.0857 - accuracy: 0.2291\n",
      "Epoch 1/100\n",
      "7/7 [==============================] - 3s 118ms/step - loss: 8.6992 - accuracy: 0.0573\n",
      "Epoch 2/100\n",
      "7/7 [==============================] - 1s 100ms/step - loss: 5.8084 - accuracy: 0.0550\n",
      "Epoch 3/100\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 3.2598 - accuracy: 0.0699\n",
      "Epoch 4/100\n",
      "7/7 [==============================] - 1s 112ms/step - loss: 1.6121 - accuracy: 0.0790\n",
      "Epoch 5/100\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.9703 - accuracy: 0.1019\n",
      "Epoch 6/100\n",
      "7/7 [==============================] - 1s 156ms/step - loss: 0.7718 - accuracy: 0.1191\n",
      "Epoch 7/100\n",
      "7/7 [==============================] - 1s 177ms/step - loss: 0.5894 - accuracy: 0.1168\n",
      "Epoch 8/100\n",
      "7/7 [==============================] - 1s 163ms/step - loss: 0.3969 - accuracy: 0.1432\n",
      "Epoch 9/100\n",
      "7/7 [==============================] - 1s 183ms/step - loss: 0.3092 - accuracy: 0.1993\n",
      "Epoch 10/100\n",
      "7/7 [==============================] - 1s 185ms/step - loss: 0.2719 - accuracy: 0.1959\n",
      "Epoch 11/100\n",
      "7/7 [==============================] - 1s 183ms/step - loss: 0.2265 - accuracy: 0.1913\n",
      "Epoch 12/100\n",
      "7/7 [==============================] - 1s 180ms/step - loss: 0.1956 - accuracy: 0.2039\n",
      "Epoch 13/100\n",
      "7/7 [==============================] - 1s 183ms/step - loss: 0.1665 - accuracy: 0.2394\n",
      "Epoch 14/100\n",
      "7/7 [==============================] - 1s 182ms/step - loss: 0.1341 - accuracy: 0.2463\n",
      "Epoch 15/100\n",
      "7/7 [==============================] - 1s 178ms/step - loss: 0.1129 - accuracy: 0.2291\n",
      "Epoch 16/100\n",
      "7/7 [==============================] - 1s 187ms/step - loss: 0.1013 - accuracy: 0.2612\n",
      "Epoch 17/100\n",
      "7/7 [==============================] - 1s 171ms/step - loss: 0.0993 - accuracy: 0.2910\n",
      "Epoch 18/100\n",
      "7/7 [==============================] - 1s 188ms/step - loss: 0.0799 - accuracy: 0.3207\n",
      "Epoch 19/100\n",
      "7/7 [==============================] - 1s 196ms/step - loss: 0.0706 - accuracy: 0.3242\n",
      "Epoch 20/100\n",
      "7/7 [==============================] - 1s 181ms/step - loss: 0.0685 - accuracy: 0.3265\n",
      "Epoch 21/100\n",
      "7/7 [==============================] - 1s 188ms/step - loss: 0.0608 - accuracy: 0.3139\n",
      "Epoch 22/100\n",
      "7/7 [==============================] - 1s 175ms/step - loss: 0.0567 - accuracy: 0.3196\n",
      "Epoch 23/100\n",
      "7/7 [==============================] - 1s 192ms/step - loss: 0.0538 - accuracy: 0.3459\n",
      "Epoch 24/100\n",
      "7/7 [==============================] - 1s 184ms/step - loss: 0.0539 - accuracy: 0.3505\n",
      "Epoch 25/100\n",
      "7/7 [==============================] - 1s 183ms/step - loss: 0.0465 - accuracy: 0.3780\n",
      "Epoch 26/100\n",
      "7/7 [==============================] - 1s 194ms/step - loss: 0.0419 - accuracy: 0.3814\n",
      "Epoch 27/100\n",
      "7/7 [==============================] - 1s 193ms/step - loss: 0.0457 - accuracy: 0.3780\n",
      "Epoch 28/100\n",
      "7/7 [==============================] - 1s 173ms/step - loss: 0.0414 - accuracy: 0.3711\n",
      "Epoch 29/100\n",
      "7/7 [==============================] - 1s 173ms/step - loss: 0.0456 - accuracy: 0.3792\n",
      "Epoch 30/100\n",
      "7/7 [==============================] - 1s 157ms/step - loss: 0.0536 - accuracy: 0.3746\n",
      "Epoch 31/100\n",
      "7/7 [==============================] - 1s 193ms/step - loss: 0.0569 - accuracy: 0.3688\n",
      "Epoch 32/100\n",
      "7/7 [==============================] - 1s 184ms/step - loss: 0.0498 - accuracy: 0.3837\n",
      "Epoch 33/100\n",
      "7/7 [==============================] - 1s 204ms/step - loss: 0.0518 - accuracy: 0.4078\n",
      "Epoch 34/100\n",
      "7/7 [==============================] - 2s 220ms/step - loss: 0.0503 - accuracy: 0.4399\n",
      "Epoch 35/100\n",
      "7/7 [==============================] - 1s 200ms/step - loss: 0.0439 - accuracy: 0.4204\n",
      "Epoch 36/100\n",
      "7/7 [==============================] - 1s 189ms/step - loss: 0.0485 - accuracy: 0.4032\n",
      "Epoch 37/100\n",
      "7/7 [==============================] - 1s 193ms/step - loss: 0.0381 - accuracy: 0.4192\n",
      "Epoch 38/100\n",
      "7/7 [==============================] - 1s 192ms/step - loss: 0.0429 - accuracy: 0.4284\n",
      "Epoch 39/100\n",
      "7/7 [==============================] - 1s 181ms/step - loss: 0.0398 - accuracy: 0.4078\n",
      "Epoch 40/100\n",
      "7/7 [==============================] - 1s 185ms/step - loss: 0.0451 - accuracy: 0.4147\n",
      "Epoch 41/100\n",
      "7/7 [==============================] - 1s 173ms/step - loss: 0.0445 - accuracy: 0.4250\n",
      "Epoch 42/100\n",
      "7/7 [==============================] - 1s 188ms/step - loss: 0.0501 - accuracy: 0.3975\n",
      "Epoch 43/100\n",
      "7/7 [==============================] - 1s 200ms/step - loss: 0.0487 - accuracy: 0.3952\n",
      "Epoch 44/100\n",
      "7/7 [==============================] - 1s 173ms/step - loss: 0.0430 - accuracy: 0.4032\n",
      "Epoch 45/100\n",
      "7/7 [==============================] - 1s 179ms/step - loss: 0.0472 - accuracy: 0.4135\n",
      "Epoch 46/100\n",
      "7/7 [==============================] - 1s 171ms/step - loss: 0.0446 - accuracy: 0.4296\n",
      "Epoch 47/100\n",
      "7/7 [==============================] - 1s 179ms/step - loss: 0.0520 - accuracy: 0.4250\n",
      "Epoch 48/100\n",
      "7/7 [==============================] - 1s 189ms/step - loss: 0.0597 - accuracy: 0.4089\n",
      "Epoch 49/100\n",
      "7/7 [==============================] - 1s 180ms/step - loss: 0.0556 - accuracy: 0.3986\n",
      "Epoch 50/100\n",
      "7/7 [==============================] - 1s 187ms/step - loss: 0.0623 - accuracy: 0.3906\n",
      "Epoch 51/100\n",
      "7/7 [==============================] - 1s 184ms/step - loss: 0.0624 - accuracy: 0.4101\n",
      "Epoch 52/100\n",
      "7/7 [==============================] - 1s 159ms/step - loss: 0.0626 - accuracy: 0.4124\n",
      "Epoch 53/100\n",
      "7/7 [==============================] - 1s 206ms/step - loss: 0.0541 - accuracy: 0.4021\n",
      "Epoch 54/100\n",
      "7/7 [==============================] - 1s 190ms/step - loss: 0.0609 - accuracy: 0.4032\n",
      "Epoch 55/100\n",
      "7/7 [==============================] - 1s 161ms/step - loss: 0.0602 - accuracy: 0.4147\n",
      "Epoch 56/100\n",
      "7/7 [==============================] - 1s 187ms/step - loss: 0.0578 - accuracy: 0.3872\n",
      "Epoch 57/100\n",
      "7/7 [==============================] - 1s 190ms/step - loss: 0.0704 - accuracy: 0.3929\n",
      "Epoch 58/100\n",
      "7/7 [==============================] - 1s 193ms/step - loss: 0.0609 - accuracy: 0.4376\n",
      "Epoch 59/100\n",
      "7/7 [==============================] - 1s 198ms/step - loss: 0.0634 - accuracy: 0.3929\n",
      "Epoch 60/100\n",
      "7/7 [==============================] - 1s 175ms/step - loss: 0.0618 - accuracy: 0.4170\n",
      "Epoch 61/100\n",
      "7/7 [==============================] - 1s 189ms/step - loss: 0.0537 - accuracy: 0.4021\n",
      "Epoch 62/100\n",
      "7/7 [==============================] - 1s 190ms/step - loss: 0.0545 - accuracy: 0.4021\n",
      "Epoch 63/100\n",
      "7/7 [==============================] - 1s 218ms/step - loss: 0.0617 - accuracy: 0.4078\n",
      "Epoch 64/100\n",
      "7/7 [==============================] - 1s 194ms/step - loss: 0.0495 - accuracy: 0.4215\n",
      "Epoch 65/100\n",
      "7/7 [==============================] - 1s 180ms/step - loss: 0.0481 - accuracy: 0.4078\n",
      "Epoch 66/100\n",
      "7/7 [==============================] - 2s 233ms/step - loss: 0.0502 - accuracy: 0.4009\n",
      "Epoch 67/100\n",
      "7/7 [==============================] - 1s 187ms/step - loss: 0.0513 - accuracy: 0.4204\n",
      "Epoch 68/100\n",
      "7/7 [==============================] - 1s 199ms/step - loss: 0.0550 - accuracy: 0.4101\n",
      "Epoch 69/100\n",
      "7/7 [==============================] - 1s 177ms/step - loss: 0.0444 - accuracy: 0.4170\n",
      "Epoch 70/100\n",
      "7/7 [==============================] - 1s 175ms/step - loss: 0.0519 - accuracy: 0.4238\n",
      "Epoch 71/100\n",
      "7/7 [==============================] - 1s 176ms/step - loss: 0.0665 - accuracy: 0.4009\n",
      "Epoch 72/100\n",
      "7/7 [==============================] - 1s 166ms/step - loss: 0.0744 - accuracy: 0.3952\n",
      "Epoch 73/100\n",
      "7/7 [==============================] - 1s 211ms/step - loss: 0.0700 - accuracy: 0.3895\n",
      "Epoch 74/100\n",
      "7/7 [==============================] - 1s 174ms/step - loss: 0.0830 - accuracy: 0.4238\n",
      "Epoch 75/100\n",
      "7/7 [==============================] - 1s 185ms/step - loss: 0.0702 - accuracy: 0.4009\n",
      "Epoch 76/100\n",
      "7/7 [==============================] - 1s 180ms/step - loss: 0.0937 - accuracy: 0.4066\n",
      "Epoch 77/100\n",
      "7/7 [==============================] - 1s 178ms/step - loss: 0.0821 - accuracy: 0.4170\n",
      "Epoch 78/100\n",
      "7/7 [==============================] - 1s 195ms/step - loss: 0.0784 - accuracy: 0.3711\n",
      "Epoch 79/100\n",
      "7/7 [==============================] - 1s 171ms/step - loss: 0.0778 - accuracy: 0.4089\n",
      "Epoch 80/100\n",
      "7/7 [==============================] - 1s 173ms/step - loss: 0.0888 - accuracy: 0.3986\n",
      "Epoch 81/100\n",
      "7/7 [==============================] - 1s 200ms/step - loss: 0.0753 - accuracy: 0.4181\n",
      "Epoch 82/100\n",
      "7/7 [==============================] - 1s 167ms/step - loss: 0.1041 - accuracy: 0.3906\n",
      "Epoch 83/100\n",
      "7/7 [==============================] - 1s 189ms/step - loss: 0.1165 - accuracy: 0.3654\n",
      "Epoch 84/100\n",
      "7/7 [==============================] - 1s 179ms/step - loss: 0.0881 - accuracy: 0.3986\n",
      "Epoch 85/100\n",
      "7/7 [==============================] - 1s 174ms/step - loss: 0.0949 - accuracy: 0.3940\n",
      "Epoch 86/100\n",
      "7/7 [==============================] - 1s 176ms/step - loss: 0.1079 - accuracy: 0.4032\n",
      "Epoch 87/100\n",
      "7/7 [==============================] - 1s 186ms/step - loss: 0.0739 - accuracy: 0.4078\n",
      "Epoch 88/100\n",
      "7/7 [==============================] - 1s 190ms/step - loss: 0.0771 - accuracy: 0.3711\n",
      "Epoch 89/100\n",
      "7/7 [==============================] - 1s 193ms/step - loss: 0.0825 - accuracy: 0.4101\n",
      "Epoch 90/100\n",
      "7/7 [==============================] - 1s 183ms/step - loss: 0.0749 - accuracy: 0.3654\n",
      "Epoch 91/100\n",
      "7/7 [==============================] - 1s 185ms/step - loss: 0.0833 - accuracy: 0.4089\n",
      "Epoch 92/100\n",
      "7/7 [==============================] - 1s 177ms/step - loss: 0.0781 - accuracy: 0.4032\n",
      "Epoch 93/100\n",
      "7/7 [==============================] - 1s 152ms/step - loss: 0.0846 - accuracy: 0.4009\n",
      "Epoch 94/100\n",
      "7/7 [==============================] - 1s 202ms/step - loss: 0.0766 - accuracy: 0.4101\n",
      "Epoch 95/100\n",
      "7/7 [==============================] - 1s 189ms/step - loss: 0.0750 - accuracy: 0.3734\n",
      "Epoch 96/100\n",
      "7/7 [==============================] - 1s 199ms/step - loss: 0.0794 - accuracy: 0.4181\n",
      "Epoch 97/100\n",
      "7/7 [==============================] - 1s 182ms/step - loss: 0.0729 - accuracy: 0.4101\n",
      "Epoch 98/100\n",
      "7/7 [==============================] - 1s 166ms/step - loss: 0.0720 - accuracy: 0.4227\n",
      "Epoch 99/100\n",
      "7/7 [==============================] - 1s 196ms/step - loss: 0.0739 - accuracy: 0.3998\n",
      "Epoch 100/100\n",
      "7/7 [==============================] - 1s 160ms/step - loss: 0.0622 - accuracy: 0.4009\n",
      "Epoch 1/30\n",
      "7/7 [==============================] - 6s 44ms/step - loss: 23.3344 - accuracy: 0.0424\n",
      "Epoch 2/30\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 22.7773 - accuracy: 0.0470\n",
      "Epoch 3/30\n",
      "7/7 [==============================] - 0s 50ms/step - loss: 22.2269 - accuracy: 0.0355\n",
      "Epoch 4/30\n",
      "7/7 [==============================] - 0s 35ms/step - loss: 21.7098 - accuracy: 0.0412\n",
      "Epoch 5/30\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 21.2230 - accuracy: 0.0447\n",
      "Epoch 6/30\n",
      "7/7 [==============================] - 0s 54ms/step - loss: 20.7029 - accuracy: 0.0344\n",
      "Epoch 7/30\n",
      "7/7 [==============================] - 0s 38ms/step - loss: 20.2063 - accuracy: 0.0493\n",
      "Epoch 8/30\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 19.7301 - accuracy: 0.0321\n",
      "Epoch 9/30\n",
      "7/7 [==============================] - 0s 41ms/step - loss: 19.2542 - accuracy: 0.0504\n",
      "Epoch 10/30\n",
      "7/7 [==============================] - 0s 34ms/step - loss: 18.8158 - accuracy: 0.0561\n",
      "Epoch 11/30\n",
      "7/7 [==============================] - 0s 35ms/step - loss: 18.3277 - accuracy: 0.0435\n",
      "Epoch 12/30\n",
      "7/7 [==============================] - 0s 50ms/step - loss: 17.9342 - accuracy: 0.0493\n",
      "Epoch 13/30\n",
      "7/7 [==============================] - 0s 35ms/step - loss: 17.4783 - accuracy: 0.0401\n",
      "Epoch 14/30\n",
      "7/7 [==============================] - 0s 41ms/step - loss: 17.0483 - accuracy: 0.0378\n",
      "Epoch 15/30\n",
      "7/7 [==============================] - 0s 35ms/step - loss: 16.6342 - accuracy: 0.0458\n",
      "Epoch 16/30\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 16.2384 - accuracy: 0.0344\n",
      "Epoch 17/30\n",
      "7/7 [==============================] - 0s 39ms/step - loss: 15.8283 - accuracy: 0.0561\n",
      "Epoch 18/30\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 15.4213 - accuracy: 0.0458\n",
      "Epoch 19/30\n",
      "7/7 [==============================] - 0s 33ms/step - loss: 15.0433 - accuracy: 0.0355\n",
      "Epoch 20/30\n",
      "7/7 [==============================] - 0s 47ms/step - loss: 14.6844 - accuracy: 0.0515\n",
      "Epoch 21/30\n",
      "7/7 [==============================] - 0s 50ms/step - loss: 14.3145 - accuracy: 0.0550\n",
      "Epoch 22/30\n",
      "7/7 [==============================] - 0s 51ms/step - loss: 13.9833 - accuracy: 0.0481\n",
      "Epoch 23/30\n",
      "7/7 [==============================] - 0s 37ms/step - loss: 13.6141 - accuracy: 0.0493\n",
      "Epoch 24/30\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 13.2602 - accuracy: 0.0573\n",
      "Epoch 25/30\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 12.9405 - accuracy: 0.0389\n",
      "Epoch 26/30\n",
      "7/7 [==============================] - 0s 39ms/step - loss: 12.6191 - accuracy: 0.0493\n",
      "Epoch 27/30\n",
      "7/7 [==============================] - 0s 40ms/step - loss: 12.2944 - accuracy: 0.0389\n",
      "Epoch 28/30\n",
      "7/7 [==============================] - 0s 56ms/step - loss: 11.9830 - accuracy: 0.0538\n",
      "Epoch 29/30\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 11.6696 - accuracy: 0.0596\n",
      "Epoch 30/30\n",
      "7/7 [==============================] - 0s 34ms/step - loss: 11.3750 - accuracy: 0.0527\n",
      "Epoch 1/10\n",
      "28/28 [==============================] - 5s 18ms/step - loss: 1.4819 - accuracy: 0.0298\n",
      "Epoch 2/10\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 1.4457 - accuracy: 0.0286\n",
      "Epoch 3/10\n",
      "28/28 [==============================] - 0s 18ms/step - loss: 1.4067 - accuracy: 0.0378\n",
      "Epoch 4/10\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 1.3180 - accuracy: 0.0367\n",
      "Epoch 5/10\n",
      "28/28 [==============================] - 0s 18ms/step - loss: 1.3005 - accuracy: 0.0241\n",
      "Epoch 6/10\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2735 - accuracy: 0.0378\n",
      "Epoch 7/10\n",
      "28/28 [==============================] - 0s 17ms/step - loss: 1.2239 - accuracy: 0.0389\n",
      "Epoch 8/10\n",
      "28/28 [==============================] - 0s 17ms/step - loss: 1.1830 - accuracy: 0.0447\n",
      "Epoch 9/10\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 1.1654 - accuracy: 0.0447\n",
      "Epoch 10/10\n",
      "28/28 [==============================] - 0s 17ms/step - loss: 1.1360 - accuracy: 0.0458\n",
      "Epoch 1/30\n",
      "28/28 [==============================] - 6s 24ms/step - loss: 3.3456 - accuracy: 0.0401\n",
      "Epoch 2/30\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 2.5802 - accuracy: 0.0344\n",
      "Epoch 3/30\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 2.0301 - accuracy: 0.0504\n",
      "Epoch 4/30\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.6224 - accuracy: 0.0722\n",
      "Epoch 5/30\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2736 - accuracy: 0.0951\n",
      "Epoch 6/30\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.9940 - accuracy: 0.0928\n",
      "Epoch 7/30\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 0.7791 - accuracy: 0.0997\n",
      "Epoch 8/30\n",
      "28/28 [==============================] - 1s 18ms/step - loss: 0.6059 - accuracy: 0.1203\n",
      "Epoch 9/30\n",
      "28/28 [==============================] - 1s 18ms/step - loss: 0.4714 - accuracy: 0.1420\n",
      "Epoch 10/30\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 0.3687 - accuracy: 0.1455\n",
      "Epoch 11/30\n",
      "28/28 [==============================] - 1s 18ms/step - loss: 0.2915 - accuracy: 0.1661\n",
      "Epoch 12/30\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.2241 - accuracy: 0.1810\n",
      "Epoch 13/30\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 0.1815 - accuracy: 0.1856\n",
      "Epoch 14/30\n",
      "28/28 [==============================] - 1s 18ms/step - loss: 0.1483 - accuracy: 0.2016\n",
      "Epoch 15/30\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.1200 - accuracy: 0.2325\n",
      "Epoch 16/30\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.1031 - accuracy: 0.2497\n",
      "Epoch 17/30\n",
      "28/28 [==============================] - 1s 18ms/step - loss: 0.0853 - accuracy: 0.2279\n",
      "Epoch 18/30\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 0.0762 - accuracy: 0.2772\n",
      "Epoch 19/30\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 0.0669 - accuracy: 0.2692\n",
      "Epoch 20/30\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 0.0588 - accuracy: 0.2772\n",
      "Epoch 21/30\n",
      "28/28 [==============================] - 1s 18ms/step - loss: 0.0517 - accuracy: 0.3093\n",
      "Epoch 22/30\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 0.0448 - accuracy: 0.3036\n",
      "Epoch 23/30\n",
      "28/28 [==============================] - 0s 17ms/step - loss: 0.0408 - accuracy: 0.3425\n",
      "Epoch 24/30\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0349 - accuracy: 0.3585\n",
      "Epoch 25/30\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.0310 - accuracy: 0.3517\n",
      "Epoch 26/30\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.0291 - accuracy: 0.3643\n",
      "Epoch 27/30\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.0264 - accuracy: 0.3780\n",
      "Epoch 28/30\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.0244 - accuracy: 0.3975\n",
      "Epoch 29/30\n",
      "28/28 [==============================] - 0s 16ms/step - loss: 0.0221 - accuracy: 0.3940\n",
      "Epoch 30/30\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.0203 - accuracy: 0.3940\n",
      "1/1 [==============================] - 0s 482ms/step\n",
      "1/1 [==============================] - 0s 480ms/step\n",
      "1/1 [==============================] - 0s 426ms/step\n",
      "1/1 [==============================] - 0s 303ms/step\n",
      "1/1 [==============================] - 0s 488ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Number of trades</th>\n",
       "      <th>Close_BTCUSDT</th>\n",
       "      <th>Volume_BTCUSDT</th>\n",
       "      <th>Number_of_trades_BTCUSDT</th>\n",
       "      <th>Close_ETHUSDT</th>\n",
       "      <th>Volume_ETHUSDT</th>\n",
       "      <th>Number_of_trades_ETHUSDT</th>\n",
       "      <th>Close_BNBUSDT</th>\n",
       "      <th>Volume_BNBUSDT</th>\n",
       "      <th>Number_of_trades_BNBUSDT</th>\n",
       "      <th>SMA_20</th>\n",
       "      <th>EMA_20</th>\n",
       "      <th>Upper_Band</th>\n",
       "      <th>Middle_Band</th>\n",
       "      <th>Lower_Band</th>\n",
       "      <th>RSI</th>\n",
       "      <th>buy_1000x_high_coinbase</th>\n",
       "      <th>sell_1000x_high_coinbase</th>\n",
       "      <th>total_trades_coinbase</th>\n",
       "      <th>Tweets_Utilizados</th>\n",
       "      <th>Tweets_Utilizados_coin</th>\n",
       "      <th>Tweets_Utilizados_referentes</th>\n",
       "      <th>Tweets_Utilizados_whale_alert</th>\n",
       "      <th>Buy_1000x_high</th>\n",
       "      <th>sell_1000x_high</th>\n",
       "      <th>total_trades_binance</th>\n",
       "      <th>Fecha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.568762</td>\n",
       "      <td>7.314273</td>\n",
       "      <td>8.084068</td>\n",
       "      <td>14.140716</td>\n",
       "      <td>495344.40625</td>\n",
       "      <td>42416.695312</td>\n",
       "      <td>142456.609375</td>\n",
       "      <td>-3652509.00</td>\n",
       "      <td>2722.593018</td>\n",
       "      <td>1.032206e+06</td>\n",
       "      <td>9588.093750</td>\n",
       "      <td>255.560089</td>\n",
       "      <td>282103.78125</td>\n",
       "      <td>123575.445312</td>\n",
       "      <td>13.337129</td>\n",
       "      <td>6.203595</td>\n",
       "      <td>5.185119</td>\n",
       "      <td>14.871467</td>\n",
       "      <td>12.636547</td>\n",
       "      <td>46.455318</td>\n",
       "      <td>27.768692</td>\n",
       "      <td>37.203564</td>\n",
       "      <td>20085.880859</td>\n",
       "      <td>165.505447</td>\n",
       "      <td>181.841339</td>\n",
       "      <td>-1.374227</td>\n",
       "      <td>8.755414</td>\n",
       "      <td>453.509003</td>\n",
       "      <td>166.304886</td>\n",
       "      <td>-51699.308594</td>\n",
       "      <td>2024-03-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.352944</td>\n",
       "      <td>6.729131</td>\n",
       "      <td>8.805325</td>\n",
       "      <td>13.651693</td>\n",
       "      <td>491231.65625</td>\n",
       "      <td>42662.476562</td>\n",
       "      <td>166972.312500</td>\n",
       "      <td>-3345285.25</td>\n",
       "      <td>2708.688477</td>\n",
       "      <td>1.016160e+06</td>\n",
       "      <td>-13281.995117</td>\n",
       "      <td>249.865051</td>\n",
       "      <td>378897.12500</td>\n",
       "      <td>103865.914062</td>\n",
       "      <td>13.502258</td>\n",
       "      <td>5.276132</td>\n",
       "      <td>4.634593</td>\n",
       "      <td>13.678970</td>\n",
       "      <td>13.116061</td>\n",
       "      <td>47.258823</td>\n",
       "      <td>27.858992</td>\n",
       "      <td>34.138977</td>\n",
       "      <td>24282.658203</td>\n",
       "      <td>135.206604</td>\n",
       "      <td>175.209488</td>\n",
       "      <td>-1.266967</td>\n",
       "      <td>9.644383</td>\n",
       "      <td>428.860809</td>\n",
       "      <td>152.266159</td>\n",
       "      <td>32977.546875</td>\n",
       "      <td>2024-03-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.577085</td>\n",
       "      <td>8.603601</td>\n",
       "      <td>9.478132</td>\n",
       "      <td>12.591394</td>\n",
       "      <td>424287.28125</td>\n",
       "      <td>42998.308594</td>\n",
       "      <td>183414.578125</td>\n",
       "      <td>-3219832.75</td>\n",
       "      <td>2634.895752</td>\n",
       "      <td>1.093318e+06</td>\n",
       "      <td>-40826.648438</td>\n",
       "      <td>256.898010</td>\n",
       "      <td>309525.28125</td>\n",
       "      <td>61923.730469</td>\n",
       "      <td>13.906770</td>\n",
       "      <td>4.340058</td>\n",
       "      <td>3.639909</td>\n",
       "      <td>13.985720</td>\n",
       "      <td>13.464526</td>\n",
       "      <td>46.247765</td>\n",
       "      <td>33.830688</td>\n",
       "      <td>34.866688</td>\n",
       "      <td>24842.693359</td>\n",
       "      <td>148.241348</td>\n",
       "      <td>184.944519</td>\n",
       "      <td>-1.242866</td>\n",
       "      <td>11.186186</td>\n",
       "      <td>430.419220</td>\n",
       "      <td>135.614014</td>\n",
       "      <td>97081.687500</td>\n",
       "      <td>2024-03-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.634637</td>\n",
       "      <td>9.808044</td>\n",
       "      <td>10.474800</td>\n",
       "      <td>12.578601</td>\n",
       "      <td>434249.28125</td>\n",
       "      <td>42183.648438</td>\n",
       "      <td>187439.062500</td>\n",
       "      <td>-2924145.50</td>\n",
       "      <td>2679.139404</td>\n",
       "      <td>1.048083e+06</td>\n",
       "      <td>-47120.707031</td>\n",
       "      <td>251.633316</td>\n",
       "      <td>183667.21875</td>\n",
       "      <td>44615.621094</td>\n",
       "      <td>12.739328</td>\n",
       "      <td>2.655619</td>\n",
       "      <td>2.737377</td>\n",
       "      <td>12.665237</td>\n",
       "      <td>14.134830</td>\n",
       "      <td>46.308434</td>\n",
       "      <td>33.212223</td>\n",
       "      <td>29.476658</td>\n",
       "      <td>31090.611328</td>\n",
       "      <td>118.835098</td>\n",
       "      <td>175.807266</td>\n",
       "      <td>-1.788198</td>\n",
       "      <td>13.375440</td>\n",
       "      <td>464.523315</td>\n",
       "      <td>131.658295</td>\n",
       "      <td>71866.585938</td>\n",
       "      <td>2024-03-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.267235</td>\n",
       "      <td>10.535395</td>\n",
       "      <td>11.786478</td>\n",
       "      <td>12.342828</td>\n",
       "      <td>455209.37500</td>\n",
       "      <td>42875.769531</td>\n",
       "      <td>174081.140625</td>\n",
       "      <td>-3333910.75</td>\n",
       "      <td>2614.000977</td>\n",
       "      <td>1.046208e+06</td>\n",
       "      <td>-67547.328125</td>\n",
       "      <td>247.056595</td>\n",
       "      <td>183121.46875</td>\n",
       "      <td>24934.687500</td>\n",
       "      <td>14.202191</td>\n",
       "      <td>3.334253</td>\n",
       "      <td>2.461271</td>\n",
       "      <td>11.822153</td>\n",
       "      <td>14.629593</td>\n",
       "      <td>46.222549</td>\n",
       "      <td>31.985155</td>\n",
       "      <td>40.690067</td>\n",
       "      <td>25537.763672</td>\n",
       "      <td>109.984833</td>\n",
       "      <td>181.385696</td>\n",
       "      <td>-1.550068</td>\n",
       "      <td>9.773538</td>\n",
       "      <td>458.367767</td>\n",
       "      <td>131.190948</td>\n",
       "      <td>48780.425781</td>\n",
       "      <td>2024-03-22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Open       High        Low      Close  Number of trades  Close_BTCUSDT  \\\n",
       "0  6.568762   7.314273   8.084068  14.140716      495344.40625   42416.695312   \n",
       "1  6.352944   6.729131   8.805325  13.651693      491231.65625   42662.476562   \n",
       "2  6.577085   8.603601   9.478132  12.591394      424287.28125   42998.308594   \n",
       "3  5.634637   9.808044  10.474800  12.578601      434249.28125   42183.648438   \n",
       "4  5.267235  10.535395  11.786478  12.342828      455209.37500   42875.769531   \n",
       "\n",
       "   Volume_BTCUSDT  Number_of_trades_BTCUSDT  Close_ETHUSDT  Volume_ETHUSDT  \\\n",
       "0   142456.609375               -3652509.00    2722.593018    1.032206e+06   \n",
       "1   166972.312500               -3345285.25    2708.688477    1.016160e+06   \n",
       "2   183414.578125               -3219832.75    2634.895752    1.093318e+06   \n",
       "3   187439.062500               -2924145.50    2679.139404    1.048083e+06   \n",
       "4   174081.140625               -3333910.75    2614.000977    1.046208e+06   \n",
       "\n",
       "   Number_of_trades_ETHUSDT  Close_BNBUSDT  Volume_BNBUSDT  \\\n",
       "0               9588.093750     255.560089    282103.78125   \n",
       "1             -13281.995117     249.865051    378897.12500   \n",
       "2             -40826.648438     256.898010    309525.28125   \n",
       "3             -47120.707031     251.633316    183667.21875   \n",
       "4             -67547.328125     247.056595    183121.46875   \n",
       "\n",
       "   Number_of_trades_BNBUSDT     SMA_20    EMA_20  Upper_Band  Middle_Band  \\\n",
       "0             123575.445312  13.337129  6.203595    5.185119    14.871467   \n",
       "1             103865.914062  13.502258  5.276132    4.634593    13.678970   \n",
       "2              61923.730469  13.906770  4.340058    3.639909    13.985720   \n",
       "3              44615.621094  12.739328  2.655619    2.737377    12.665237   \n",
       "4              24934.687500  14.202191  3.334253    2.461271    11.822153   \n",
       "\n",
       "   Lower_Band        RSI  buy_1000x_high_coinbase  sell_1000x_high_coinbase  \\\n",
       "0   12.636547  46.455318                27.768692                 37.203564   \n",
       "1   13.116061  47.258823                27.858992                 34.138977   \n",
       "2   13.464526  46.247765                33.830688                 34.866688   \n",
       "3   14.134830  46.308434                33.212223                 29.476658   \n",
       "4   14.629593  46.222549                31.985155                 40.690067   \n",
       "\n",
       "   total_trades_coinbase  Tweets_Utilizados  Tweets_Utilizados_coin  \\\n",
       "0           20085.880859         165.505447              181.841339   \n",
       "1           24282.658203         135.206604              175.209488   \n",
       "2           24842.693359         148.241348              184.944519   \n",
       "3           31090.611328         118.835098              175.807266   \n",
       "4           25537.763672         109.984833              181.385696   \n",
       "\n",
       "   Tweets_Utilizados_referentes  Tweets_Utilizados_whale_alert  \\\n",
       "0                     -1.374227                       8.755414   \n",
       "1                     -1.266967                       9.644383   \n",
       "2                     -1.242866                      11.186186   \n",
       "3                     -1.788198                      13.375440   \n",
       "4                     -1.550068                       9.773538   \n",
       "\n",
       "   Buy_1000x_high  sell_1000x_high  total_trades_binance      Fecha  \n",
       "0      453.509003       166.304886         -51699.308594 2024-03-18  \n",
       "1      428.860809       152.266159          32977.546875 2024-03-19  \n",
       "2      430.419220       135.614014          97081.687500 2024-03-20  \n",
       "3      464.523315       131.658295          71866.585938 2024-03-21  \n",
       "4      458.367767       131.190948          48780.425781 2024-03-22  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2. Predecir 5 das en el futuro con los modelos entrenados\n",
    "ensemble = MultivariableVotingRegressor(models)\n",
    "ensemble.fit(X, y)\n",
    "future_dataset, predicted_values_desnormalized = predict_next_days(ensemble, feature_dataset, scalers, n_steps, n_features, 5)\n",
    "    \n",
    "display(predicted_values_desnormalized.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Light GBM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
