{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam, RMSprop, SGD, AdamW, Nadam\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.metrics import log_loss\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from keras.utils import to_categorical\n",
    "from adabound import AdaBound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evito que ciertas columnas se transformen a notacion cientifica en las predicciones\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    'Open_time',\n",
    "    'Open',\n",
    "    'High',\n",
    "    'Low',\n",
    "    # 'Close',\n",
    "    'Number of trades',\n",
    "    'Close_BTCUSDT',\n",
    "    'Volume_BTCUSDT',\n",
    "    'Number_of_trades_BTCUSDT',\n",
    "    'Close_ETHUSDT',\n",
    "    'Volume_ETHUSDT',\n",
    "    'Number_of_trades_ETHUSDT',\n",
    "    'Close_BNBUSDT',\n",
    "    'Volume_BNBUSDT',\n",
    "    'Number_of_trades_BNBUSDT',\n",
    "    'SMA_20',\n",
    "    'EMA_20',\n",
    "    'Upper_Band',\n",
    "    'Middle_Band',\n",
    "    'Lower_Band',\n",
    "    'RSI',\n",
    "    'buy_1000x_high_coinbase',\n",
    "    'sell_1000x_high_coinbase',\n",
    "    'total_trades_coinbase',\t\n",
    "    'Tweets_Utilizados',\n",
    "    'Tweets_Utilizados_coin',\n",
    "    'Tweets_Utilizados_referentes',\n",
    "    'Tweets_Utilizados_whale_alert',\n",
    "    'Buy_1000x_high',\n",
    "    'sell_1000x_high',\n",
    "    'total_trades_binance'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Armado y entrenamiento de un clasificador a partir de los datos originales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelo convolucional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_dataset = pd.read_csv('/Users/mmarchetta/Desktop/Tesis-2024/data-visualization/final_dataset.csv') \n",
    "classifier_dataset = complete_dataset[columns]\n",
    "classifier_dataset['Open_time'] = pd.to_datetime(classifier_dataset['Open_time'])\n",
    "classifier_dataset['Tendencia'] = complete_dataset['Tendencia']\n",
    "\n",
    "clasifier_validation = classifier_dataset[-10:]\n",
    "classifier_dataset = classifier_dataset[:-10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open_time</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Number of trades</th>\n",
       "      <th>Close_BTCUSDT</th>\n",
       "      <th>Volume_BTCUSDT</th>\n",
       "      <th>Number_of_trades_BTCUSDT</th>\n",
       "      <th>Close_ETHUSDT</th>\n",
       "      <th>Volume_ETHUSDT</th>\n",
       "      <th>Number_of_trades_ETHUSDT</th>\n",
       "      <th>Close_BNBUSDT</th>\n",
       "      <th>Volume_BNBUSDT</th>\n",
       "      <th>Number_of_trades_BNBUSDT</th>\n",
       "      <th>SMA_20</th>\n",
       "      <th>EMA_20</th>\n",
       "      <th>Upper_Band</th>\n",
       "      <th>Middle_Band</th>\n",
       "      <th>Lower_Band</th>\n",
       "      <th>RSI</th>\n",
       "      <th>buy_1000x_high_coinbase</th>\n",
       "      <th>sell_1000x_high_coinbase</th>\n",
       "      <th>total_trades_coinbase</th>\n",
       "      <th>Tweets_Utilizados</th>\n",
       "      <th>Tweets_Utilizados_coin</th>\n",
       "      <th>Tweets_Utilizados_referentes</th>\n",
       "      <th>Tweets_Utilizados_whale_alert</th>\n",
       "      <th>Buy_1000x_high</th>\n",
       "      <th>sell_1000x_high</th>\n",
       "      <th>total_trades_binance</th>\n",
       "      <th>Tendencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>941</th>\n",
       "      <td>2024-04-25</td>\n",
       "      <td>6.93</td>\n",
       "      <td>7.00</td>\n",
       "      <td>6.70</td>\n",
       "      <td>71088.00</td>\n",
       "      <td>64498.34</td>\n",
       "      <td>31341.46</td>\n",
       "      <td>1375324.00</td>\n",
       "      <td>3155.80</td>\n",
       "      <td>352288.55</td>\n",
       "      <td>861077.00</td>\n",
       "      <td>613.20</td>\n",
       "      <td>453745.52</td>\n",
       "      <td>353114.00</td>\n",
       "      <td>7.43</td>\n",
       "      <td>7.45</td>\n",
       "      <td>9.08</td>\n",
       "      <td>7.43</td>\n",
       "      <td>5.77</td>\n",
       "      <td>38.83</td>\n",
       "      <td>21.00</td>\n",
       "      <td>26.00</td>\n",
       "      <td>33468.00</td>\n",
       "      <td>151</td>\n",
       "      <td>114</td>\n",
       "      <td>0.00</td>\n",
       "      <td>22.00</td>\n",
       "      <td>242.00</td>\n",
       "      <td>219.00</td>\n",
       "      <td>48000.00</td>\n",
       "      <td>Lateral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942</th>\n",
       "      <td>2024-04-26</td>\n",
       "      <td>6.86</td>\n",
       "      <td>6.95</td>\n",
       "      <td>6.71</td>\n",
       "      <td>67383.00</td>\n",
       "      <td>63770.01</td>\n",
       "      <td>27085.19</td>\n",
       "      <td>1025561.00</td>\n",
       "      <td>3131.30</td>\n",
       "      <td>252522.65</td>\n",
       "      <td>628635.00</td>\n",
       "      <td>598.00</td>\n",
       "      <td>302119.88</td>\n",
       "      <td>269508.00</td>\n",
       "      <td>7.34</td>\n",
       "      <td>7.38</td>\n",
       "      <td>8.94</td>\n",
       "      <td>7.34</td>\n",
       "      <td>5.74</td>\n",
       "      <td>37.81</td>\n",
       "      <td>29.00</td>\n",
       "      <td>24.00</td>\n",
       "      <td>26619.00</td>\n",
       "      <td>117</td>\n",
       "      <td>106</td>\n",
       "      <td>0.00</td>\n",
       "      <td>14.00</td>\n",
       "      <td>292.00</td>\n",
       "      <td>324.00</td>\n",
       "      <td>42000.00</td>\n",
       "      <td>Lateral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>943</th>\n",
       "      <td>2024-04-27</td>\n",
       "      <td>6.76</td>\n",
       "      <td>6.87</td>\n",
       "      <td>6.51</td>\n",
       "      <td>64779.00</td>\n",
       "      <td>63461.98</td>\n",
       "      <td>20933.06</td>\n",
       "      <td>912422.00</td>\n",
       "      <td>3255.56</td>\n",
       "      <td>323811.19</td>\n",
       "      <td>734026.00</td>\n",
       "      <td>596.20</td>\n",
       "      <td>268783.91</td>\n",
       "      <td>233820.00</td>\n",
       "      <td>7.24</td>\n",
       "      <td>7.33</td>\n",
       "      <td>8.73</td>\n",
       "      <td>7.24</td>\n",
       "      <td>5.76</td>\n",
       "      <td>38.57</td>\n",
       "      <td>17.00</td>\n",
       "      <td>17.00</td>\n",
       "      <td>25565.00</td>\n",
       "      <td>101</td>\n",
       "      <td>138</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>248.00</td>\n",
       "      <td>179.00</td>\n",
       "      <td>41000.00</td>\n",
       "      <td>Lateral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>944</th>\n",
       "      <td>2024-04-28</td>\n",
       "      <td>6.81</td>\n",
       "      <td>6.95</td>\n",
       "      <td>6.69</td>\n",
       "      <td>43208.00</td>\n",
       "      <td>63118.62</td>\n",
       "      <td>16949.20</td>\n",
       "      <td>790652.00</td>\n",
       "      <td>3263.45</td>\n",
       "      <td>304766.01</td>\n",
       "      <td>753239.00</td>\n",
       "      <td>600.20</td>\n",
       "      <td>258059.43</td>\n",
       "      <td>206703.00</td>\n",
       "      <td>7.13</td>\n",
       "      <td>7.27</td>\n",
       "      <td>8.38</td>\n",
       "      <td>7.13</td>\n",
       "      <td>5.88</td>\n",
       "      <td>37.66</td>\n",
       "      <td>16.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>20954.00</td>\n",
       "      <td>82</td>\n",
       "      <td>106</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13.00</td>\n",
       "      <td>173.00</td>\n",
       "      <td>165.00</td>\n",
       "      <td>26000.00</td>\n",
       "      <td>Lateral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>945</th>\n",
       "      <td>2024-04-29</td>\n",
       "      <td>6.73</td>\n",
       "      <td>6.83</td>\n",
       "      <td>6.47</td>\n",
       "      <td>63006.00</td>\n",
       "      <td>63866.00</td>\n",
       "      <td>28150.23</td>\n",
       "      <td>1152296.00</td>\n",
       "      <td>3216.73</td>\n",
       "      <td>421831.29</td>\n",
       "      <td>943719.00</td>\n",
       "      <td>592.80</td>\n",
       "      <td>330474.01</td>\n",
       "      <td>271926.00</td>\n",
       "      <td>7.03</td>\n",
       "      <td>7.20</td>\n",
       "      <td>8.08</td>\n",
       "      <td>7.03</td>\n",
       "      <td>5.97</td>\n",
       "      <td>36.02</td>\n",
       "      <td>69.00</td>\n",
       "      <td>37.00</td>\n",
       "      <td>33959.00</td>\n",
       "      <td>115</td>\n",
       "      <td>125</td>\n",
       "      <td>0.00</td>\n",
       "      <td>24.00</td>\n",
       "      <td>260.00</td>\n",
       "      <td>188.00</td>\n",
       "      <td>41000.00</td>\n",
       "      <td>Bajista</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Open_time  Open  High  Low  Number of trades  Close_BTCUSDT  \\\n",
       "941 2024-04-25  6.93  7.00 6.70          71088.00       64498.34   \n",
       "942 2024-04-26  6.86  6.95 6.71          67383.00       63770.01   \n",
       "943 2024-04-27  6.76  6.87 6.51          64779.00       63461.98   \n",
       "944 2024-04-28  6.81  6.95 6.69          43208.00       63118.62   \n",
       "945 2024-04-29  6.73  6.83 6.47          63006.00       63866.00   \n",
       "\n",
       "     Volume_BTCUSDT  Number_of_trades_BTCUSDT  Close_ETHUSDT  Volume_ETHUSDT  \\\n",
       "941        31341.46                1375324.00        3155.80       352288.55   \n",
       "942        27085.19                1025561.00        3131.30       252522.65   \n",
       "943        20933.06                 912422.00        3255.56       323811.19   \n",
       "944        16949.20                 790652.00        3263.45       304766.01   \n",
       "945        28150.23                1152296.00        3216.73       421831.29   \n",
       "\n",
       "     Number_of_trades_ETHUSDT  Close_BNBUSDT  Volume_BNBUSDT  \\\n",
       "941                 861077.00         613.20       453745.52   \n",
       "942                 628635.00         598.00       302119.88   \n",
       "943                 734026.00         596.20       268783.91   \n",
       "944                 753239.00         600.20       258059.43   \n",
       "945                 943719.00         592.80       330474.01   \n",
       "\n",
       "     Number_of_trades_BNBUSDT  SMA_20  EMA_20  Upper_Band  Middle_Band  \\\n",
       "941                 353114.00    7.43    7.45        9.08         7.43   \n",
       "942                 269508.00    7.34    7.38        8.94         7.34   \n",
       "943                 233820.00    7.24    7.33        8.73         7.24   \n",
       "944                 206703.00    7.13    7.27        8.38         7.13   \n",
       "945                 271926.00    7.03    7.20        8.08         7.03   \n",
       "\n",
       "     Lower_Band   RSI  buy_1000x_high_coinbase  sell_1000x_high_coinbase  \\\n",
       "941        5.77 38.83                    21.00                     26.00   \n",
       "942        5.74 37.81                    29.00                     24.00   \n",
       "943        5.76 38.57                    17.00                     17.00   \n",
       "944        5.88 37.66                    16.00                     20.00   \n",
       "945        5.97 36.02                    69.00                     37.00   \n",
       "\n",
       "     total_trades_coinbase  Tweets_Utilizados  Tweets_Utilizados_coin  \\\n",
       "941               33468.00                151                     114   \n",
       "942               26619.00                117                     106   \n",
       "943               25565.00                101                     138   \n",
       "944               20954.00                 82                     106   \n",
       "945               33959.00                115                     125   \n",
       "\n",
       "     Tweets_Utilizados_referentes  Tweets_Utilizados_whale_alert  \\\n",
       "941                          0.00                          22.00   \n",
       "942                          0.00                          14.00   \n",
       "943                          0.00                           7.00   \n",
       "944                          0.00                          13.00   \n",
       "945                          0.00                          24.00   \n",
       "\n",
       "     Buy_1000x_high  sell_1000x_high  total_trades_binance Tendencia  \n",
       "941          242.00           219.00              48000.00   Lateral  \n",
       "942          292.00           324.00              42000.00   Lateral  \n",
       "943          248.00           179.00              41000.00   Lateral  \n",
       "944          173.00           165.00              26000.00   Lateral  \n",
       "945          260.00           188.00              41000.00   Bajista  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(classifier_dataset.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(946, 31)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = classifier_dataset.drop(columns=[\"Tendencia\", \"Open_time\"])\n",
    "y = classifier_dataset[\"Tendencia\"]\n",
    "\n",
    "y = y.to_numpy().reshape(-1, 1)\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "y_one_hot = onehot_encoder.fit_transform(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DESCOMENTAR PARA NORMALIZACION\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_validation = clasifier_validation.drop(columns=[\"Tendencia\", \"Open_time\"])\n",
    "y_validation = clasifier_validation[\"Tendencia\"]\n",
    "\n",
    "X_validation_scaled = scaler.transform(X_validation)\n",
    "y_validation = y_validation.to_numpy().reshape(-1, 1)\n",
    "y_validation_one_hot = onehot_encoder.transform(y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(y_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create the model\n",
    "def create_model(activation, units, dropout, learning_rate, l2_penalty, depth, optimizer='adam'):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(units, kernel_size=3, activation=activation, input_shape=(len(X.columns), 1)))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    for _ in range(depth - 1):\n",
    "        model.add(Conv1D(units, kernel_size=3, activation=activation))\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units, activation=activation, kernel_regularizer=l2(l2_penalty)))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(units=3, activation='softmax'))\n",
    "\n",
    "    if optimizer == 'adam':\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == 'rmsprop':\n",
    "        optimizer = RMSprop(learning_rate=learning_rate)\n",
    "    elif optimizer == 'sgd':\n",
    "        optimizer = SGD(learning_rate=learning_rate)\n",
    "    elif optimizer == 'adamw':\n",
    "        optimizer = AdamW(learning_rate=learning_rate)\n",
    "    # elif optimizer == 'nadam':\n",
    "    #     optimizer = Nadam(learning_rate=learning_rate)\n",
    "    # elif optimizer == 'adabound':\n",
    "    #     optimizer = AdaBound(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=[AUC(name='auc')])\n",
    "    return model\n",
    "\n",
    "\n",
    "classifier = KerasClassifier(build_fn=create_model, verbose=0, activation='relu', units=50, dropout=0.2, learning_rate=0.1, l2_penalty=0.001, depth=2, optimizer='adam')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=0.00001)\n",
    "\n",
    "# Define cross-validation\n",
    "cv = TimeSeriesSplit(n_splits=10)\n",
    "\n",
    "# Define parameter space\n",
    "param_space = {\n",
    "    'depth': [2, 3],\n",
    "    'activation': ['relu', 'tanh', 'swish', 'selu'],\n",
    "    'units': [64, 128, 256, 512],\n",
    "    'dropout': [0.1, 0.2, 0.3, 0.4],\n",
    "    'learning_rate': [0.01, 0.001, 0.0001],\n",
    "    'epochs': [10, 20, 30, 50, 100],\n",
    "    'batch_size': [32, 64, 128],\n",
    "    'optimizer': ['adam', 'adamw', 'rmsprop', 'sgd'],\n",
    "    # 'optimizer': ['adam', , 'nadam', 'adabound'],\n",
    "    'l2_penalty': [0.001, 0.01, 0.1]\n",
    "}\n",
    "\n",
    "def categorical_crossentropy_loss(estimator, X_test, y_test):\n",
    "    y_pred = estimator.predict_proba(X_test)\n",
    "    loss = log_loss(y_test, y_pred)\n",
    "    return loss\n",
    "\n",
    "# Perform Bayesian optimization\n",
    "bayes_search = BayesSearchCV(classifier, param_space, scoring=categorical_crossentropy_loss, verbose=0, cv=cv)\n",
    "bayes_result = bayes_search.fit(X_scaled, y_one_hot, callbacks=[early_stopping, reduce_lr])\n",
    "# bayes_result = bayes_search.fit(X, y_one_hot, callbacks=[early_stopping, reduce_lr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 10.844193264652308\n",
      "Best parameters: OrderedDict([('activation', 'tanh'), ('batch_size', 64), ('depth', 3), ('dropout', 0.2), ('epochs', 100), ('l2_penalty', 0.1), ('learning_rate', 0.001), ('optimizer', 'adamw'), ('units', 512)])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KerasClassifier(\n",
       "\tmodel=None\n",
       "\tbuild_fn=&lt;function create_model at 0x7f86acb073a0&gt;\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=adamw\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=64\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=0\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=100\n",
       "\tactivation=tanh\n",
       "\tunits=512\n",
       "\tdropout=0.2\n",
       "\tlearning_rate=0.001\n",
       "\tl2_penalty=0.1\n",
       "\tdepth=3\n",
       "\tclass_weight=None\n",
       ")</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KerasClassifier</label><div class=\"sk-toggleable__content\"><pre>KerasClassifier(\n",
       "\tmodel=None\n",
       "\tbuild_fn=&lt;function create_model at 0x7f86acb073a0&gt;\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=adamw\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=64\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=0\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=100\n",
       "\tactivation=tanh\n",
       "\tunits=512\n",
       "\tdropout=0.2\n",
       "\tlearning_rate=0.001\n",
       "\tl2_penalty=0.1\n",
       "\tdepth=3\n",
       "\tclass_weight=None\n",
       ")</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KerasClassifier(\n",
       "\tmodel=None\n",
       "\tbuild_fn=<function create_model at 0x7f86acb073a0>\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=adamw\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=64\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=0\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=100\n",
       "\tactivation=tanh\n",
       "\tunits=512\n",
       "\tdropout=0.2\n",
       "\tlearning_rate=0.001\n",
       "\tl2_penalty=0.1\n",
       "\tdepth=3\n",
       "\tclass_weight=None\n",
       ")"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show best results\n",
    "print(\"Best score:\", bayes_result.best_score_)\n",
    "print(\"Best parameters:\", bayes_result.best_params_)\n",
    "\n",
    "# Entrenar el modelo con los mejores hiperparámetros\n",
    "best_model = bayes_result.best_estimator_\n",
    "best_model.fit(X_scaled, y_one_hot)\n",
    "# best_model.fit(X, y_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history = bayes_result.best_estimator_.model_.history\n",
    "\n",
    "plt.plot(history.history['auc'])\n",
    "# plt.plot(history.history['val_auc'])\n",
    "plt.title('Model AUC')\n",
    "plt.ylabel('AUC')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 mejores modelos:\n",
      "Modelo 1\n",
      "Hiperparámetros: OrderedDict([('activation', 'tanh'), ('batch_size', 64), ('depth', 3), ('dropout', 0.2), ('epochs', 100), ('l2_penalty', 0.1), ('learning_rate', 0.001), ('optimizer', 'adamw'), ('units', 512)])\n",
      "Puntaje: 10.844193264652308\n",
      "Modelo 2\n",
      "Hiperparámetros: OrderedDict([('activation', 'tanh'), ('batch_size', 64), ('depth', 3), ('dropout', 0.3), ('epochs', 100), ('l2_penalty', 0.01), ('learning_rate', 0.001), ('optimizer', 'sgd'), ('units', 512)])\n",
      "Puntaje: 9.893563327676508\n",
      "Modelo 3\n",
      "Hiperparámetros: OrderedDict([('activation', 'tanh'), ('batch_size', 64), ('depth', 3), ('dropout', 0.1), ('epochs', 100), ('l2_penalty', 0.001), ('learning_rate', 0.001), ('optimizer', 'adamw'), ('units', 512)])\n",
      "Puntaje: 9.523062947877543\n",
      "Modelo 4\n",
      "Hiperparámetros: OrderedDict([('activation', 'tanh'), ('batch_size', 32), ('depth', 3), ('dropout', 0.2), ('epochs', 100), ('l2_penalty', 0.1), ('learning_rate', 0.001), ('optimizer', 'sgd'), ('units', 512)])\n",
      "Puntaje: 9.507484760114245\n",
      "Modelo 5\n",
      "Hiperparámetros: OrderedDict([('activation', 'tanh'), ('batch_size', 64), ('depth', 3), ('dropout', 0.2), ('epochs', 100), ('l2_penalty', 0.1), ('learning_rate', 0.001), ('optimizer', 'sgd'), ('units', 512)])\n",
      "Puntaje: 7.8749370943773584\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Obtener los hiperparámetros y puntajes de los 5 mejores modelos\n",
    "top_n_models = 5\n",
    "best_params_list = []\n",
    "best_scores_list = []\n",
    "\n",
    "if 'rank_test_score' in bayes_search.cv_results_:\n",
    "    sorted_indices = np.argsort(bayes_search.cv_results_['rank_test_score'])\n",
    "\n",
    "    for i in range(min(top_n_models, len(sorted_indices))):\n",
    "        best_params_list.append(bayes_search.cv_results_['params'][sorted_indices[i]])\n",
    "        best_scores_list.append(bayes_search.cv_results_['mean_test_score'][sorted_indices[i]])\n",
    "\n",
    "    # Guardar los hiperparámetros de los 5 mejores modelos en un archivo JSON\n",
    "    with open('conv_classifier/top_5_hyperparameters.json', 'w') as f:\n",
    "        json.dump({'best_params': best_params_list, 'best_scores': best_scores_list}, f)\n",
    "\n",
    "    # O imprimir los hiperparámetros\n",
    "    print(\"Top 5 mejores modelos:\")\n",
    "    for i in range(len(best_params_list)):\n",
    "        print(\"Modelo\", i + 1)\n",
    "        print(\"Hiperparámetros:\", best_params_list[i])\n",
    "        print(\"Puntaje:\", best_scores_list[i])\n",
    "\n",
    "else:\n",
    "    print(\"Error: 'rank_test_score' no encontrado en cv_results_\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Armado del ensamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prime_seeds(n):\n",
    "    seeds = []\n",
    "    num = 70001  # Comenzamos desde el primer número primo mayor que 70000\n",
    "    while len(seeds) < n:\n",
    "        is_prime = True\n",
    "        for i in range(2, int(num**0.5) + 1):\n",
    "            if num % i == 0:\n",
    "                is_prime = False\n",
    "                break\n",
    "        if is_prime:\n",
    "            seeds.append(num)\n",
    "        num += 1\n",
    "    return seeds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clase personalizada para hacer el ensamble, dado que sklearn no provee ninguna clase que permita hacer ensmble\n",
    "## de modelos re regresion multivariados\n",
    "class MultivariableVotingClassifier:\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for model in self.models:\n",
    "            model.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Hacer predicciones con cada modelo\n",
    "        predictions = np.array([model.predict(X) for model in self.models])\n",
    "        \n",
    "        # Calcular la moda de las predicciones\n",
    "        mode_predictions = np.argmax(np.sum(predictions, axis=0), axis=1)\n",
    "        \n",
    "        return mode_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model number 4, seed number 4\n",
      "model number 4, seed number 5\n",
      "model number 4, seed number 6\n",
      "model number 4, seed number 7\n",
      "model number 4, seed number 8\n",
      "model number 4, seed number 9\n",
      "model number 4, seed number 10\n",
      "model number 4, seed number 11\n",
      "model number 4, seed number 12\n",
      "model number 4, seed number 13\n",
      "model number 4, seed number 14\n",
      "model number 4, seed number 15\n",
      "model number 4, seed number 16\n",
      "model number 4, seed number 17\n",
      "model number 4, seed number 18\n",
      "model number 4, seed number 19\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "# Leer los hiperparámetros desde el archivo JSON\n",
    "with open('conv_classifier/top_5_hyperparameters.json', 'r') as f:\n",
    "    top_hyperparameters = json.load(f)\n",
    "\n",
    "prime_seeds = generate_prime_seeds(20)\n",
    "models = []\n",
    "best_seeds= {}\n",
    "\n",
    "# Train models with different seeds for each set of hyperparameters\n",
    "for mode_number, params in enumerate(top_hyperparameters['best_params']):\n",
    "    best_validation_errors = {}\n",
    "    \n",
    "    for seed_number, seed in enumerate(prime_seeds):\n",
    "        model = KerasClassifier(build_fn=create_model, random_state=seed, verbose=0, **params)\n",
    "        model.fit(X_scaled, y_one_hot)\n",
    "        # model.fit(X, y_one_hot)\n",
    "        \n",
    "        train_error = categorical_crossentropy_loss(model, X_scaled, y_one_hot)\n",
    "        # train_error = categorical_crossentropy_loss(model, X, y_one_hot)\n",
    "        \n",
    "        mean_train_error = np.mean(train_error)\n",
    "        \n",
    "        # Update best validation error for this seed\n",
    "        best_validation_errors[seed] = mean_train_error\n",
    "        \n",
    "        print(f\"model number {mode_number}, seed number {seed_number}\")\n",
    "    \n",
    "    # print(\"Best validation errors:\", best_validation_errors)\n",
    "\n",
    "    # Find the best seed for this set of hyperparameters\n",
    "    best_seed_for_params = min(best_validation_errors, key=lambda k: best_validation_errors[k])\n",
    "    best_seeds[str(params)] = best_seed_for_params\n",
    "    \n",
    "    # Create and train the model with the best seed\n",
    "    model = KerasClassifier(build_fn=create_model, random_state=best_seed_for_params, verbose=0, **params)\n",
    "    model.fit(X_scaled, y_one_hot)\n",
    "    # model.fit(X, y_one_hot)\n",
    "    \n",
    "    models.append(model)\n",
    "\n",
    "# Define and train the ensemble model\n",
    "ensemble = MultivariableVotingClassifier(models)\n",
    "ensemble.fit(X_scaled, y_one_hot)\n",
    "# ensemble.fit(X, y_one_hot)\n",
    "\n",
    "# Save the best seeds to a JSON file\n",
    "with open('conv_classifier/best_seeds.json', 'w') as f:\n",
    "    json.dump(best_seeds, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clasificacion con el ensamble sobre las redicciones de los modelos generativos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "15/15 [==============================] - 14s 82ms/step - loss: 43.9164 - auc: 0.5871\n",
      "Epoch 2/100\n",
      "15/15 [==============================] - 1s 79ms/step - loss: 27.3612 - auc: 0.6070\n",
      "Epoch 3/100\n",
      "15/15 [==============================] - 1s 77ms/step - loss: 16.3435 - auc: 0.6459\n",
      "Epoch 4/100\n",
      "15/15 [==============================] - 1s 85ms/step - loss: 9.6473 - auc: 0.6613\n",
      "Epoch 5/100\n",
      "15/15 [==============================] - 1s 95ms/step - loss: 5.8032 - auc: 0.6475\n",
      "Epoch 6/100\n",
      "15/15 [==============================] - 1s 88ms/step - loss: 3.5955 - auc: 0.6773\n",
      "Epoch 7/100\n",
      "15/15 [==============================] - 1s 85ms/step - loss: 2.4229 - auc: 0.6763\n",
      "Epoch 8/100\n",
      "15/15 [==============================] - 1s 83ms/step - loss: 1.7859 - auc: 0.6966\n",
      "Epoch 9/100\n",
      "15/15 [==============================] - 1s 81ms/step - loss: 1.4773 - auc: 0.6875\n",
      "Epoch 10/100\n",
      "15/15 [==============================] - 1s 79ms/step - loss: 1.3469 - auc: 0.6827\n",
      "Epoch 11/100\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 1.2324 - auc: 0.7036\n",
      "Epoch 12/100\n",
      "15/15 [==============================] - 1s 94ms/step - loss: 1.1966 - auc: 0.6901\n",
      "Epoch 13/100\n",
      "15/15 [==============================] - 2s 109ms/step - loss: 1.1818 - auc: 0.6871\n",
      "Epoch 14/100\n",
      "15/15 [==============================] - 2s 100ms/step - loss: 1.1725 - auc: 0.6976\n",
      "Epoch 15/100\n",
      "15/15 [==============================] - 1s 89ms/step - loss: 1.1906 - auc: 0.6941\n",
      "Epoch 16/100\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 1.1711 - auc: 0.7005\n",
      "Epoch 17/100\n",
      "15/15 [==============================] - 2s 107ms/step - loss: 1.1065 - auc: 0.7322\n",
      "Epoch 18/100\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 1.1923 - auc: 0.7176\n",
      "Epoch 19/100\n",
      "15/15 [==============================] - 1s 94ms/step - loss: 1.2653 - auc: 0.7336\n",
      "Epoch 20/100\n",
      "15/15 [==============================] - 2s 100ms/step - loss: 1.2934 - auc: 0.7355\n",
      "Epoch 21/100\n",
      "15/15 [==============================] - 2s 101ms/step - loss: 1.2507 - auc: 0.7535\n",
      "Epoch 22/100\n",
      "15/15 [==============================] - 2s 114ms/step - loss: 1.2297 - auc: 0.7365\n",
      "Epoch 23/100\n",
      "15/15 [==============================] - 1s 100ms/step - loss: 1.2310 - auc: 0.7298\n",
      "Epoch 24/100\n",
      "15/15 [==============================] - 2s 108ms/step - loss: 1.1269 - auc: 0.7719\n",
      "Epoch 25/100\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 1.1858 - auc: 0.7484\n",
      "Epoch 26/100\n",
      "15/15 [==============================] - 2s 101ms/step - loss: 1.2207 - auc: 0.7537\n",
      "Epoch 27/100\n",
      "15/15 [==============================] - 1s 95ms/step - loss: 1.1227 - auc: 0.8083\n",
      "Epoch 28/100\n",
      "15/15 [==============================] - 1s 94ms/step - loss: 1.1451 - auc: 0.7905\n",
      "Epoch 29/100\n",
      "15/15 [==============================] - 1s 88ms/step - loss: 1.0844 - auc: 0.7933\n",
      "Epoch 30/100\n",
      "15/15 [==============================] - 1s 89ms/step - loss: 1.0455 - auc: 0.8048\n",
      "Epoch 31/100\n",
      "15/15 [==============================] - 2s 106ms/step - loss: 1.0189 - auc: 0.8102\n",
      "Epoch 32/100\n",
      "15/15 [==============================] - 1s 99ms/step - loss: 0.9967 - auc: 0.8160\n",
      "Epoch 33/100\n",
      "15/15 [==============================] - 1s 87ms/step - loss: 0.9904 - auc: 0.8233\n",
      "Epoch 34/100\n",
      "15/15 [==============================] - 1s 90ms/step - loss: 0.9886 - auc: 0.8282\n",
      "Epoch 35/100\n",
      "15/15 [==============================] - 1s 92ms/step - loss: 0.9630 - auc: 0.8364\n",
      "Epoch 36/100\n",
      "15/15 [==============================] - 1s 95ms/step - loss: 0.9744 - auc: 0.8232\n",
      "Epoch 37/100\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 1.0361 - auc: 0.8056\n",
      "Epoch 38/100\n",
      "15/15 [==============================] - 2s 117ms/step - loss: 1.0091 - auc: 0.8332\n",
      "Epoch 39/100\n",
      "15/15 [==============================] - 1s 97ms/step - loss: 0.9436 - auc: 0.8601\n",
      "Epoch 40/100\n",
      "15/15 [==============================] - 1s 98ms/step - loss: 0.9010 - auc: 0.8623\n",
      "Epoch 41/100\n",
      "15/15 [==============================] - 1s 89ms/step - loss: 0.9444 - auc: 0.8389\n",
      "Epoch 42/100\n",
      "15/15 [==============================] - 1s 84ms/step - loss: 0.8751 - auc: 0.8560\n",
      "Epoch 43/100\n",
      "15/15 [==============================] - 1s 83ms/step - loss: 0.9372 - auc: 0.8338\n",
      "Epoch 44/100\n",
      "15/15 [==============================] - 1s 91ms/step - loss: 0.9608 - auc: 0.8131\n",
      "Epoch 45/100\n",
      "15/15 [==============================] - 2s 111ms/step - loss: 0.9986 - auc: 0.8112\n",
      "Epoch 46/100\n",
      "15/15 [==============================] - 2s 101ms/step - loss: 0.9124 - auc: 0.8516\n",
      "Epoch 47/100\n",
      "15/15 [==============================] - 1s 89ms/step - loss: 0.9052 - auc: 0.8530\n",
      "Epoch 48/100\n",
      "15/15 [==============================] - 1s 85ms/step - loss: 0.9144 - auc: 0.8416\n",
      "Epoch 49/100\n",
      "15/15 [==============================] - 1s 77ms/step - loss: 0.9146 - auc: 0.8365\n",
      "Epoch 50/100\n",
      "15/15 [==============================] - 1s 79ms/step - loss: 0.8909 - auc: 0.8566\n",
      "Epoch 51/100\n",
      "15/15 [==============================] - 1s 79ms/step - loss: 0.9188 - auc: 0.8381\n",
      "Epoch 52/100\n",
      "15/15 [==============================] - 2s 111ms/step - loss: 0.8462 - auc: 0.8645\n",
      "Epoch 53/100\n",
      "15/15 [==============================] - 1s 94ms/step - loss: 0.9163 - auc: 0.8357\n",
      "Epoch 54/100\n",
      "15/15 [==============================] - 1s 85ms/step - loss: 0.9379 - auc: 0.8271\n",
      "Epoch 55/100\n",
      "15/15 [==============================] - 1s 83ms/step - loss: 0.8462 - auc: 0.8617\n",
      "Epoch 56/100\n",
      "15/15 [==============================] - 1s 86ms/step - loss: 0.8537 - auc: 0.8562\n",
      "Epoch 57/100\n",
      "15/15 [==============================] - 1s 92ms/step - loss: 0.8685 - auc: 0.8454\n",
      "Epoch 58/100\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 0.9048 - auc: 0.8263\n",
      "Epoch 59/100\n",
      "15/15 [==============================] - 1s 88ms/step - loss: 0.7884 - auc: 0.8830\n",
      "Epoch 60/100\n",
      "15/15 [==============================] - 1s 84ms/step - loss: 0.8098 - auc: 0.8694\n",
      "Epoch 61/100\n",
      "15/15 [==============================] - 1s 87ms/step - loss: 0.7951 - auc: 0.8709\n",
      "Epoch 62/100\n",
      "15/15 [==============================] - 1s 81ms/step - loss: 0.7960 - auc: 0.8688\n",
      "Epoch 63/100\n",
      "15/15 [==============================] - 1s 90ms/step - loss: 0.8158 - auc: 0.8658\n",
      "Epoch 64/100\n",
      "15/15 [==============================] - 1s 100ms/step - loss: 0.7973 - auc: 0.8701\n",
      "Epoch 65/100\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.9178 - auc: 0.8292\n",
      "Epoch 66/100\n",
      "15/15 [==============================] - 2s 106ms/step - loss: 0.8192 - auc: 0.8651\n",
      "Epoch 67/100\n",
      "15/15 [==============================] - 1s 88ms/step - loss: 0.8513 - auc: 0.8544\n",
      "Epoch 68/100\n",
      "15/15 [==============================] - 1s 84ms/step - loss: 0.7972 - auc: 0.8713\n",
      "Epoch 69/100\n",
      "15/15 [==============================] - 1s 97ms/step - loss: 0.8187 - auc: 0.8557\n",
      "Epoch 70/100\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.7353 - auc: 0.8805\n",
      "Epoch 71/100\n",
      "15/15 [==============================] - 2s 113ms/step - loss: 0.7667 - auc: 0.8687\n",
      "Epoch 72/100\n",
      "15/15 [==============================] - 2s 109ms/step - loss: 0.7732 - auc: 0.8637\n",
      "Epoch 73/100\n",
      "15/15 [==============================] - 1s 97ms/step - loss: 0.8054 - auc: 0.8589\n",
      "Epoch 74/100\n",
      "15/15 [==============================] - 1s 84ms/step - loss: 0.8054 - auc: 0.8568\n",
      "Epoch 75/100\n",
      "15/15 [==============================] - 1s 81ms/step - loss: 0.7683 - auc: 0.8713\n",
      "Epoch 76/100\n",
      "15/15 [==============================] - 1s 87ms/step - loss: 0.7783 - auc: 0.8640\n",
      "Epoch 77/100\n",
      "15/15 [==============================] - 1s 95ms/step - loss: 0.7587 - auc: 0.8754\n",
      "Epoch 78/100\n",
      "15/15 [==============================] - 1s 99ms/step - loss: 0.7376 - auc: 0.8865\n",
      "Epoch 79/100\n",
      "15/15 [==============================] - 2s 109ms/step - loss: 0.7939 - auc: 0.8609\n",
      "Epoch 80/100\n",
      "15/15 [==============================] - 1s 98ms/step - loss: 0.8658 - auc: 0.8311\n",
      "Epoch 81/100\n",
      "15/15 [==============================] - 2s 123ms/step - loss: 0.7617 - auc: 0.8764\n",
      "Epoch 82/100\n",
      "15/15 [==============================] - 2s 119ms/step - loss: 0.7949 - auc: 0.8631\n",
      "Epoch 83/100\n",
      "15/15 [==============================] - 2s 125ms/step - loss: 0.7484 - auc: 0.8729\n",
      "Epoch 84/100\n",
      "15/15 [==============================] - 1s 96ms/step - loss: 0.7451 - auc: 0.8749\n",
      "Epoch 85/100\n",
      "15/15 [==============================] - 1s 80ms/step - loss: 0.7868 - auc: 0.8595\n",
      "Epoch 86/100\n",
      "15/15 [==============================] - 1s 79ms/step - loss: 0.7981 - auc: 0.8644\n",
      "Epoch 87/100\n",
      "15/15 [==============================] - 1s 89ms/step - loss: 0.7889 - auc: 0.8708\n",
      "Epoch 88/100\n",
      "15/15 [==============================] - 1s 81ms/step - loss: 0.7774 - auc: 0.8758\n",
      "Epoch 89/100\n",
      "15/15 [==============================] - 1s 88ms/step - loss: 0.7463 - auc: 0.8811\n",
      "Epoch 90/100\n",
      "15/15 [==============================] - 1s 101ms/step - loss: 0.7094 - auc: 0.8910\n",
      "Epoch 91/100\n",
      "15/15 [==============================] - 2s 106ms/step - loss: 0.7175 - auc: 0.8840\n",
      "Epoch 92/100\n",
      "15/15 [==============================] - 2s 117ms/step - loss: 0.7153 - auc: 0.8857\n",
      "Epoch 93/100\n",
      "15/15 [==============================] - 1s 94ms/step - loss: 0.7978 - auc: 0.8567\n",
      "Epoch 94/100\n",
      "15/15 [==============================] - 1s 97ms/step - loss: 0.7611 - auc: 0.8679\n",
      "Epoch 95/100\n",
      "15/15 [==============================] - 1s 91ms/step - loss: 0.7283 - auc: 0.8829\n",
      "Epoch 96/100\n",
      "15/15 [==============================] - 1s 83ms/step - loss: 0.7056 - auc: 0.8867\n",
      "Epoch 97/100\n",
      "15/15 [==============================] - 1s 90ms/step - loss: 0.7546 - auc: 0.8682\n",
      "Epoch 98/100\n",
      "15/15 [==============================] - 1s 98ms/step - loss: 0.7403 - auc: 0.8763\n",
      "Epoch 99/100\n",
      "15/15 [==============================] - 1s 97ms/step - loss: 0.7145 - auc: 0.8898\n",
      "Epoch 100/100\n",
      "15/15 [==============================] - 1s 94ms/step - loss: 0.6763 - auc: 0.8978\n",
      "Epoch 1/100\n",
      "15/15 [==============================] - 11s 60ms/step - loss: 6.6407 - auc: 0.5699\n",
      "Epoch 2/100\n",
      "15/15 [==============================] - 1s 50ms/step - loss: 5.6947 - auc: 0.6260\n",
      "Epoch 3/100\n",
      "15/15 [==============================] - 1s 57ms/step - loss: 5.1697 - auc: 0.6202\n",
      "Epoch 4/100\n",
      "15/15 [==============================] - 1s 75ms/step - loss: 4.6286 - auc: 0.6391\n",
      "Epoch 5/100\n",
      "15/15 [==============================] - 1s 60ms/step - loss: 4.1355 - auc: 0.6644\n",
      "Epoch 6/100\n",
      "15/15 [==============================] - 1s 58ms/step - loss: 3.7590 - auc: 0.6592\n",
      "Epoch 7/100\n",
      "15/15 [==============================] - 1s 87ms/step - loss: 3.3737 - auc: 0.6698\n",
      "Epoch 8/100\n",
      "15/15 [==============================] - 1s 76ms/step - loss: 3.0430 - auc: 0.6785\n",
      "Epoch 9/100\n",
      "15/15 [==============================] - 1s 50ms/step - loss: 2.7240 - auc: 0.7010\n",
      "Epoch 10/100\n",
      "15/15 [==============================] - 1s 49ms/step - loss: 2.5341 - auc: 0.6923\n",
      "Epoch 11/100\n",
      "15/15 [==============================] - 1s 59ms/step - loss: 2.3254 - auc: 0.6874\n",
      "Epoch 12/100\n",
      "15/15 [==============================] - 1s 51ms/step - loss: 2.1219 - auc: 0.7083\n",
      "Epoch 13/100\n",
      "15/15 [==============================] - 1s 53ms/step - loss: 2.0223 - auc: 0.6850\n",
      "Epoch 14/100\n",
      "15/15 [==============================] - 1s 51ms/step - loss: 1.8457 - auc: 0.7054\n",
      "Epoch 15/100\n",
      "15/15 [==============================] - 1s 56ms/step - loss: 1.7663 - auc: 0.6915\n",
      "Epoch 16/100\n",
      "15/15 [==============================] - 1s 51ms/step - loss: 1.6908 - auc: 0.6930\n",
      "Epoch 17/100\n",
      "15/15 [==============================] - 1s 52ms/step - loss: 1.5773 - auc: 0.7004\n",
      "Epoch 18/100\n",
      "15/15 [==============================] - 1s 71ms/step - loss: 1.4995 - auc: 0.7165\n",
      "Epoch 19/100\n",
      "15/15 [==============================] - 1s 55ms/step - loss: 1.4558 - auc: 0.7090\n",
      "Epoch 20/100\n",
      "15/15 [==============================] - 1s 58ms/step - loss: 1.3478 - auc: 0.7331\n",
      "Epoch 21/100\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 1.3583 - auc: 0.7079\n",
      "Epoch 22/100\n",
      "15/15 [==============================] - 1s 59ms/step - loss: 1.3217 - auc: 0.7057\n",
      "Epoch 23/100\n",
      "15/15 [==============================] - 1s 59ms/step - loss: 1.2551 - auc: 0.7333\n",
      "Epoch 24/100\n",
      "15/15 [==============================] - 1s 60ms/step - loss: 1.2856 - auc: 0.6932\n",
      "Epoch 25/100\n",
      "15/15 [==============================] - 1s 65ms/step - loss: 1.1982 - auc: 0.7340\n",
      "Epoch 26/100\n",
      "15/15 [==============================] - 1s 79ms/step - loss: 1.2145 - auc: 0.7148\n",
      "Epoch 27/100\n",
      "15/15 [==============================] - 1s 70ms/step - loss: 1.1160 - auc: 0.7495\n",
      "Epoch 28/100\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 1.1216 - auc: 0.7465\n",
      "Epoch 29/100\n",
      "15/15 [==============================] - 1s 58ms/step - loss: 1.0967 - auc: 0.7502\n",
      "Epoch 30/100\n",
      "15/15 [==============================] - 1s 56ms/step - loss: 1.1257 - auc: 0.7277\n",
      "Epoch 31/100\n",
      "15/15 [==============================] - 1s 49ms/step - loss: 1.1074 - auc: 0.7260\n",
      "Epoch 32/100\n",
      "15/15 [==============================] - 1s 45ms/step - loss: 1.0902 - auc: 0.7335\n",
      "Epoch 33/100\n",
      "15/15 [==============================] - 1s 61ms/step - loss: 1.0459 - auc: 0.7547\n",
      "Epoch 34/100\n",
      "15/15 [==============================] - 1s 46ms/step - loss: 1.0619 - auc: 0.7451\n",
      "Epoch 35/100\n",
      "15/15 [==============================] - 1s 45ms/step - loss: 1.0750 - auc: 0.7378\n",
      "Epoch 36/100\n",
      "15/15 [==============================] - 1s 48ms/step - loss: 1.0420 - auc: 0.7587\n",
      "Epoch 37/100\n",
      "15/15 [==============================] - 1s 47ms/step - loss: 1.0513 - auc: 0.7497\n",
      "Epoch 38/100\n",
      "15/15 [==============================] - 1s 51ms/step - loss: 1.0666 - auc: 0.7334\n",
      "Epoch 39/100\n",
      "15/15 [==============================] - 1s 59ms/step - loss: 1.0370 - auc: 0.7480\n",
      "Epoch 40/100\n",
      "15/15 [==============================] - 1s 49ms/step - loss: 0.9851 - auc: 0.7828\n",
      "Epoch 41/100\n",
      "15/15 [==============================] - 1s 52ms/step - loss: 0.9613 - auc: 0.7937\n",
      "Epoch 42/100\n",
      "15/15 [==============================] - 1s 49ms/step - loss: 0.9445 - auc: 0.7969\n",
      "Epoch 43/100\n",
      "15/15 [==============================] - 1s 57ms/step - loss: 0.9699 - auc: 0.7840\n",
      "Epoch 44/100\n",
      "15/15 [==============================] - 1s 61ms/step - loss: 0.9112 - auc: 0.8091\n",
      "Epoch 45/100\n",
      "15/15 [==============================] - 1s 52ms/step - loss: 0.9229 - auc: 0.8043\n",
      "Epoch 46/100\n",
      "15/15 [==============================] - 1s 55ms/step - loss: 0.8928 - auc: 0.8223\n",
      "Epoch 47/100\n",
      "15/15 [==============================] - 1s 62ms/step - loss: 0.8718 - auc: 0.8240\n",
      "Epoch 48/100\n",
      "15/15 [==============================] - 1s 50ms/step - loss: 0.8493 - auc: 0.8339\n",
      "Epoch 49/100\n",
      "15/15 [==============================] - 1s 49ms/step - loss: 0.8663 - auc: 0.8252\n",
      "Epoch 50/100\n",
      "15/15 [==============================] - 1s 52ms/step - loss: 0.8968 - auc: 0.8044\n",
      "Epoch 51/100\n",
      "15/15 [==============================] - 1s 48ms/step - loss: 0.8874 - auc: 0.8101\n",
      "Epoch 52/100\n",
      "15/15 [==============================] - 1s 60ms/step - loss: 0.9138 - auc: 0.8019\n",
      "Epoch 53/100\n",
      "15/15 [==============================] - 1s 48ms/step - loss: 0.8302 - auc: 0.8421\n",
      "Epoch 54/100\n",
      "15/15 [==============================] - 1s 47ms/step - loss: 0.8037 - auc: 0.8544\n",
      "Epoch 55/100\n",
      "15/15 [==============================] - 1s 48ms/step - loss: 0.7731 - auc: 0.8622\n",
      "Epoch 56/100\n",
      "15/15 [==============================] - 1s 49ms/step - loss: 0.7595 - auc: 0.8638\n",
      "Epoch 57/100\n",
      "15/15 [==============================] - 1s 49ms/step - loss: 0.7992 - auc: 0.8535\n",
      "Epoch 58/100\n",
      "15/15 [==============================] - 1s 59ms/step - loss: 0.7651 - auc: 0.8590\n",
      "Epoch 59/100\n",
      "15/15 [==============================] - 1s 46ms/step - loss: 0.7877 - auc: 0.8512\n",
      "Epoch 60/100\n",
      "15/15 [==============================] - 1s 48ms/step - loss: 0.7537 - auc: 0.8650\n",
      "Epoch 61/100\n",
      "15/15 [==============================] - 1s 49ms/step - loss: 0.8193 - auc: 0.8382\n",
      "Epoch 62/100\n",
      "15/15 [==============================] - 1s 49ms/step - loss: 0.7407 - auc: 0.8692\n",
      "Epoch 63/100\n",
      "15/15 [==============================] - 1s 49ms/step - loss: 0.6986 - auc: 0.8855\n",
      "Epoch 64/100\n",
      "15/15 [==============================] - 1s 60ms/step - loss: 0.7740 - auc: 0.8579\n",
      "Epoch 65/100\n",
      "15/15 [==============================] - 1s 89ms/step - loss: 0.7699 - auc: 0.8592\n",
      "Epoch 66/100\n",
      "15/15 [==============================] - 1s 63ms/step - loss: 0.7320 - auc: 0.8704\n",
      "Epoch 67/100\n",
      "15/15 [==============================] - 1s 62ms/step - loss: 0.7167 - auc: 0.8751\n",
      "Epoch 68/100\n",
      "15/15 [==============================] - 1s 76ms/step - loss: 0.7015 - auc: 0.8843\n",
      "Epoch 69/100\n",
      "15/15 [==============================] - 1s 82ms/step - loss: 0.7619 - auc: 0.8594\n",
      "Epoch 70/100\n",
      "15/15 [==============================] - 1s 70ms/step - loss: 0.7761 - auc: 0.8534\n",
      "Epoch 71/100\n",
      "15/15 [==============================] - 1s 54ms/step - loss: 0.7738 - auc: 0.8538\n",
      "Epoch 72/100\n",
      "15/15 [==============================] - 1s 90ms/step - loss: 0.7529 - auc: 0.8623\n",
      "Epoch 73/100\n",
      "15/15 [==============================] - 1s 93ms/step - loss: 0.7047 - auc: 0.8805\n",
      "Epoch 74/100\n",
      "15/15 [==============================] - 1s 96ms/step - loss: 0.6711 - auc: 0.8916\n",
      "Epoch 75/100\n",
      "15/15 [==============================] - 1s 85ms/step - loss: 0.6870 - auc: 0.8836\n",
      "Epoch 76/100\n",
      "15/15 [==============================] - 1s 90ms/step - loss: 0.6771 - auc: 0.8864\n",
      "Epoch 77/100\n",
      "15/15 [==============================] - 1s 82ms/step - loss: 0.7090 - auc: 0.8767\n",
      "Epoch 78/100\n",
      "15/15 [==============================] - 1s 98ms/step - loss: 0.6885 - auc: 0.8817\n",
      "Epoch 79/100\n",
      "15/15 [==============================] - 1s 83ms/step - loss: 0.6858 - auc: 0.8819\n",
      "Epoch 80/100\n",
      "15/15 [==============================] - 1s 80ms/step - loss: 0.7048 - auc: 0.8800\n",
      "Epoch 81/100\n",
      "15/15 [==============================] - 1s 92ms/step - loss: 0.7088 - auc: 0.8747\n",
      "Epoch 82/100\n",
      "15/15 [==============================] - 1s 94ms/step - loss: 0.6860 - auc: 0.8823\n",
      "Epoch 83/100\n",
      "15/15 [==============================] - 1s 96ms/step - loss: 0.6813 - auc: 0.8831\n",
      "Epoch 84/100\n",
      "15/15 [==============================] - 1s 85ms/step - loss: 0.7313 - auc: 0.8683\n",
      "Epoch 85/100\n",
      "15/15 [==============================] - 1s 94ms/step - loss: 0.6441 - auc: 0.8973\n",
      "Epoch 86/100\n",
      "15/15 [==============================] - 1s 83ms/step - loss: 0.6966 - auc: 0.8809\n",
      "Epoch 87/100\n",
      "15/15 [==============================] - 1s 79ms/step - loss: 0.6791 - auc: 0.8861\n",
      "Epoch 88/100\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 0.6692 - auc: 0.8907\n",
      "Epoch 89/100\n",
      "15/15 [==============================] - 1s 95ms/step - loss: 0.6828 - auc: 0.8844\n",
      "Epoch 90/100\n",
      "15/15 [==============================] - 1s 48ms/step - loss: 0.7005 - auc: 0.8793\n",
      "Epoch 91/100\n",
      "15/15 [==============================] - 1s 57ms/step - loss: 0.6900 - auc: 0.8847\n",
      "Epoch 92/100\n",
      "15/15 [==============================] - 1s 47ms/step - loss: 0.7099 - auc: 0.8783\n",
      "Epoch 93/100\n",
      "15/15 [==============================] - 1s 48ms/step - loss: 0.7819 - auc: 0.8449\n",
      "Epoch 94/100\n",
      "15/15 [==============================] - 1s 57ms/step - loss: 0.6786 - auc: 0.8850\n",
      "Epoch 95/100\n",
      "15/15 [==============================] - 1s 92ms/step - loss: 0.7481 - auc: 0.8631\n",
      "Epoch 96/100\n",
      "15/15 [==============================] - 1s 81ms/step - loss: 0.7412 - auc: 0.8607\n",
      "Epoch 97/100\n",
      "15/15 [==============================] - 1s 100ms/step - loss: 0.7103 - auc: 0.8740\n",
      "Epoch 98/100\n",
      "15/15 [==============================] - 1s 84ms/step - loss: 0.6898 - auc: 0.8831\n",
      "Epoch 99/100\n",
      "15/15 [==============================] - 1s 85ms/step - loss: 0.6533 - auc: 0.8968\n",
      "Epoch 100/100\n",
      "15/15 [==============================] - 1s 98ms/step - loss: 0.6635 - auc: 0.8906\n",
      "Epoch 1/100\n",
      "15/15 [==============================] - 10s 94ms/step - loss: 2.2402 - auc: 0.5690\n",
      "Epoch 2/100\n",
      "15/15 [==============================] - 1s 90ms/step - loss: 1.7133 - auc: 0.6242\n",
      "Epoch 3/100\n",
      "15/15 [==============================] - 1s 92ms/step - loss: 1.5984 - auc: 0.6546\n",
      "Epoch 4/100\n",
      "15/15 [==============================] - 2s 115ms/step - loss: 1.5189 - auc: 0.6911\n",
      "Epoch 5/100\n",
      "15/15 [==============================] - 1s 81ms/step - loss: 1.5056 - auc: 0.6911\n",
      "Epoch 6/100\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 1.4846 - auc: 0.7004\n",
      "Epoch 7/100\n",
      "15/15 [==============================] - 1s 89ms/step - loss: 1.5143 - auc: 0.6833\n",
      "Epoch 8/100\n",
      "15/15 [==============================] - 1s 81ms/step - loss: 1.4911 - auc: 0.6889\n",
      "Epoch 9/100\n",
      "15/15 [==============================] - 1s 82ms/step - loss: 1.4488 - auc: 0.7051\n",
      "Epoch 10/100\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 1.4550 - auc: 0.6947\n",
      "Epoch 11/100\n",
      "15/15 [==============================] - 1s 82ms/step - loss: 1.4274 - auc: 0.7059\n",
      "Epoch 12/100\n",
      "15/15 [==============================] - 1s 100ms/step - loss: 1.4512 - auc: 0.6837\n",
      "Epoch 13/100\n",
      "15/15 [==============================] - 2s 98ms/step - loss: 1.4207 - auc: 0.6996\n",
      "Epoch 14/100\n",
      "15/15 [==============================] - 1s 97ms/step - loss: 1.4022 - auc: 0.6933\n",
      "Epoch 15/100\n",
      "15/15 [==============================] - 1s 94ms/step - loss: 1.3769 - auc: 0.7172\n",
      "Epoch 16/100\n",
      "15/15 [==============================] - 2s 101ms/step - loss: 1.3473 - auc: 0.7239\n",
      "Epoch 17/100\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 1.3691 - auc: 0.7124\n",
      "Epoch 18/100\n",
      "15/15 [==============================] - 1s 89ms/step - loss: 1.4035 - auc: 0.6781\n",
      "Epoch 19/100\n",
      "15/15 [==============================] - 1s 93ms/step - loss: 1.3693 - auc: 0.6976\n",
      "Epoch 20/100\n",
      "15/15 [==============================] - 1s 93ms/step - loss: 1.2836 - auc: 0.7400\n",
      "Epoch 21/100\n",
      "15/15 [==============================] - 2s 107ms/step - loss: 1.3251 - auc: 0.7126\n",
      "Epoch 22/100\n",
      "15/15 [==============================] - 2s 114ms/step - loss: 1.2844 - auc: 0.7343\n",
      "Epoch 23/100\n",
      "15/15 [==============================] - 1s 95ms/step - loss: 1.2889 - auc: 0.7214\n",
      "Epoch 24/100\n",
      "15/15 [==============================] - 1s 81ms/step - loss: 1.2705 - auc: 0.7363\n",
      "Epoch 25/100\n",
      "15/15 [==============================] - 1s 89ms/step - loss: 1.2759 - auc: 0.7232\n",
      "Epoch 26/100\n",
      "15/15 [==============================] - 1s 77ms/step - loss: 1.2559 - auc: 0.7342\n",
      "Epoch 27/100\n",
      "15/15 [==============================] - 2s 112ms/step - loss: 1.2218 - auc: 0.7471\n",
      "Epoch 28/100\n",
      "15/15 [==============================] - 1s 95ms/step - loss: 1.2807 - auc: 0.7115\n",
      "Epoch 29/100\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 1.2176 - auc: 0.7387\n",
      "Epoch 30/100\n",
      "15/15 [==============================] - 1s 88ms/step - loss: 1.2293 - auc: 0.7280\n",
      "Epoch 31/100\n",
      "15/15 [==============================] - 1s 77ms/step - loss: 1.1934 - auc: 0.7399\n",
      "Epoch 32/100\n",
      "15/15 [==============================] - 1s 89ms/step - loss: 1.1873 - auc: 0.7447\n",
      "Epoch 33/100\n",
      "15/15 [==============================] - 1s 83ms/step - loss: 1.1690 - auc: 0.7512\n",
      "Epoch 34/100\n",
      "15/15 [==============================] - 2s 108ms/step - loss: 1.1497 - auc: 0.7670\n",
      "Epoch 35/100\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 1.1678 - auc: 0.7462\n",
      "Epoch 36/100\n",
      "15/15 [==============================] - 1s 87ms/step - loss: 1.1354 - auc: 0.7611\n",
      "Epoch 37/100\n",
      "15/15 [==============================] - 1s 76ms/step - loss: 1.1520 - auc: 0.7508\n",
      "Epoch 38/100\n",
      "15/15 [==============================] - 1s 83ms/step - loss: 1.1565 - auc: 0.7464\n",
      "Epoch 39/100\n",
      "15/15 [==============================] - 1s 83ms/step - loss: 1.1140 - auc: 0.7695\n",
      "Epoch 40/100\n",
      "15/15 [==============================] - 1s 94ms/step - loss: 1.0980 - auc: 0.7744\n",
      "Epoch 41/100\n",
      "15/15 [==============================] - 2s 108ms/step - loss: 1.1527 - auc: 0.7417\n",
      "Epoch 42/100\n",
      "15/15 [==============================] - 1s 84ms/step - loss: 1.1656 - auc: 0.7162\n",
      "Epoch 43/100\n",
      "15/15 [==============================] - 1s 93ms/step - loss: 1.1320 - auc: 0.7334\n",
      "Epoch 44/100\n",
      "15/15 [==============================] - 1s 77ms/step - loss: 1.1515 - auc: 0.7286\n",
      "Epoch 45/100\n",
      "15/15 [==============================] - 1s 91ms/step - loss: 1.1085 - auc: 0.7519\n",
      "Epoch 46/100\n",
      "15/15 [==============================] - 2s 107ms/step - loss: 1.0772 - auc: 0.7700\n",
      "Epoch 47/100\n",
      "15/15 [==============================] - 1s 91ms/step - loss: 1.0508 - auc: 0.7809\n",
      "Epoch 48/100\n",
      "15/15 [==============================] - 1s 93ms/step - loss: 1.0158 - auc: 0.8001\n",
      "Epoch 49/100\n",
      "15/15 [==============================] - 1s 91ms/step - loss: 1.0347 - auc: 0.7893\n",
      "Epoch 50/100\n",
      "15/15 [==============================] - 1s 94ms/step - loss: 1.0661 - auc: 0.7579\n",
      "Epoch 51/100\n",
      "15/15 [==============================] - 2s 106ms/step - loss: 0.9635 - auc: 0.8188\n",
      "Epoch 52/100\n",
      "15/15 [==============================] - 2s 109ms/step - loss: 1.0199 - auc: 0.7875\n",
      "Epoch 53/100\n",
      "15/15 [==============================] - 2s 112ms/step - loss: 0.9738 - auc: 0.8149\n",
      "Epoch 54/100\n",
      "15/15 [==============================] - 2s 112ms/step - loss: 1.0123 - auc: 0.7900\n",
      "Epoch 55/100\n",
      "15/15 [==============================] - 1s 97ms/step - loss: 1.0179 - auc: 0.7853\n",
      "Epoch 56/100\n",
      "15/15 [==============================] - 1s 100ms/step - loss: 0.9303 - auc: 0.8317\n",
      "Epoch 57/100\n",
      "15/15 [==============================] - 2s 101ms/step - loss: 0.9201 - auc: 0.8308\n",
      "Epoch 58/100\n",
      "15/15 [==============================] - 1s 94ms/step - loss: 0.9407 - auc: 0.8191\n",
      "Epoch 59/100\n",
      "15/15 [==============================] - 2s 107ms/step - loss: 0.8657 - auc: 0.8549\n",
      "Epoch 60/100\n",
      "15/15 [==============================] - 2s 106ms/step - loss: 0.9328 - auc: 0.8243\n",
      "Epoch 61/100\n",
      "15/15 [==============================] - 2s 107ms/step - loss: 1.0063 - auc: 0.7858\n",
      "Epoch 62/100\n",
      "15/15 [==============================] - 2s 100ms/step - loss: 0.9958 - auc: 0.7803\n",
      "Epoch 63/100\n",
      "15/15 [==============================] - 2s 108ms/step - loss: 0.9878 - auc: 0.7952\n",
      "Epoch 64/100\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.9846 - auc: 0.7902\n",
      "Epoch 65/100\n",
      "15/15 [==============================] - 2s 108ms/step - loss: 0.9212 - auc: 0.8236\n",
      "Epoch 66/100\n",
      "15/15 [==============================] - 2s 108ms/step - loss: 0.9805 - auc: 0.7959\n",
      "Epoch 67/100\n",
      "15/15 [==============================] - 2s 109ms/step - loss: 0.8822 - auc: 0.8403\n",
      "Epoch 68/100\n",
      "15/15 [==============================] - 2s 106ms/step - loss: 0.8989 - auc: 0.8318\n",
      "Epoch 69/100\n",
      "15/15 [==============================] - 2s 113ms/step - loss: 0.8769 - auc: 0.8373\n",
      "Epoch 70/100\n",
      "15/15 [==============================] - 2s 111ms/step - loss: 0.8406 - auc: 0.8540\n",
      "Epoch 71/100\n",
      "15/15 [==============================] - 2s 108ms/step - loss: 0.8887 - auc: 0.8386\n",
      "Epoch 72/100\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 0.8348 - auc: 0.8512\n",
      "Epoch 73/100\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.8467 - auc: 0.8475\n",
      "Epoch 74/100\n",
      "15/15 [==============================] - 1s 95ms/step - loss: 0.8034 - auc: 0.8650\n",
      "Epoch 75/100\n",
      "15/15 [==============================] - 2s 110ms/step - loss: 0.7871 - auc: 0.8697\n",
      "Epoch 76/100\n",
      "15/15 [==============================] - 1s 92ms/step - loss: 0.7800 - auc: 0.8710\n",
      "Epoch 77/100\n",
      "15/15 [==============================] - 1s 96ms/step - loss: 0.7831 - auc: 0.8661\n",
      "Epoch 78/100\n",
      "15/15 [==============================] - 1s 93ms/step - loss: 0.8022 - auc: 0.8602\n",
      "Epoch 79/100\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 0.8096 - auc: 0.8578\n",
      "Epoch 80/100\n",
      "15/15 [==============================] - 1s 92ms/step - loss: 0.8042 - auc: 0.8613\n",
      "Epoch 81/100\n",
      "15/15 [==============================] - 1s 97ms/step - loss: 0.7934 - auc: 0.8588\n",
      "Epoch 82/100\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.8455 - auc: 0.8358\n",
      "Epoch 83/100\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.8267 - auc: 0.8490\n",
      "Epoch 84/100\n",
      "15/15 [==============================] - 1s 96ms/step - loss: 0.8180 - auc: 0.8523\n",
      "Epoch 85/100\n",
      "15/15 [==============================] - 2s 117ms/step - loss: 0.8206 - auc: 0.8546\n",
      "Epoch 86/100\n",
      "15/15 [==============================] - 1s 100ms/step - loss: 0.7714 - auc: 0.8706\n",
      "Epoch 87/100\n",
      "15/15 [==============================] - 1s 96ms/step - loss: 0.7620 - auc: 0.8711\n",
      "Epoch 88/100\n",
      "15/15 [==============================] - 1s 95ms/step - loss: 0.7737 - auc: 0.8691\n",
      "Epoch 89/100\n",
      "15/15 [==============================] - 2s 109ms/step - loss: 0.8137 - auc: 0.8503\n",
      "Epoch 90/100\n",
      "15/15 [==============================] - 2s 117ms/step - loss: 0.7691 - auc: 0.8664\n",
      "Epoch 91/100\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.7610 - auc: 0.8678\n",
      "Epoch 92/100\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 0.7304 - auc: 0.8809\n",
      "Epoch 93/100\n",
      "15/15 [==============================] - 1s 99ms/step - loss: 0.7714 - auc: 0.8618\n",
      "Epoch 94/100\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.7893 - auc: 0.8624\n",
      "Epoch 95/100\n",
      "15/15 [==============================] - 2s 108ms/step - loss: 0.8065 - auc: 0.8427\n",
      "Epoch 96/100\n",
      "15/15 [==============================] - 2s 106ms/step - loss: 0.7468 - auc: 0.8739\n",
      "Epoch 97/100\n",
      "15/15 [==============================] - 1s 94ms/step - loss: 0.7334 - auc: 0.8753\n",
      "Epoch 98/100\n",
      "15/15 [==============================] - 1s 95ms/step - loss: 0.7458 - auc: 0.8705\n",
      "Epoch 99/100\n",
      "15/15 [==============================] - 1s 97ms/step - loss: 0.7215 - auc: 0.8793\n",
      "Epoch 100/100\n",
      "15/15 [==============================] - 1s 98ms/step - loss: 0.7269 - auc: 0.8767\n",
      "Epoch 1/100\n",
      "30/30 [==============================] - 12s 69ms/step - loss: 37.2860 - auc: 0.5852\n",
      "Epoch 2/100\n",
      "30/30 [==============================] - 2s 83ms/step - loss: 15.3895 - auc: 0.6010\n",
      "Epoch 3/100\n",
      "30/30 [==============================] - 3s 88ms/step - loss: 6.2521 - auc: 0.6183\n",
      "Epoch 4/100\n",
      "30/30 [==============================] - 2s 68ms/step - loss: 2.9400 - auc: 0.6259\n",
      "Epoch 5/100\n",
      "30/30 [==============================] - 2s 69ms/step - loss: 1.8302 - auc: 0.6413\n",
      "Epoch 6/100\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 1.4234 - auc: 0.6404\n",
      "Epoch 7/100\n",
      "30/30 [==============================] - 2s 67ms/step - loss: 1.3067 - auc: 0.6406\n",
      "Epoch 8/100\n",
      "30/30 [==============================] - 2s 64ms/step - loss: 1.2939 - auc: 0.6315\n",
      "Epoch 9/100\n",
      "30/30 [==============================] - 2s 54ms/step - loss: 1.2649 - auc: 0.6309\n",
      "Epoch 10/100\n",
      "30/30 [==============================] - 2s 69ms/step - loss: 1.2667 - auc: 0.6254\n",
      "Epoch 11/100\n",
      "30/30 [==============================] - 2s 58ms/step - loss: 1.2093 - auc: 0.6585\n",
      "Epoch 12/100\n",
      "30/30 [==============================] - 2s 53ms/step - loss: 1.2569 - auc: 0.6497\n",
      "Epoch 13/100\n",
      "30/30 [==============================] - 2s 66ms/step - loss: 1.2305 - auc: 0.6508\n",
      "Epoch 14/100\n",
      "30/30 [==============================] - 2s 53ms/step - loss: 1.2311 - auc: 0.6363\n",
      "Epoch 15/100\n",
      "30/30 [==============================] - 2s 54ms/step - loss: 1.2013 - auc: 0.6440\n",
      "Epoch 16/100\n",
      "30/30 [==============================] - 2s 58ms/step - loss: 1.2287 - auc: 0.6233\n",
      "Epoch 17/100\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.2229 - auc: 0.6475\n",
      "Epoch 18/100\n",
      "30/30 [==============================] - 2s 54ms/step - loss: 1.2428 - auc: 0.6189\n",
      "Epoch 19/100\n",
      "30/30 [==============================] - 2s 51ms/step - loss: 1.2372 - auc: 0.6063\n",
      "Epoch 20/100\n",
      "30/30 [==============================] - 2s 67ms/step - loss: 1.2142 - auc: 0.6447\n",
      "Epoch 21/100\n",
      "30/30 [==============================] - 2s 61ms/step - loss: 1.2197 - auc: 0.6170\n",
      "Epoch 22/100\n",
      "30/30 [==============================] - 2s 64ms/step - loss: 1.1961 - auc: 0.6357\n",
      "Epoch 23/100\n",
      "30/30 [==============================] - 2s 68ms/step - loss: 1.2026 - auc: 0.6402\n",
      "Epoch 24/100\n",
      "30/30 [==============================] - 2s 66ms/step - loss: 1.2164 - auc: 0.6249\n",
      "Epoch 25/100\n",
      "30/30 [==============================] - 2s 64ms/step - loss: 1.2166 - auc: 0.6439\n",
      "Epoch 26/100\n",
      "30/30 [==============================] - 2s 66ms/step - loss: 1.2350 - auc: 0.6261\n",
      "Epoch 27/100\n",
      "30/30 [==============================] - 2s 65ms/step - loss: 1.2919 - auc: 0.6305\n",
      "Epoch 28/100\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 1.2941 - auc: 0.6116\n",
      "Epoch 29/100\n",
      "30/30 [==============================] - 2s 65ms/step - loss: 1.2453 - auc: 0.6179\n",
      "Epoch 30/100\n",
      "30/30 [==============================] - 2s 70ms/step - loss: 1.2515 - auc: 0.6266\n",
      "Epoch 31/100\n",
      "30/30 [==============================] - 2s 59ms/step - loss: 1.2233 - auc: 0.6287\n",
      "Epoch 32/100\n",
      "30/30 [==============================] - 2s 59ms/step - loss: 1.2586 - auc: 0.6339\n",
      "Epoch 33/100\n",
      "30/30 [==============================] - 2s 70ms/step - loss: 1.2562 - auc: 0.6403\n",
      "Epoch 34/100\n",
      "30/30 [==============================] - 2s 63ms/step - loss: 1.2261 - auc: 0.6439\n",
      "Epoch 35/100\n",
      "30/30 [==============================] - 2s 63ms/step - loss: 1.2076 - auc: 0.6171\n",
      "Epoch 36/100\n",
      "30/30 [==============================] - 2s 62ms/step - loss: 1.2159 - auc: 0.6338\n",
      "Epoch 37/100\n",
      "30/30 [==============================] - 2s 68ms/step - loss: 1.2273 - auc: 0.6113\n",
      "Epoch 38/100\n",
      "30/30 [==============================] - 2s 73ms/step - loss: 1.1806 - auc: 0.6488\n",
      "Epoch 39/100\n",
      "30/30 [==============================] - 2s 64ms/step - loss: 1.2142 - auc: 0.6286\n",
      "Epoch 40/100\n",
      "30/30 [==============================] - 2s 67ms/step - loss: 1.2028 - auc: 0.6240\n",
      "Epoch 41/100\n",
      "30/30 [==============================] - 2s 73ms/step - loss: 1.2381 - auc: 0.6336\n",
      "Epoch 42/100\n",
      "30/30 [==============================] - 2s 69ms/step - loss: 1.1841 - auc: 0.6560\n",
      "Epoch 43/100\n",
      "30/30 [==============================] - 2s 66ms/step - loss: 1.1647 - auc: 0.6561\n",
      "Epoch 44/100\n",
      "30/30 [==============================] - 2s 63ms/step - loss: 1.2018 - auc: 0.6630\n",
      "Epoch 45/100\n",
      "30/30 [==============================] - 2s 68ms/step - loss: 1.2165 - auc: 0.6447\n",
      "Epoch 46/100\n",
      "30/30 [==============================] - 2s 66ms/step - loss: 1.2226 - auc: 0.6043\n",
      "Epoch 47/100\n",
      "30/30 [==============================] - 2s 61ms/step - loss: 1.1844 - auc: 0.6316\n",
      "Epoch 48/100\n",
      "30/30 [==============================] - 2s 69ms/step - loss: 1.1875 - auc: 0.6325\n",
      "Epoch 49/100\n",
      "30/30 [==============================] - 2s 62ms/step - loss: 1.1803 - auc: 0.6186\n",
      "Epoch 50/100\n",
      "30/30 [==============================] - 2s 65ms/step - loss: 1.1957 - auc: 0.5997\n",
      "Epoch 51/100\n",
      "30/30 [==============================] - 2s 66ms/step - loss: 1.1649 - auc: 0.6453\n",
      "Epoch 52/100\n",
      "30/30 [==============================] - 2s 68ms/step - loss: 1.1918 - auc: 0.6145\n",
      "Epoch 53/100\n",
      "30/30 [==============================] - 2s 64ms/step - loss: 1.2383 - auc: 0.6188\n",
      "Epoch 54/100\n",
      "30/30 [==============================] - 2s 70ms/step - loss: 1.2213 - auc: 0.5947\n",
      "Epoch 55/100\n",
      "30/30 [==============================] - 2s 67ms/step - loss: 1.1772 - auc: 0.6136\n",
      "Epoch 56/100\n",
      "30/30 [==============================] - 2s 67ms/step - loss: 1.1914 - auc: 0.6143\n",
      "Epoch 57/100\n",
      "30/30 [==============================] - 2s 59ms/step - loss: 1.1670 - auc: 0.6245\n",
      "Epoch 58/100\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.1719 - auc: 0.6096\n",
      "Epoch 59/100\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.1616 - auc: 0.6233\n",
      "Epoch 60/100\n",
      "30/30 [==============================] - 2s 57ms/step - loss: 1.1757 - auc: 0.6411\n",
      "Epoch 61/100\n",
      "30/30 [==============================] - 2s 50ms/step - loss: 1.1549 - auc: 0.6446\n",
      "Epoch 62/100\n",
      "30/30 [==============================] - 2s 51ms/step - loss: 1.1402 - auc: 0.6431\n",
      "Epoch 63/100\n",
      "30/30 [==============================] - 2s 51ms/step - loss: 1.1441 - auc: 0.6330\n",
      "Epoch 64/100\n",
      "30/30 [==============================] - 2s 60ms/step - loss: 1.1438 - auc: 0.6381\n",
      "Epoch 65/100\n",
      "30/30 [==============================] - 2s 54ms/step - loss: 1.1328 - auc: 0.6587\n",
      "Epoch 66/100\n",
      "30/30 [==============================] - 2s 57ms/step - loss: 1.1959 - auc: 0.6126\n",
      "Epoch 67/100\n",
      "30/30 [==============================] - 2s 58ms/step - loss: 1.2167 - auc: 0.6113\n",
      "Epoch 68/100\n",
      "30/30 [==============================] - 2s 64ms/step - loss: 1.2044 - auc: 0.6087\n",
      "Epoch 69/100\n",
      "30/30 [==============================] - 2s 66ms/step - loss: 1.1795 - auc: 0.6003\n",
      "Epoch 70/100\n",
      "30/30 [==============================] - 2s 68ms/step - loss: 1.1569 - auc: 0.6225\n",
      "Epoch 71/100\n",
      "30/30 [==============================] - 2s 70ms/step - loss: 1.1810 - auc: 0.6203\n",
      "Epoch 72/100\n",
      "30/30 [==============================] - 2s 63ms/step - loss: 1.1650 - auc: 0.6429\n",
      "Epoch 73/100\n",
      "30/30 [==============================] - 2s 67ms/step - loss: 1.2037 - auc: 0.6012\n",
      "Epoch 74/100\n",
      "30/30 [==============================] - 2s 59ms/step - loss: 1.1573 - auc: 0.6341\n",
      "Epoch 75/100\n",
      "30/30 [==============================] - 2s 59ms/step - loss: 1.1556 - auc: 0.6121\n",
      "Epoch 76/100\n",
      "30/30 [==============================] - 2s 65ms/step - loss: 1.1454 - auc: 0.6165\n",
      "Epoch 77/100\n",
      "30/30 [==============================] - 2s 62ms/step - loss: 1.1389 - auc: 0.6368\n",
      "Epoch 78/100\n",
      "30/30 [==============================] - 2s 64ms/step - loss: 1.1689 - auc: 0.6082\n",
      "Epoch 79/100\n",
      "30/30 [==============================] - 2s 67ms/step - loss: 1.1617 - auc: 0.5951\n",
      "Epoch 80/100\n",
      "30/30 [==============================] - 2s 61ms/step - loss: 1.1512 - auc: 0.6140\n",
      "Epoch 81/100\n",
      "30/30 [==============================] - 2s 61ms/step - loss: 1.1317 - auc: 0.6368\n",
      "Epoch 82/100\n",
      "30/30 [==============================] - 2s 65ms/step - loss: 1.1476 - auc: 0.6132\n",
      "Epoch 83/100\n",
      "30/30 [==============================] - 2s 60ms/step - loss: 1.1249 - auc: 0.6380\n",
      "Epoch 84/100\n",
      "30/30 [==============================] - 2s 57ms/step - loss: 1.1309 - auc: 0.6302\n",
      "Epoch 85/100\n",
      "30/30 [==============================] - 2s 62ms/step - loss: 1.1305 - auc: 0.6162\n",
      "Epoch 86/100\n",
      "30/30 [==============================] - 2s 61ms/step - loss: 1.1532 - auc: 0.5677\n",
      "Epoch 87/100\n",
      "30/30 [==============================] - 2s 70ms/step - loss: 1.1357 - auc: 0.6185\n",
      "Epoch 88/100\n",
      "30/30 [==============================] - 2s 60ms/step - loss: 1.1420 - auc: 0.6244\n",
      "Epoch 89/100\n",
      "30/30 [==============================] - 2s 64ms/step - loss: 1.1382 - auc: 0.6351\n",
      "Epoch 90/100\n",
      "30/30 [==============================] - 2s 61ms/step - loss: 1.1476 - auc: 0.6322\n",
      "Epoch 91/100\n",
      "30/30 [==============================] - 2s 67ms/step - loss: 1.1280 - auc: 0.6388\n",
      "Epoch 92/100\n",
      "30/30 [==============================] - 2s 64ms/step - loss: 1.1516 - auc: 0.6107\n",
      "Epoch 93/100\n",
      "30/30 [==============================] - 2s 59ms/step - loss: 1.1551 - auc: 0.6247\n",
      "Epoch 94/100\n",
      "30/30 [==============================] - 2s 69ms/step - loss: 1.1374 - auc: 0.6116\n",
      "Epoch 95/100\n",
      "30/30 [==============================] - 2s 52ms/step - loss: 1.1378 - auc: 0.6407\n",
      "Epoch 96/100\n",
      "30/30 [==============================] - 2s 57ms/step - loss: 1.1484 - auc: 0.6256\n",
      "Epoch 97/100\n",
      "30/30 [==============================] - 2s 64ms/step - loss: 1.1131 - auc: 0.6687\n",
      "Epoch 98/100\n",
      "30/30 [==============================] - 2s 53ms/step - loss: 1.1365 - auc: 0.6457\n",
      "Epoch 99/100\n",
      "30/30 [==============================] - 2s 58ms/step - loss: 1.1379 - auc: 0.6163\n",
      "Epoch 100/100\n",
      "30/30 [==============================] - 2s 63ms/step - loss: 1.1683 - auc: 0.5902\n",
      "Epoch 1/100\n",
      "15/15 [==============================] - 12s 93ms/step - loss: 43.9164 - auc: 0.5871\n",
      "Epoch 2/100\n",
      "15/15 [==============================] - 1s 89ms/step - loss: 27.3612 - auc: 0.6070\n",
      "Epoch 3/100\n",
      "15/15 [==============================] - 1s 87ms/step - loss: 16.3435 - auc: 0.6459\n",
      "Epoch 4/100\n",
      "15/15 [==============================] - 1s 94ms/step - loss: 9.6473 - auc: 0.6613\n",
      "Epoch 5/100\n",
      "15/15 [==============================] - 1s 96ms/step - loss: 5.8032 - auc: 0.6475\n",
      "Epoch 6/100\n",
      "15/15 [==============================] - 1s 78ms/step - loss: 3.5955 - auc: 0.6773\n",
      "Epoch 7/100\n",
      "15/15 [==============================] - 1s 96ms/step - loss: 2.4229 - auc: 0.6763\n",
      "Epoch 8/100\n",
      "15/15 [==============================] - 1s 80ms/step - loss: 1.7859 - auc: 0.6966\n",
      "Epoch 9/100\n",
      "15/15 [==============================] - 1s 78ms/step - loss: 1.4773 - auc: 0.6875\n",
      "Epoch 10/100\n",
      "15/15 [==============================] - 1s 88ms/step - loss: 1.3469 - auc: 0.6827\n",
      "Epoch 11/100\n",
      "15/15 [==============================] - 2s 119ms/step - loss: 1.2324 - auc: 0.7036\n",
      "Epoch 12/100\n",
      "15/15 [==============================] - 2s 101ms/step - loss: 1.1966 - auc: 0.6901\n",
      "Epoch 13/100\n",
      "15/15 [==============================] - 1s 93ms/step - loss: 1.1818 - auc: 0.6871\n",
      "Epoch 14/100\n",
      "15/15 [==============================] - 1s 91ms/step - loss: 1.1725 - auc: 0.6976\n",
      "Epoch 15/100\n",
      "15/15 [==============================] - 2s 116ms/step - loss: 1.1906 - auc: 0.6941\n",
      "Epoch 16/100\n",
      "15/15 [==============================] - 1s 94ms/step - loss: 1.1711 - auc: 0.7005\n",
      "Epoch 17/100\n",
      "15/15 [==============================] - 1s 81ms/step - loss: 1.1065 - auc: 0.7322\n",
      "Epoch 18/100\n",
      "15/15 [==============================] - 1s 79ms/step - loss: 1.1923 - auc: 0.7176\n",
      "Epoch 19/100\n",
      "15/15 [==============================] - 1s 81ms/step - loss: 1.2653 - auc: 0.7336\n",
      "Epoch 20/100\n",
      "15/15 [==============================] - 1s 91ms/step - loss: 1.2934 - auc: 0.7355\n",
      "Epoch 21/100\n",
      "15/15 [==============================] - 1s 90ms/step - loss: 1.2507 - auc: 0.7535\n",
      "Epoch 22/100\n",
      "15/15 [==============================] - 1s 79ms/step - loss: 1.2297 - auc: 0.7365\n",
      "Epoch 23/100\n",
      "15/15 [==============================] - 1s 80ms/step - loss: 1.2310 - auc: 0.7298\n",
      "Epoch 24/100\n",
      "15/15 [==============================] - 1s 79ms/step - loss: 1.1269 - auc: 0.7719\n",
      "Epoch 25/100\n",
      "15/15 [==============================] - 1s 92ms/step - loss: 1.1858 - auc: 0.7484\n",
      "Epoch 26/100\n",
      "15/15 [==============================] - 1s 83ms/step - loss: 1.2207 - auc: 0.7537\n",
      "Epoch 27/100\n",
      "15/15 [==============================] - 1s 79ms/step - loss: 1.1227 - auc: 0.8083\n",
      "Epoch 28/100\n",
      "15/15 [==============================] - 1s 83ms/step - loss: 1.1451 - auc: 0.7905\n",
      "Epoch 29/100\n",
      "15/15 [==============================] - 1s 89ms/step - loss: 1.0844 - auc: 0.7933\n",
      "Epoch 30/100\n",
      "15/15 [==============================] - 1s 82ms/step - loss: 1.0455 - auc: 0.8048\n",
      "Epoch 31/100\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 1.0189 - auc: 0.8102\n",
      "Epoch 32/100\n",
      "15/15 [==============================] - 1s 86ms/step - loss: 0.9967 - auc: 0.8160\n",
      "Epoch 33/100\n",
      "15/15 [==============================] - 1s 98ms/step - loss: 0.9904 - auc: 0.8233\n",
      "Epoch 34/100\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 0.9886 - auc: 0.8282\n",
      "Epoch 35/100\n",
      "15/15 [==============================] - 1s 87ms/step - loss: 0.9630 - auc: 0.8364\n",
      "Epoch 36/100\n",
      "15/15 [==============================] - 1s 79ms/step - loss: 0.9744 - auc: 0.8232\n",
      "Epoch 37/100\n",
      "15/15 [==============================] - 1s 82ms/step - loss: 1.0361 - auc: 0.8056\n",
      "Epoch 38/100\n",
      "15/15 [==============================] - 1s 77ms/step - loss: 1.0091 - auc: 0.8332\n",
      "Epoch 39/100\n",
      "15/15 [==============================] - 1s 77ms/step - loss: 0.9436 - auc: 0.8601\n",
      "Epoch 40/100\n",
      "15/15 [==============================] - 1s 93ms/step - loss: 0.9010 - auc: 0.8623\n",
      "Epoch 41/100\n",
      "15/15 [==============================] - 2s 117ms/step - loss: 0.9444 - auc: 0.8389\n",
      "Epoch 42/100\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 0.8751 - auc: 0.8560\n",
      "Epoch 43/100\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.9372 - auc: 0.8338\n",
      "Epoch 44/100\n",
      "15/15 [==============================] - 2s 111ms/step - loss: 0.9608 - auc: 0.8131\n",
      "Epoch 45/100\n",
      "15/15 [==============================] - 1s 97ms/step - loss: 0.9986 - auc: 0.8112\n",
      "Epoch 46/100\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 0.9124 - auc: 0.8516\n",
      "Epoch 47/100\n",
      "15/15 [==============================] - 1s 101ms/step - loss: 0.9052 - auc: 0.8530\n",
      "Epoch 48/100\n",
      "15/15 [==============================] - 1s 100ms/step - loss: 0.9144 - auc: 0.8416\n",
      "Epoch 49/100\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 0.9146 - auc: 0.8365\n",
      "Epoch 50/100\n",
      "15/15 [==============================] - 2s 109ms/step - loss: 0.8909 - auc: 0.8566\n",
      "Epoch 51/100\n",
      "15/15 [==============================] - 1s 93ms/step - loss: 0.9188 - auc: 0.8381\n",
      "Epoch 52/100\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 0.8462 - auc: 0.8645\n",
      "Epoch 53/100\n",
      "15/15 [==============================] - 1s 80ms/step - loss: 0.9163 - auc: 0.8357\n",
      "Epoch 54/100\n",
      "15/15 [==============================] - 1s 96ms/step - loss: 0.9379 - auc: 0.8271\n",
      "Epoch 55/100\n",
      "15/15 [==============================] - 1s 83ms/step - loss: 0.8462 - auc: 0.8617\n",
      "Epoch 56/100\n",
      "15/15 [==============================] - 1s 92ms/step - loss: 0.8537 - auc: 0.8562\n",
      "Epoch 57/100\n",
      "15/15 [==============================] - 1s 89ms/step - loss: 0.8685 - auc: 0.8454\n",
      "Epoch 58/100\n",
      "15/15 [==============================] - 1s 83ms/step - loss: 0.9048 - auc: 0.8263\n",
      "Epoch 59/100\n",
      "15/15 [==============================] - 1s 92ms/step - loss: 0.7884 - auc: 0.8830\n",
      "Epoch 60/100\n",
      "15/15 [==============================] - 1s 91ms/step - loss: 0.8098 - auc: 0.8694\n",
      "Epoch 61/100\n",
      "15/15 [==============================] - 1s 77ms/step - loss: 0.7951 - auc: 0.8709\n",
      "Epoch 62/100\n",
      "15/15 [==============================] - 1s 87ms/step - loss: 0.7960 - auc: 0.8688\n",
      "Epoch 63/100\n",
      "15/15 [==============================] - 1s 76ms/step - loss: 0.8158 - auc: 0.8658\n",
      "Epoch 64/100\n",
      "15/15 [==============================] - 1s 79ms/step - loss: 0.7973 - auc: 0.8701\n",
      "Epoch 65/100\n",
      "15/15 [==============================] - 1s 83ms/step - loss: 0.9178 - auc: 0.8292\n",
      "Epoch 66/100\n",
      "15/15 [==============================] - 1s 92ms/step - loss: 0.8192 - auc: 0.8651\n",
      "Epoch 67/100\n",
      "15/15 [==============================] - 1s 79ms/step - loss: 0.8513 - auc: 0.8544\n",
      "Epoch 68/100\n",
      "15/15 [==============================] - 1s 94ms/step - loss: 0.7972 - auc: 0.8713\n",
      "Epoch 69/100\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.8187 - auc: 0.8557\n",
      "Epoch 70/100\n",
      "15/15 [==============================] - 2s 109ms/step - loss: 0.7353 - auc: 0.8805\n",
      "Epoch 71/100\n",
      "15/15 [==============================] - 1s 91ms/step - loss: 0.7667 - auc: 0.8687\n",
      "Epoch 72/100\n",
      "15/15 [==============================] - 2s 100ms/step - loss: 0.7732 - auc: 0.8637\n",
      "Epoch 73/100\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.8054 - auc: 0.8589\n",
      "Epoch 74/100\n",
      "15/15 [==============================] - 1s 98ms/step - loss: 0.8054 - auc: 0.8568\n",
      "Epoch 75/100\n",
      "15/15 [==============================] - 1s 82ms/step - loss: 0.7683 - auc: 0.8713\n",
      "Epoch 76/100\n",
      "15/15 [==============================] - 1s 84ms/step - loss: 0.7783 - auc: 0.8640\n",
      "Epoch 77/100\n",
      "15/15 [==============================] - 1s 94ms/step - loss: 0.7587 - auc: 0.8754\n",
      "Epoch 78/100\n",
      "15/15 [==============================] - 1s 82ms/step - loss: 0.7376 - auc: 0.8865\n",
      "Epoch 79/100\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 0.7939 - auc: 0.8609\n",
      "Epoch 80/100\n",
      "15/15 [==============================] - 2s 107ms/step - loss: 0.8658 - auc: 0.8311\n",
      "Epoch 81/100\n",
      "15/15 [==============================] - 2s 152ms/step - loss: 0.7617 - auc: 0.8764\n",
      "Epoch 82/100\n",
      "15/15 [==============================] - 2s 125ms/step - loss: 0.7949 - auc: 0.8631\n",
      "Epoch 83/100\n",
      "15/15 [==============================] - 2s 101ms/step - loss: 0.7484 - auc: 0.8729\n",
      "Epoch 84/100\n",
      "15/15 [==============================] - 2s 114ms/step - loss: 0.7451 - auc: 0.8749\n",
      "Epoch 85/100\n",
      "15/15 [==============================] - 2s 111ms/step - loss: 0.7868 - auc: 0.8595\n",
      "Epoch 86/100\n",
      "15/15 [==============================] - 2s 107ms/step - loss: 0.7981 - auc: 0.8644\n",
      "Epoch 87/100\n",
      "15/15 [==============================] - 2s 106ms/step - loss: 0.7889 - auc: 0.8708\n",
      "Epoch 88/100\n",
      "15/15 [==============================] - 1s 97ms/step - loss: 0.7774 - auc: 0.8758\n",
      "Epoch 89/100\n",
      "15/15 [==============================] - 1s 95ms/step - loss: 0.7463 - auc: 0.8811\n",
      "Epoch 90/100\n",
      "15/15 [==============================] - 2s 106ms/step - loss: 0.7094 - auc: 0.8910\n",
      "Epoch 91/100\n",
      "15/15 [==============================] - 1s 95ms/step - loss: 0.7175 - auc: 0.8840\n",
      "Epoch 92/100\n",
      "15/15 [==============================] - 1s 95ms/step - loss: 0.7153 - auc: 0.8857\n",
      "Epoch 93/100\n",
      "15/15 [==============================] - 1s 93ms/step - loss: 0.7978 - auc: 0.8567\n",
      "Epoch 94/100\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.7611 - auc: 0.8679\n",
      "Epoch 95/100\n",
      "15/15 [==============================] - 2s 106ms/step - loss: 0.7283 - auc: 0.8829\n",
      "Epoch 96/100\n",
      "15/15 [==============================] - 2s 100ms/step - loss: 0.7056 - auc: 0.8867\n",
      "Epoch 97/100\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 0.7546 - auc: 0.8682\n",
      "Epoch 98/100\n",
      "15/15 [==============================] - 1s 94ms/step - loss: 0.7403 - auc: 0.8763\n",
      "Epoch 99/100\n",
      "15/15 [==============================] - 1s 92ms/step - loss: 0.7145 - auc: 0.8898\n",
      "Epoch 100/100\n",
      "15/15 [==============================] - 2s 101ms/step - loss: 0.6763 - auc: 0.8978\n"
     ]
    }
   ],
   "source": [
    "with open('conv_classifier/best_seeds.json', 'r') as f:\n",
    "    best_seeds = json.load(f)\n",
    "\n",
    "# 21 Crear y entrenar los modelos con los hiperparámetros y semillas guardados\n",
    "models = []\n",
    "for params_str, seed in best_seeds.items():\n",
    "    params = json.loads(params_str.replace(\"'\", \"\\\"\"))\n",
    "    model = KerasClassifier(build_fn=create_model, random_state=seed, **params)\n",
    "    model.fit(X_scaled, y_one_hot)\n",
    "    # model.fit(X, y_one_hot)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "15/15 [==============================] - 11s 97ms/step - loss: 43.9164 - auc: 0.5871\n",
      "Epoch 2/100\n",
      "15/15 [==============================] - 1s 97ms/step - loss: 27.3612 - auc: 0.6070\n",
      "Epoch 3/100\n",
      "15/15 [==============================] - 1s 100ms/step - loss: 16.3435 - auc: 0.6459\n",
      "Epoch 4/100\n",
      "15/15 [==============================] - 1s 97ms/step - loss: 9.6473 - auc: 0.6613\n",
      "Epoch 5/100\n",
      "15/15 [==============================] - 1s 96ms/step - loss: 5.8032 - auc: 0.6475\n",
      "Epoch 6/100\n",
      "15/15 [==============================] - 2s 101ms/step - loss: 3.5955 - auc: 0.6773\n",
      "Epoch 7/100\n",
      "15/15 [==============================] - 1s 96ms/step - loss: 2.4229 - auc: 0.6763\n",
      "Epoch 8/100\n",
      "15/15 [==============================] - 2s 108ms/step - loss: 1.7859 - auc: 0.6966\n",
      "Epoch 9/100\n",
      "15/15 [==============================] - 1s 93ms/step - loss: 1.4773 - auc: 0.6875\n",
      "Epoch 10/100\n",
      "15/15 [==============================] - 1s 99ms/step - loss: 1.3469 - auc: 0.6827\n",
      "Epoch 11/100\n",
      "15/15 [==============================] - 1s 98ms/step - loss: 1.2324 - auc: 0.7036\n",
      "Epoch 12/100\n",
      "15/15 [==============================] - 1s 99ms/step - loss: 1.1966 - auc: 0.6901\n",
      "Epoch 13/100\n",
      "15/15 [==============================] - 2s 100ms/step - loss: 1.1818 - auc: 0.6871\n",
      "Epoch 14/100\n",
      "15/15 [==============================] - 2s 111ms/step - loss: 1.1725 - auc: 0.6976\n",
      "Epoch 15/100\n",
      "15/15 [==============================] - 1s 90ms/step - loss: 1.1906 - auc: 0.6941\n",
      "Epoch 16/100\n",
      "15/15 [==============================] - 1s 95ms/step - loss: 1.1711 - auc: 0.7005\n",
      "Epoch 17/100\n",
      "15/15 [==============================] - 1s 93ms/step - loss: 1.1065 - auc: 0.7322\n",
      "Epoch 18/100\n",
      "15/15 [==============================] - 1s 92ms/step - loss: 1.1923 - auc: 0.7176\n",
      "Epoch 19/100\n",
      "15/15 [==============================] - 1s 97ms/step - loss: 1.2653 - auc: 0.7336\n",
      "Epoch 20/100\n",
      "15/15 [==============================] - 1s 101ms/step - loss: 1.2934 - auc: 0.7355\n",
      "Epoch 21/100\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 1.2507 - auc: 0.7535\n",
      "Epoch 22/100\n",
      "15/15 [==============================] - 1s 97ms/step - loss: 1.2297 - auc: 0.7365\n",
      "Epoch 23/100\n",
      "15/15 [==============================] - 1s 96ms/step - loss: 1.2310 - auc: 0.7298\n",
      "Epoch 24/100\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 1.1269 - auc: 0.7719\n",
      "Epoch 25/100\n",
      "15/15 [==============================] - 1s 98ms/step - loss: 1.1858 - auc: 0.7484\n",
      "Epoch 26/100\n",
      "15/15 [==============================] - 1s 96ms/step - loss: 1.2207 - auc: 0.7537\n",
      "Epoch 27/100\n",
      "15/15 [==============================] - 2s 101ms/step - loss: 1.1227 - auc: 0.8083\n",
      "Epoch 28/100\n",
      "15/15 [==============================] - 1s 100ms/step - loss: 1.1451 - auc: 0.7905\n",
      "Epoch 29/100\n",
      "15/15 [==============================] - 1s 90ms/step - loss: 1.0844 - auc: 0.7933\n",
      "Epoch 30/100\n",
      "15/15 [==============================] - 1s 99ms/step - loss: 1.0455 - auc: 0.8048\n",
      "Epoch 31/100\n",
      "15/15 [==============================] - 2s 113ms/step - loss: 1.0189 - auc: 0.8102\n",
      "Epoch 32/100\n",
      "15/15 [==============================] - 1s 95ms/step - loss: 0.9967 - auc: 0.8160\n",
      "Epoch 33/100\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.9904 - auc: 0.8233\n",
      "Epoch 34/100\n",
      "15/15 [==============================] - 1s 94ms/step - loss: 0.9886 - auc: 0.8282\n",
      "Epoch 35/100\n",
      "15/15 [==============================] - 1s 94ms/step - loss: 0.9630 - auc: 0.8364\n",
      "Epoch 36/100\n",
      "15/15 [==============================] - 1s 96ms/step - loss: 0.9744 - auc: 0.8232\n",
      "Epoch 37/100\n",
      "15/15 [==============================] - 1s 96ms/step - loss: 1.0361 - auc: 0.8056\n",
      "Epoch 38/100\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 1.0091 - auc: 0.8332\n",
      "Epoch 39/100\n",
      "15/15 [==============================] - 1s 91ms/step - loss: 0.9436 - auc: 0.8601\n",
      "Epoch 40/100\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.9010 - auc: 0.8623\n",
      "Epoch 41/100\n",
      "15/15 [==============================] - 2s 117ms/step - loss: 0.9444 - auc: 0.8389\n",
      "Epoch 42/100\n",
      "15/15 [==============================] - 2s 115ms/step - loss: 0.8751 - auc: 0.8560\n",
      "Epoch 43/100\n",
      "15/15 [==============================] - 1s 97ms/step - loss: 0.9372 - auc: 0.8338\n",
      "Epoch 44/100\n",
      "15/15 [==============================] - 1s 87ms/step - loss: 0.9608 - auc: 0.8131\n",
      "Epoch 45/100\n",
      "15/15 [==============================] - 1s 95ms/step - loss: 0.9986 - auc: 0.8112\n",
      "Epoch 46/100\n",
      "15/15 [==============================] - 1s 96ms/step - loss: 0.9124 - auc: 0.8516\n",
      "Epoch 47/100\n",
      "15/15 [==============================] - 1s 87ms/step - loss: 0.9052 - auc: 0.8530\n",
      "Epoch 48/100\n",
      "15/15 [==============================] - 2s 101ms/step - loss: 0.9144 - auc: 0.8416\n",
      "Epoch 49/100\n",
      "15/15 [==============================] - 1s 93ms/step - loss: 0.9146 - auc: 0.8365\n",
      "Epoch 50/100\n",
      "15/15 [==============================] - 1s 94ms/step - loss: 0.8909 - auc: 0.8566\n",
      "Epoch 51/100\n",
      "15/15 [==============================] - 1s 85ms/step - loss: 0.9188 - auc: 0.8381\n",
      "Epoch 52/100\n",
      "15/15 [==============================] - 2s 106ms/step - loss: 0.8462 - auc: 0.8645\n",
      "Epoch 53/100\n",
      "15/15 [==============================] - 1s 97ms/step - loss: 0.9163 - auc: 0.8357\n",
      "Epoch 54/100\n",
      "15/15 [==============================] - 1s 93ms/step - loss: 0.9379 - auc: 0.8271\n",
      "Epoch 55/100\n",
      "15/15 [==============================] - 1s 92ms/step - loss: 0.8462 - auc: 0.8617\n",
      "Epoch 56/100\n",
      "15/15 [==============================] - 1s 91ms/step - loss: 0.8537 - auc: 0.8562\n",
      "Epoch 57/100\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 0.8685 - auc: 0.8454\n",
      "Epoch 58/100\n",
      "15/15 [==============================] - 2s 100ms/step - loss: 0.9048 - auc: 0.8263\n",
      "Epoch 59/100\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.7884 - auc: 0.8830\n",
      "Epoch 60/100\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.8098 - auc: 0.8694\n",
      "Epoch 61/100\n",
      "15/15 [==============================] - 1s 98ms/step - loss: 0.7951 - auc: 0.8709\n",
      "Epoch 62/100\n",
      "15/15 [==============================] - 1s 97ms/step - loss: 0.7960 - auc: 0.8688\n",
      "Epoch 63/100\n",
      "15/15 [==============================] - 2s 101ms/step - loss: 0.8158 - auc: 0.8658\n",
      "Epoch 64/100\n",
      "15/15 [==============================] - 1s 95ms/step - loss: 0.7973 - auc: 0.8701\n",
      "Epoch 65/100\n",
      "15/15 [==============================] - 1s 86ms/step - loss: 0.9178 - auc: 0.8292\n",
      "Epoch 66/100\n",
      "15/15 [==============================] - 1s 87ms/step - loss: 0.8192 - auc: 0.8651\n",
      "Epoch 67/100\n",
      "15/15 [==============================] - 1s 79ms/step - loss: 0.8513 - auc: 0.8544\n",
      "Epoch 68/100\n",
      "15/15 [==============================] - 1s 86ms/step - loss: 0.7972 - auc: 0.8713\n",
      "Epoch 69/100\n",
      "15/15 [==============================] - 1s 94ms/step - loss: 0.8187 - auc: 0.8557\n",
      "Epoch 70/100\n",
      "15/15 [==============================] - 1s 97ms/step - loss: 0.7353 - auc: 0.8805\n",
      "Epoch 71/100\n",
      "15/15 [==============================] - 1s 97ms/step - loss: 0.7667 - auc: 0.8687\n",
      "Epoch 72/100\n",
      "15/15 [==============================] - 1s 90ms/step - loss: 0.7732 - auc: 0.8637\n",
      "Epoch 73/100\n",
      "15/15 [==============================] - 1s 88ms/step - loss: 0.8054 - auc: 0.8589\n",
      "Epoch 74/100\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.8054 - auc: 0.8568\n",
      "Epoch 75/100\n",
      "15/15 [==============================] - 1s 101ms/step - loss: 0.7683 - auc: 0.8713\n",
      "Epoch 76/100\n",
      "15/15 [==============================] - 1s 94ms/step - loss: 0.7783 - auc: 0.8640\n",
      "Epoch 77/100\n",
      "15/15 [==============================] - 1s 99ms/step - loss: 0.7587 - auc: 0.8754\n",
      "Epoch 78/100\n",
      "15/15 [==============================] - 1s 95ms/step - loss: 0.7376 - auc: 0.8865\n",
      "Epoch 79/100\n",
      "15/15 [==============================] - 2s 113ms/step - loss: 0.7939 - auc: 0.8609\n",
      "Epoch 80/100\n",
      "15/15 [==============================] - 1s 92ms/step - loss: 0.8658 - auc: 0.8311\n",
      "Epoch 81/100\n",
      "15/15 [==============================] - 1s 98ms/step - loss: 0.7617 - auc: 0.8764\n",
      "Epoch 82/100\n",
      "15/15 [==============================] - 1s 98ms/step - loss: 0.7949 - auc: 0.8631\n",
      "Epoch 83/100\n",
      "15/15 [==============================] - 2s 100ms/step - loss: 0.7484 - auc: 0.8729\n",
      "Epoch 84/100\n",
      "15/15 [==============================] - 1s 95ms/step - loss: 0.7451 - auc: 0.8749\n",
      "Epoch 85/100\n",
      "15/15 [==============================] - 1s 100ms/step - loss: 0.7868 - auc: 0.8595\n",
      "Epoch 86/100\n",
      "15/15 [==============================] - 1s 99ms/step - loss: 0.7981 - auc: 0.8644\n",
      "Epoch 87/100\n",
      "15/15 [==============================] - 2s 100ms/step - loss: 0.7889 - auc: 0.8708\n",
      "Epoch 88/100\n",
      "15/15 [==============================] - 1s 101ms/step - loss: 0.7774 - auc: 0.8758\n",
      "Epoch 89/100\n",
      "15/15 [==============================] - 2s 109ms/step - loss: 0.7463 - auc: 0.8811\n",
      "Epoch 90/100\n",
      "15/15 [==============================] - 1s 91ms/step - loss: 0.7094 - auc: 0.8910\n",
      "Epoch 91/100\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.7175 - auc: 0.8840\n",
      "Epoch 92/100\n",
      "15/15 [==============================] - 1s 86ms/step - loss: 0.7153 - auc: 0.8857\n",
      "Epoch 93/100\n",
      "15/15 [==============================] - 1s 101ms/step - loss: 0.7978 - auc: 0.8567\n",
      "Epoch 94/100\n",
      "15/15 [==============================] - 2s 101ms/step - loss: 0.7611 - auc: 0.8679\n",
      "Epoch 95/100\n",
      "15/15 [==============================] - 1s 87ms/step - loss: 0.7283 - auc: 0.8829\n",
      "Epoch 96/100\n",
      "15/15 [==============================] - 1s 98ms/step - loss: 0.7056 - auc: 0.8867\n",
      "Epoch 97/100\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 0.7546 - auc: 0.8682\n",
      "Epoch 98/100\n",
      "15/15 [==============================] - 1s 87ms/step - loss: 0.7403 - auc: 0.8763\n",
      "Epoch 99/100\n",
      "15/15 [==============================] - 1s 94ms/step - loss: 0.7145 - auc: 0.8898\n",
      "Epoch 100/100\n",
      "15/15 [==============================] - 2s 100ms/step - loss: 0.6763 - auc: 0.8978\n",
      "Epoch 1/100\n",
      "15/15 [==============================] - 10s 96ms/step - loss: 6.6407 - auc: 0.5699\n",
      "Epoch 2/100\n",
      "15/15 [==============================] - 2s 107ms/step - loss: 5.6947 - auc: 0.6260\n",
      "Epoch 3/100\n",
      "15/15 [==============================] - 1s 95ms/step - loss: 5.1697 - auc: 0.6202\n",
      "Epoch 4/100\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 4.6286 - auc: 0.6391\n",
      "Epoch 5/100\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 4.1355 - auc: 0.6644\n",
      "Epoch 6/100\n",
      "15/15 [==============================] - 1s 95ms/step - loss: 3.7590 - auc: 0.6592\n",
      "Epoch 7/100\n",
      "15/15 [==============================] - 2s 107ms/step - loss: 3.3737 - auc: 0.6698\n",
      "Epoch 8/100\n",
      "15/15 [==============================] - 1s 97ms/step - loss: 3.0430 - auc: 0.6785\n",
      "Epoch 9/100\n",
      "15/15 [==============================] - 2s 111ms/step - loss: 2.7240 - auc: 0.7010\n",
      "Epoch 10/100\n",
      "15/15 [==============================] - 1s 96ms/step - loss: 2.5341 - auc: 0.6923\n",
      "Epoch 11/100\n",
      "15/15 [==============================] - 1s 95ms/step - loss: 2.3254 - auc: 0.6874\n",
      "Epoch 12/100\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 2.1219 - auc: 0.7083\n",
      "Epoch 13/100\n",
      "15/15 [==============================] - 1s 85ms/step - loss: 2.0223 - auc: 0.6850\n",
      "Epoch 14/100\n",
      "15/15 [==============================] - 1s 72ms/step - loss: 1.8457 - auc: 0.7054\n",
      "Epoch 15/100\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 1.7663 - auc: 0.6915\n",
      "Epoch 16/100\n",
      "15/15 [==============================] - 1s 64ms/step - loss: 1.6908 - auc: 0.6930\n",
      "Epoch 17/100\n",
      "15/15 [==============================] - 1s 92ms/step - loss: 1.5773 - auc: 0.7004\n",
      "Epoch 18/100\n",
      "15/15 [==============================] - 1s 71ms/step - loss: 1.4995 - auc: 0.7165\n",
      "Epoch 19/100\n",
      "15/15 [==============================] - 1s 72ms/step - loss: 1.4558 - auc: 0.7090\n",
      "Epoch 20/100\n",
      "15/15 [==============================] - 1s 61ms/step - loss: 1.3478 - auc: 0.7331\n",
      "Epoch 21/100\n",
      "15/15 [==============================] - 1s 59ms/step - loss: 1.3583 - auc: 0.7079\n",
      "Epoch 22/100\n",
      "15/15 [==============================] - 1s 59ms/step - loss: 1.3217 - auc: 0.7057\n",
      "Epoch 23/100\n",
      "15/15 [==============================] - 1s 60ms/step - loss: 1.2551 - auc: 0.7333\n",
      "Epoch 24/100\n",
      "15/15 [==============================] - 1s 83ms/step - loss: 1.2856 - auc: 0.6932\n",
      "Epoch 25/100\n",
      "15/15 [==============================] - 1s 62ms/step - loss: 1.1982 - auc: 0.7340\n",
      "Epoch 26/100\n",
      "15/15 [==============================] - 1s 76ms/step - loss: 1.2145 - auc: 0.7148\n",
      "Epoch 27/100\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 1.1160 - auc: 0.7495\n",
      "Epoch 28/100\n",
      "15/15 [==============================] - 1s 72ms/step - loss: 1.1216 - auc: 0.7465\n",
      "Epoch 29/100\n",
      "15/15 [==============================] - 1s 65ms/step - loss: 1.0967 - auc: 0.7502\n",
      "Epoch 30/100\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 1.1257 - auc: 0.7277\n",
      "Epoch 31/100\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 1.1074 - auc: 0.7260\n",
      "Epoch 32/100\n",
      "15/15 [==============================] - 1s 58ms/step - loss: 1.0902 - auc: 0.7335\n",
      "Epoch 33/100\n",
      "15/15 [==============================] - 1s 57ms/step - loss: 1.0459 - auc: 0.7547\n",
      "Epoch 34/100\n",
      "15/15 [==============================] - 1s 54ms/step - loss: 1.0619 - auc: 0.7451\n",
      "Epoch 35/100\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 1.0750 - auc: 0.7378\n",
      "Epoch 36/100\n",
      "15/15 [==============================] - 1s 53ms/step - loss: 1.0420 - auc: 0.7587\n",
      "Epoch 37/100\n",
      "15/15 [==============================] - 1s 55ms/step - loss: 1.0513 - auc: 0.7497\n",
      "Epoch 38/100\n",
      "15/15 [==============================] - 1s 58ms/step - loss: 1.0666 - auc: 0.7334\n",
      "Epoch 39/100\n",
      "15/15 [==============================] - 1s 69ms/step - loss: 1.0370 - auc: 0.7480\n",
      "Epoch 40/100\n",
      "15/15 [==============================] - 1s 61ms/step - loss: 0.9851 - auc: 0.7828\n",
      "Epoch 41/100\n",
      "15/15 [==============================] - 1s 59ms/step - loss: 0.9613 - auc: 0.7937\n",
      "Epoch 42/100\n",
      "15/15 [==============================] - 1s 60ms/step - loss: 0.9445 - auc: 0.7969\n",
      "Epoch 43/100\n",
      "15/15 [==============================] - 1s 71ms/step - loss: 0.9699 - auc: 0.7840\n",
      "Epoch 44/100\n",
      "15/15 [==============================] - 1s 71ms/step - loss: 0.9112 - auc: 0.8091\n",
      "Epoch 45/100\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 0.9229 - auc: 0.8043\n",
      "Epoch 46/100\n",
      "15/15 [==============================] - 1s 89ms/step - loss: 0.8928 - auc: 0.8223\n",
      "Epoch 47/100\n",
      "15/15 [==============================] - 1s 77ms/step - loss: 0.8718 - auc: 0.8240\n",
      "Epoch 48/100\n",
      "15/15 [==============================] - 1s 59ms/step - loss: 0.8493 - auc: 0.8339\n",
      "Epoch 49/100\n",
      "15/15 [==============================] - 1s 64ms/step - loss: 0.8663 - auc: 0.8252\n",
      "Epoch 50/100\n",
      "15/15 [==============================] - 1s 83ms/step - loss: 0.8968 - auc: 0.8044\n",
      "Epoch 51/100\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 0.8874 - auc: 0.8101\n",
      "Epoch 52/100\n",
      "15/15 [==============================] - 1s 65ms/step - loss: 0.9138 - auc: 0.8019\n",
      "Epoch 53/100\n",
      "15/15 [==============================] - 1s 59ms/step - loss: 0.8302 - auc: 0.8421\n",
      "Epoch 54/100\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.8037 - auc: 0.8544\n",
      "Epoch 55/100\n",
      "15/15 [==============================] - 1s 63ms/step - loss: 0.7731 - auc: 0.8622\n",
      "Epoch 56/100\n",
      "15/15 [==============================] - 1s 55ms/step - loss: 0.7595 - auc: 0.8638\n",
      "Epoch 57/100\n",
      "15/15 [==============================] - 1s 61ms/step - loss: 0.7992 - auc: 0.8535\n",
      "Epoch 58/100\n",
      "15/15 [==============================] - 1s 62ms/step - loss: 0.7651 - auc: 0.8590\n",
      "Epoch 59/100\n",
      "15/15 [==============================] - 1s 56ms/step - loss: 0.7877 - auc: 0.8512\n",
      "Epoch 60/100\n",
      "15/15 [==============================] - 1s 55ms/step - loss: 0.7537 - auc: 0.8650\n",
      "Epoch 61/100\n",
      "15/15 [==============================] - 1s 52ms/step - loss: 0.8193 - auc: 0.8382\n",
      "Epoch 62/100\n",
      "15/15 [==============================] - 1s 81ms/step - loss: 0.7407 - auc: 0.8692\n",
      "Epoch 63/100\n",
      "15/15 [==============================] - 1s 88ms/step - loss: 0.6986 - auc: 0.8855\n",
      "Epoch 64/100\n",
      "15/15 [==============================] - 1s 89ms/step - loss: 0.7740 - auc: 0.8579\n",
      "Epoch 65/100\n",
      "15/15 [==============================] - 1s 79ms/step - loss: 0.7699 - auc: 0.8592\n",
      "Epoch 66/100\n",
      "15/15 [==============================] - 1s 86ms/step - loss: 0.7320 - auc: 0.8704\n",
      "Epoch 67/100\n",
      "15/15 [==============================] - 2s 124ms/step - loss: 0.7167 - auc: 0.8751\n",
      "Epoch 68/100\n",
      "15/15 [==============================] - 1s 93ms/step - loss: 0.7015 - auc: 0.8843\n",
      "Epoch 69/100\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.7619 - auc: 0.8594\n",
      "Epoch 70/100\n",
      "15/15 [==============================] - 1s 93ms/step - loss: 0.7761 - auc: 0.8534\n",
      "Epoch 71/100\n",
      "15/15 [==============================] - 1s 96ms/step - loss: 0.7738 - auc: 0.8538\n",
      "Epoch 72/100\n",
      "15/15 [==============================] - 1s 93ms/step - loss: 0.7529 - auc: 0.8623\n",
      "Epoch 73/100\n",
      "15/15 [==============================] - 2s 108ms/step - loss: 0.7047 - auc: 0.8805\n",
      "Epoch 74/100\n",
      "15/15 [==============================] - 1s 92ms/step - loss: 0.6711 - auc: 0.8916\n",
      "Epoch 75/100\n",
      "15/15 [==============================] - 2s 143ms/step - loss: 0.6870 - auc: 0.8836\n",
      "Epoch 76/100\n",
      "15/15 [==============================] - 1s 97ms/step - loss: 0.6771 - auc: 0.8864\n",
      "Epoch 77/100\n",
      "15/15 [==============================] - 1s 97ms/step - loss: 0.7090 - auc: 0.8767\n",
      "Epoch 78/100\n",
      "15/15 [==============================] - 1s 91ms/step - loss: 0.6885 - auc: 0.8817\n",
      "Epoch 79/100\n",
      "15/15 [==============================] - 1s 75ms/step - loss: 0.6858 - auc: 0.8819\n",
      "Epoch 80/100\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.7048 - auc: 0.8800\n",
      "Epoch 81/100\n",
      "15/15 [==============================] - 1s 80ms/step - loss: 0.7088 - auc: 0.8747\n",
      "Epoch 82/100\n",
      "15/15 [==============================] - 1s 57ms/step - loss: 0.6860 - auc: 0.8823\n",
      "Epoch 83/100\n",
      "15/15 [==============================] - 1s 96ms/step - loss: 0.6813 - auc: 0.8831\n",
      "Epoch 84/100\n",
      "15/15 [==============================] - 1s 90ms/step - loss: 0.7313 - auc: 0.8683\n",
      "Epoch 85/100\n",
      "15/15 [==============================] - 2s 111ms/step - loss: 0.6441 - auc: 0.8973\n",
      "Epoch 86/100\n",
      "15/15 [==============================] - 2s 108ms/step - loss: 0.6966 - auc: 0.8809\n",
      "Epoch 87/100\n",
      "15/15 [==============================] - 1s 92ms/step - loss: 0.6791 - auc: 0.8861\n",
      "Epoch 88/100\n",
      "15/15 [==============================] - 2s 144ms/step - loss: 0.6692 - auc: 0.8907\n",
      "Epoch 89/100\n",
      "15/15 [==============================] - 2s 120ms/step - loss: 0.6828 - auc: 0.8844\n",
      "Epoch 90/100\n",
      "15/15 [==============================] - 2s 145ms/step - loss: 0.7005 - auc: 0.8793\n",
      "Epoch 91/100\n",
      "15/15 [==============================] - 2s 106ms/step - loss: 0.6900 - auc: 0.8847\n",
      "Epoch 92/100\n",
      "15/15 [==============================] - 2s 139ms/step - loss: 0.7099 - auc: 0.8783\n",
      "Epoch 93/100\n",
      "15/15 [==============================] - 2s 162ms/step - loss: 0.7819 - auc: 0.8449\n",
      "Epoch 94/100\n",
      "15/15 [==============================] - 2s 127ms/step - loss: 0.6786 - auc: 0.8850\n",
      "Epoch 95/100\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.7481 - auc: 0.8631\n",
      "Epoch 96/100\n",
      "15/15 [==============================] - 1s 91ms/step - loss: 0.7412 - auc: 0.8607\n",
      "Epoch 97/100\n",
      "15/15 [==============================] - 1s 85ms/step - loss: 0.7103 - auc: 0.8740\n",
      "Epoch 98/100\n",
      "15/15 [==============================] - 1s 85ms/step - loss: 0.6898 - auc: 0.8831\n",
      "Epoch 99/100\n",
      "15/15 [==============================] - 1s 92ms/step - loss: 0.6533 - auc: 0.8968\n",
      "Epoch 100/100\n",
      "15/15 [==============================] - 1s 96ms/step - loss: 0.6635 - auc: 0.8906\n",
      "Epoch 1/100\n",
      "15/15 [==============================] - 11s 87ms/step - loss: 2.2402 - auc: 0.5690\n",
      "Epoch 2/100\n",
      "15/15 [==============================] - 1s 97ms/step - loss: 1.7133 - auc: 0.6242\n",
      "Epoch 3/100\n",
      "15/15 [==============================] - 1s 93ms/step - loss: 1.5984 - auc: 0.6546\n",
      "Epoch 4/100\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 1.5189 - auc: 0.6911\n",
      "Epoch 5/100\n",
      "15/15 [==============================] - 1s 92ms/step - loss: 1.5056 - auc: 0.6911\n",
      "Epoch 6/100\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 1.4846 - auc: 0.7004\n",
      "Epoch 7/100\n",
      "15/15 [==============================] - 1s 95ms/step - loss: 1.5143 - auc: 0.6833\n",
      "Epoch 8/100\n",
      "15/15 [==============================] - 1s 91ms/step - loss: 1.4911 - auc: 0.6889\n",
      "Epoch 9/100\n",
      "15/15 [==============================] - 1s 84ms/step - loss: 1.4488 - auc: 0.7051\n",
      "Epoch 10/100\n",
      "15/15 [==============================] - 1s 94ms/step - loss: 1.4550 - auc: 0.6947\n",
      "Epoch 11/100\n",
      "15/15 [==============================] - 1s 86ms/step - loss: 1.4274 - auc: 0.7059\n",
      "Epoch 12/100\n",
      "15/15 [==============================] - 1s 88ms/step - loss: 1.4512 - auc: 0.6837\n",
      "Epoch 13/100\n",
      "15/15 [==============================] - 2s 106ms/step - loss: 1.4207 - auc: 0.6996\n",
      "Epoch 14/100\n",
      "15/15 [==============================] - 2s 115ms/step - loss: 1.4022 - auc: 0.6933\n",
      "Epoch 15/100\n",
      "15/15 [==============================] - 2s 135ms/step - loss: 1.3769 - auc: 0.7172\n",
      "Epoch 16/100\n",
      "15/15 [==============================] - 1s 96ms/step - loss: 1.3473 - auc: 0.7239\n",
      "Epoch 17/100\n",
      "15/15 [==============================] - 2s 109ms/step - loss: 1.3691 - auc: 0.7124\n",
      "Epoch 18/100\n",
      "15/15 [==============================] - 2s 101ms/step - loss: 1.4035 - auc: 0.6781\n",
      "Epoch 19/100\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 1.3693 - auc: 0.6976\n",
      "Epoch 20/100\n",
      "15/15 [==============================] - 2s 110ms/step - loss: 1.2836 - auc: 0.7400\n",
      "Epoch 21/100\n",
      "15/15 [==============================] - 2s 111ms/step - loss: 1.3251 - auc: 0.7126\n",
      "Epoch 22/100\n",
      "15/15 [==============================] - 2s 110ms/step - loss: 1.2844 - auc: 0.7343\n",
      "Epoch 23/100\n",
      "15/15 [==============================] - 2s 128ms/step - loss: 1.2889 - auc: 0.7214\n",
      "Epoch 24/100\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 1.2705 - auc: 0.7363\n",
      "Epoch 25/100\n",
      "15/15 [==============================] - 1s 89ms/step - loss: 1.2759 - auc: 0.7232\n",
      "Epoch 26/100\n",
      "15/15 [==============================] - 1s 99ms/step - loss: 1.2559 - auc: 0.7342\n",
      "Epoch 27/100\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 1.2218 - auc: 0.7471\n",
      "Epoch 28/100\n",
      "15/15 [==============================] - 2s 107ms/step - loss: 1.2807 - auc: 0.7115\n",
      "Epoch 29/100\n",
      "15/15 [==============================] - 2s 106ms/step - loss: 1.2176 - auc: 0.7387\n",
      "Epoch 30/100\n",
      "15/15 [==============================] - 2s 117ms/step - loss: 1.2293 - auc: 0.7280\n",
      "Epoch 31/100\n",
      "15/15 [==============================] - 2s 133ms/step - loss: 1.1934 - auc: 0.7399\n",
      "Epoch 32/100\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 1.1873 - auc: 0.7447\n",
      "Epoch 33/100\n",
      "15/15 [==============================] - 2s 115ms/step - loss: 1.1690 - auc: 0.7512\n",
      "Epoch 34/100\n",
      "15/15 [==============================] - 1s 90ms/step - loss: 1.1497 - auc: 0.7670\n",
      "Epoch 35/100\n",
      "15/15 [==============================] - 2s 108ms/step - loss: 1.1678 - auc: 0.7462\n",
      "Epoch 36/100\n",
      "15/15 [==============================] - 1s 88ms/step - loss: 1.1354 - auc: 0.7611\n",
      "Epoch 37/100\n",
      "15/15 [==============================] - 1s 101ms/step - loss: 1.1520 - auc: 0.7508\n",
      "Epoch 38/100\n",
      "15/15 [==============================] - 1s 87ms/step - loss: 1.1565 - auc: 0.7464\n",
      "Epoch 39/100\n",
      "15/15 [==============================] - 2s 133ms/step - loss: 1.1140 - auc: 0.7695\n",
      "Epoch 40/100\n",
      "15/15 [==============================] - 2s 130ms/step - loss: 1.0980 - auc: 0.7744\n",
      "Epoch 41/100\n",
      "15/15 [==============================] - 1s 98ms/step - loss: 1.1527 - auc: 0.7417\n",
      "Epoch 42/100\n",
      "15/15 [==============================] - 1s 95ms/step - loss: 1.1656 - auc: 0.7162\n",
      "Epoch 43/100\n",
      "15/15 [==============================] - 2s 112ms/step - loss: 1.1320 - auc: 0.7334\n",
      "Epoch 44/100\n",
      "15/15 [==============================] - 2s 108ms/step - loss: 1.1515 - auc: 0.7286\n",
      "Epoch 45/100\n",
      "15/15 [==============================] - 2s 133ms/step - loss: 1.1085 - auc: 0.7519\n",
      "Epoch 46/100\n",
      "15/15 [==============================] - 2s 153ms/step - loss: 1.0772 - auc: 0.7700\n",
      "Epoch 47/100\n",
      "15/15 [==============================] - 2s 134ms/step - loss: 1.0508 - auc: 0.7809\n",
      "Epoch 48/100\n",
      "15/15 [==============================] - 2s 99ms/step - loss: 1.0158 - auc: 0.8001\n",
      "Epoch 49/100\n",
      "15/15 [==============================] - 2s 124ms/step - loss: 1.0347 - auc: 0.7893\n",
      "Epoch 50/100\n",
      "15/15 [==============================] - 2s 141ms/step - loss: 1.0661 - auc: 0.7579\n",
      "Epoch 51/100\n",
      "15/15 [==============================] - 3s 176ms/step - loss: 0.9635 - auc: 0.8188\n",
      "Epoch 52/100\n",
      "15/15 [==============================] - 2s 137ms/step - loss: 1.0199 - auc: 0.7875\n",
      "Epoch 53/100\n",
      "15/15 [==============================] - 2s 161ms/step - loss: 0.9738 - auc: 0.8149\n",
      "Epoch 54/100\n",
      "15/15 [==============================] - 2s 123ms/step - loss: 1.0123 - auc: 0.7900\n",
      "Epoch 55/100\n",
      "15/15 [==============================] - 2s 151ms/step - loss: 1.0179 - auc: 0.7853\n",
      "Epoch 56/100\n",
      "15/15 [==============================] - 2s 123ms/step - loss: 0.9303 - auc: 0.8317\n",
      "Epoch 57/100\n",
      "15/15 [==============================] - 2s 109ms/step - loss: 0.9201 - auc: 0.8308\n",
      "Epoch 58/100\n",
      "15/15 [==============================] - 2s 110ms/step - loss: 0.9407 - auc: 0.8191\n",
      "Epoch 59/100\n",
      "15/15 [==============================] - 1s 94ms/step - loss: 0.8657 - auc: 0.8549\n",
      "Epoch 60/100\n",
      "15/15 [==============================] - 1s 97ms/step - loss: 0.9328 - auc: 0.8243\n",
      "Epoch 61/100\n",
      "15/15 [==============================] - 1s 97ms/step - loss: 1.0063 - auc: 0.7858\n",
      "Epoch 62/100\n",
      "15/15 [==============================] - 2s 110ms/step - loss: 0.9958 - auc: 0.7803\n",
      "Epoch 63/100\n",
      "15/15 [==============================] - 2s 109ms/step - loss: 0.9878 - auc: 0.7952\n",
      "Epoch 64/100\n",
      "15/15 [==============================] - 2s 111ms/step - loss: 0.9846 - auc: 0.7902\n",
      "Epoch 65/100\n",
      "15/15 [==============================] - 1s 88ms/step - loss: 0.9212 - auc: 0.8236\n",
      "Epoch 66/100\n",
      "15/15 [==============================] - 1s 97ms/step - loss: 0.9805 - auc: 0.7959\n",
      "Epoch 67/100\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.8822 - auc: 0.8403\n",
      "Epoch 68/100\n",
      "15/15 [==============================] - 2s 109ms/step - loss: 0.8989 - auc: 0.8318\n",
      "Epoch 69/100\n",
      "15/15 [==============================] - 1s 84ms/step - loss: 0.8769 - auc: 0.8373\n",
      "Epoch 70/100\n",
      "15/15 [==============================] - 2s 107ms/step - loss: 0.8406 - auc: 0.8540\n",
      "Epoch 71/100\n",
      "15/15 [==============================] - 2s 138ms/step - loss: 0.8887 - auc: 0.8386\n",
      "Epoch 72/100\n",
      "15/15 [==============================] - 2s 148ms/step - loss: 0.8348 - auc: 0.8512\n",
      "Epoch 73/100\n",
      "15/15 [==============================] - 2s 137ms/step - loss: 0.8467 - auc: 0.8475\n",
      "Epoch 74/100\n",
      "15/15 [==============================] - 1s 99ms/step - loss: 0.8034 - auc: 0.8650\n",
      "Epoch 75/100\n",
      "15/15 [==============================] - 1s 85ms/step - loss: 0.7871 - auc: 0.8697\n",
      "Epoch 76/100\n",
      "15/15 [==============================] - 1s 93ms/step - loss: 0.7800 - auc: 0.8710\n",
      "Epoch 77/100\n",
      "15/15 [==============================] - 1s 90ms/step - loss: 0.7831 - auc: 0.8661\n",
      "Epoch 78/100\n",
      "15/15 [==============================] - 1s 98ms/step - loss: 0.8022 - auc: 0.8602\n",
      "Epoch 79/100\n",
      "15/15 [==============================] - 1s 85ms/step - loss: 0.8096 - auc: 0.8578\n",
      "Epoch 80/100\n",
      "15/15 [==============================] - 1s 90ms/step - loss: 0.8042 - auc: 0.8613\n",
      "Epoch 81/100\n",
      "15/15 [==============================] - 1s 83ms/step - loss: 0.7934 - auc: 0.8588\n",
      "Epoch 82/100\n",
      "15/15 [==============================] - 1s 92ms/step - loss: 0.8455 - auc: 0.8358\n",
      "Epoch 83/100\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 0.8267 - auc: 0.8490\n",
      "Epoch 84/100\n",
      "15/15 [==============================] - 1s 93ms/step - loss: 0.8180 - auc: 0.8523\n",
      "Epoch 85/100\n",
      "15/15 [==============================] - 1s 97ms/step - loss: 0.8206 - auc: 0.8546\n",
      "Epoch 86/100\n",
      "15/15 [==============================] - 2s 124ms/step - loss: 0.7714 - auc: 0.8706\n",
      "Epoch 87/100\n",
      "15/15 [==============================] - 2s 132ms/step - loss: 0.7620 - auc: 0.8711\n",
      "Epoch 88/100\n",
      "15/15 [==============================] - 2s 129ms/step - loss: 0.7737 - auc: 0.8691\n",
      "Epoch 89/100\n",
      "15/15 [==============================] - 2s 107ms/step - loss: 0.8137 - auc: 0.8503\n",
      "Epoch 90/100\n",
      "15/15 [==============================] - 1s 88ms/step - loss: 0.7691 - auc: 0.8664\n",
      "Epoch 91/100\n",
      "15/15 [==============================] - 1s 82ms/step - loss: 0.7610 - auc: 0.8678\n",
      "Epoch 92/100\n",
      "15/15 [==============================] - 1s 98ms/step - loss: 0.7304 - auc: 0.8809\n",
      "Epoch 93/100\n",
      "15/15 [==============================] - 1s 85ms/step - loss: 0.7714 - auc: 0.8618\n",
      "Epoch 94/100\n",
      "15/15 [==============================] - 1s 83ms/step - loss: 0.7893 - auc: 0.8624\n",
      "Epoch 95/100\n",
      "15/15 [==============================] - 1s 84ms/step - loss: 0.8065 - auc: 0.8427\n",
      "Epoch 96/100\n",
      "15/15 [==============================] - 1s 84ms/step - loss: 0.7468 - auc: 0.8739\n",
      "Epoch 97/100\n",
      "15/15 [==============================] - 1s 90ms/step - loss: 0.7334 - auc: 0.8753\n",
      "Epoch 98/100\n",
      "15/15 [==============================] - 2s 115ms/step - loss: 0.7458 - auc: 0.8705\n",
      "Epoch 99/100\n",
      "15/15 [==============================] - 2s 131ms/step - loss: 0.7215 - auc: 0.8793\n",
      "Epoch 100/100\n",
      "15/15 [==============================] - 1s 89ms/step - loss: 0.7269 - auc: 0.8767\n",
      "Epoch 1/100\n",
      "30/30 [==============================] - 13s 59ms/step - loss: 37.2860 - auc: 0.5852\n",
      "Epoch 2/100\n",
      "30/30 [==============================] - 2s 61ms/step - loss: 15.3895 - auc: 0.6010\n",
      "Epoch 3/100\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 6.2521 - auc: 0.6183\n",
      "Epoch 4/100\n",
      "30/30 [==============================] - 2s 58ms/step - loss: 2.9400 - auc: 0.6259\n",
      "Epoch 5/100\n",
      "30/30 [==============================] - 2s 79ms/step - loss: 1.8302 - auc: 0.6413\n",
      "Epoch 6/100\n",
      "30/30 [==============================] - 2s 51ms/step - loss: 1.4234 - auc: 0.6404\n",
      "Epoch 7/100\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.3067 - auc: 0.6406\n",
      "Epoch 8/100\n",
      "30/30 [==============================] - 2s 61ms/step - loss: 1.2939 - auc: 0.6315\n",
      "Epoch 9/100\n",
      "30/30 [==============================] - 2s 60ms/step - loss: 1.2649 - auc: 0.6309\n",
      "Epoch 10/100\n",
      "30/30 [==============================] - 2s 62ms/step - loss: 1.2667 - auc: 0.6254\n",
      "Epoch 11/100\n",
      "30/30 [==============================] - 2s 76ms/step - loss: 1.2093 - auc: 0.6585\n",
      "Epoch 12/100\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 1.2569 - auc: 0.6497\n",
      "Epoch 13/100\n",
      "30/30 [==============================] - 2s 58ms/step - loss: 1.2305 - auc: 0.6508\n",
      "Epoch 14/100\n",
      "30/30 [==============================] - 2s 57ms/step - loss: 1.2311 - auc: 0.6363\n",
      "Epoch 15/100\n",
      "30/30 [==============================] - 2s 65ms/step - loss: 1.2013 - auc: 0.6440\n",
      "Epoch 16/100\n",
      "30/30 [==============================] - 2s 58ms/step - loss: 1.2287 - auc: 0.6233\n",
      "Epoch 17/100\n",
      "30/30 [==============================] - 2s 60ms/step - loss: 1.2229 - auc: 0.6475\n",
      "Epoch 18/100\n",
      "30/30 [==============================] - 2s 64ms/step - loss: 1.2428 - auc: 0.6189\n",
      "Epoch 19/100\n",
      "30/30 [==============================] - 2s 66ms/step - loss: 1.2372 - auc: 0.6063\n",
      "Epoch 20/100\n",
      "30/30 [==============================] - 2s 59ms/step - loss: 1.2142 - auc: 0.6447\n",
      "Epoch 21/100\n",
      "30/30 [==============================] - 2s 77ms/step - loss: 1.2197 - auc: 0.6170\n",
      "Epoch 22/100\n",
      "30/30 [==============================] - 3s 82ms/step - loss: 1.1961 - auc: 0.6357\n",
      "Epoch 23/100\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 1.2026 - auc: 0.6402\n",
      "Epoch 24/100\n",
      "30/30 [==============================] - 2s 81ms/step - loss: 1.2164 - auc: 0.6249\n",
      "Epoch 25/100\n",
      "30/30 [==============================] - 2s 63ms/step - loss: 1.2166 - auc: 0.6439\n",
      "Epoch 26/100\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.2350 - auc: 0.6261\n",
      "Epoch 27/100\n",
      "30/30 [==============================] - 2s 70ms/step - loss: 1.2919 - auc: 0.6305\n",
      "Epoch 28/100\n",
      "30/30 [==============================] - 2s 80ms/step - loss: 1.2941 - auc: 0.6116\n",
      "Epoch 29/100\n",
      "30/30 [==============================] - 2s 81ms/step - loss: 1.2453 - auc: 0.6179\n",
      "Epoch 30/100\n",
      "30/30 [==============================] - 2s 64ms/step - loss: 1.2515 - auc: 0.6266\n",
      "Epoch 31/100\n",
      "30/30 [==============================] - 2s 58ms/step - loss: 1.2233 - auc: 0.6287\n",
      "Epoch 32/100\n",
      "30/30 [==============================] - 3s 89ms/step - loss: 1.2586 - auc: 0.6339\n",
      "Epoch 33/100\n",
      "30/30 [==============================] - 2s 67ms/step - loss: 1.2562 - auc: 0.6403\n",
      "Epoch 34/100\n",
      "30/30 [==============================] - 2s 55ms/step - loss: 1.2261 - auc: 0.6439\n",
      "Epoch 35/100\n",
      "30/30 [==============================] - 2s 61ms/step - loss: 1.2076 - auc: 0.6171\n",
      "Epoch 36/100\n",
      "30/30 [==============================] - 2s 61ms/step - loss: 1.2159 - auc: 0.6338\n",
      "Epoch 37/100\n",
      "30/30 [==============================] - 2s 55ms/step - loss: 1.2273 - auc: 0.6113\n",
      "Epoch 38/100\n",
      "30/30 [==============================] - 2s 59ms/step - loss: 1.1806 - auc: 0.6488\n",
      "Epoch 39/100\n",
      "30/30 [==============================] - 2s 60ms/step - loss: 1.2142 - auc: 0.6286\n",
      "Epoch 40/100\n",
      "30/30 [==============================] - 2s 62ms/step - loss: 1.2028 - auc: 0.6240\n",
      "Epoch 41/100\n",
      "30/30 [==============================] - 2s 61ms/step - loss: 1.2381 - auc: 0.6336\n",
      "Epoch 42/100\n",
      "30/30 [==============================] - 2s 57ms/step - loss: 1.1841 - auc: 0.6560\n",
      "Epoch 43/100\n",
      "30/30 [==============================] - 2s 77ms/step - loss: 1.1647 - auc: 0.6561\n",
      "Epoch 44/100\n",
      "30/30 [==============================] - 2s 61ms/step - loss: 1.2018 - auc: 0.6630\n",
      "Epoch 45/100\n",
      "30/30 [==============================] - 2s 58ms/step - loss: 1.2165 - auc: 0.6447\n",
      "Epoch 46/100\n",
      "30/30 [==============================] - 2s 64ms/step - loss: 1.2226 - auc: 0.6043\n",
      "Epoch 47/100\n",
      "30/30 [==============================] - 2s 75ms/step - loss: 1.1844 - auc: 0.6316\n",
      "Epoch 48/100\n",
      "30/30 [==============================] - 2s 74ms/step - loss: 1.1875 - auc: 0.6325\n",
      "Epoch 49/100\n",
      "30/30 [==============================] - 2s 79ms/step - loss: 1.1803 - auc: 0.6186\n",
      "Epoch 50/100\n",
      "30/30 [==============================] - 2s 76ms/step - loss: 1.1957 - auc: 0.5997\n",
      "Epoch 51/100\n",
      "30/30 [==============================] - 2s 72ms/step - loss: 1.1649 - auc: 0.6453\n",
      "Epoch 52/100\n",
      "30/30 [==============================] - 2s 80ms/step - loss: 1.1918 - auc: 0.6145\n",
      "Epoch 53/100\n",
      "30/30 [==============================] - 2s 62ms/step - loss: 1.2383 - auc: 0.6188\n",
      "Epoch 54/100\n",
      "30/30 [==============================] - 2s 81ms/step - loss: 1.2213 - auc: 0.5947\n",
      "Epoch 55/100\n",
      "30/30 [==============================] - 2s 57ms/step - loss: 1.1772 - auc: 0.6136\n",
      "Epoch 56/100\n",
      "30/30 [==============================] - 2s 59ms/step - loss: 1.1914 - auc: 0.6143\n",
      "Epoch 57/100\n",
      "30/30 [==============================] - 2s 52ms/step - loss: 1.1670 - auc: 0.6245\n",
      "Epoch 58/100\n",
      "30/30 [==============================] - 2s 52ms/step - loss: 1.1719 - auc: 0.6096\n",
      "Epoch 59/100\n",
      "30/30 [==============================] - 2s 54ms/step - loss: 1.1616 - auc: 0.6233\n",
      "Epoch 60/100\n",
      "30/30 [==============================] - 2s 61ms/step - loss: 1.1757 - auc: 0.6411\n",
      "Epoch 61/100\n",
      "30/30 [==============================] - 2s 67ms/step - loss: 1.1549 - auc: 0.6446\n",
      "Epoch 62/100\n",
      "30/30 [==============================] - 2s 63ms/step - loss: 1.1402 - auc: 0.6431\n",
      "Epoch 63/100\n",
      "30/30 [==============================] - 2s 57ms/step - loss: 1.1441 - auc: 0.6330\n",
      "Epoch 64/100\n",
      "30/30 [==============================] - 2s 83ms/step - loss: 1.1438 - auc: 0.6381\n",
      "Epoch 65/100\n",
      "30/30 [==============================] - 2s 72ms/step - loss: 1.1328 - auc: 0.6587\n",
      "Epoch 66/100\n",
      "30/30 [==============================] - 3s 100ms/step - loss: 1.1959 - auc: 0.6126\n",
      "Epoch 67/100\n",
      "30/30 [==============================] - 2s 57ms/step - loss: 1.2167 - auc: 0.6113\n",
      "Epoch 68/100\n",
      "30/30 [==============================] - 2s 61ms/step - loss: 1.2044 - auc: 0.6087\n",
      "Epoch 69/100\n",
      "30/30 [==============================] - 2s 57ms/step - loss: 1.1795 - auc: 0.6003\n",
      "Epoch 70/100\n",
      "30/30 [==============================] - 2s 55ms/step - loss: 1.1569 - auc: 0.6225\n",
      "Epoch 71/100\n",
      "30/30 [==============================] - 2s 61ms/step - loss: 1.1810 - auc: 0.6203\n",
      "Epoch 72/100\n",
      "30/30 [==============================] - 2s 54ms/step - loss: 1.1650 - auc: 0.6429\n",
      "Epoch 73/100\n",
      "30/30 [==============================] - 2s 54ms/step - loss: 1.2037 - auc: 0.6012\n",
      "Epoch 74/100\n",
      "30/30 [==============================] - 2s 61ms/step - loss: 1.1573 - auc: 0.6341\n",
      "Epoch 75/100\n",
      "30/30 [==============================] - 2s 60ms/step - loss: 1.1556 - auc: 0.6121\n",
      "Epoch 76/100\n",
      "30/30 [==============================] - 2s 60ms/step - loss: 1.1454 - auc: 0.6165\n",
      "Epoch 77/100\n",
      "30/30 [==============================] - 2s 57ms/step - loss: 1.1389 - auc: 0.6368\n",
      "Epoch 78/100\n",
      "30/30 [==============================] - 2s 61ms/step - loss: 1.1689 - auc: 0.6082\n",
      "Epoch 79/100\n",
      "30/30 [==============================] - 2s 52ms/step - loss: 1.1617 - auc: 0.5951\n",
      "Epoch 80/100\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.1512 - auc: 0.6140\n",
      "Epoch 81/100\n",
      "30/30 [==============================] - 2s 53ms/step - loss: 1.1317 - auc: 0.6368\n",
      "Epoch 82/100\n",
      "30/30 [==============================] - 2s 55ms/step - loss: 1.1476 - auc: 0.6132\n",
      "Epoch 83/100\n",
      "30/30 [==============================] - 2s 57ms/step - loss: 1.1249 - auc: 0.6380\n",
      "Epoch 84/100\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.1309 - auc: 0.6302\n",
      "Epoch 85/100\n",
      "30/30 [==============================] - 2s 55ms/step - loss: 1.1305 - auc: 0.6162\n",
      "Epoch 86/100\n",
      "30/30 [==============================] - 2s 53ms/step - loss: 1.1532 - auc: 0.5677\n",
      "Epoch 87/100\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.1357 - auc: 0.6185\n",
      "Epoch 88/100\n",
      "30/30 [==============================] - 2s 63ms/step - loss: 1.1420 - auc: 0.6244\n",
      "Epoch 89/100\n",
      "30/30 [==============================] - 2s 52ms/step - loss: 1.1382 - auc: 0.6351\n",
      "Epoch 90/100\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.1476 - auc: 0.6322\n",
      "Epoch 91/100\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.1280 - auc: 0.6388\n",
      "Epoch 92/100\n",
      "30/30 [==============================] - 2s 53ms/step - loss: 1.1516 - auc: 0.6107\n",
      "Epoch 93/100\n",
      "30/30 [==============================] - 2s 57ms/step - loss: 1.1551 - auc: 0.6247\n",
      "Epoch 94/100\n",
      "30/30 [==============================] - 2s 57ms/step - loss: 1.1374 - auc: 0.6116\n",
      "Epoch 95/100\n",
      "30/30 [==============================] - 2s 66ms/step - loss: 1.1378 - auc: 0.6407\n",
      "Epoch 96/100\n",
      "30/30 [==============================] - 2s 73ms/step - loss: 1.1484 - auc: 0.6256\n",
      "Epoch 97/100\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 1.1131 - auc: 0.6687\n",
      "Epoch 98/100\n",
      "30/30 [==============================] - 2s 66ms/step - loss: 1.1365 - auc: 0.6457\n",
      "Epoch 99/100\n",
      "30/30 [==============================] - 2s 66ms/step - loss: 1.1379 - auc: 0.6163\n",
      "Epoch 100/100\n",
      "30/30 [==============================] - 2s 67ms/step - loss: 1.1683 - auc: 0.5902\n",
      "Epoch 1/100\n",
      "15/15 [==============================] - 14s 98ms/step - loss: 43.9164 - auc: 0.5871\n",
      "Epoch 2/100\n",
      "15/15 [==============================] - 1s 89ms/step - loss: 27.3612 - auc: 0.6070\n",
      "Epoch 3/100\n",
      "15/15 [==============================] - 2s 112ms/step - loss: 16.3435 - auc: 0.6459\n",
      "Epoch 4/100\n",
      "15/15 [==============================] - 2s 111ms/step - loss: 9.6473 - auc: 0.6613\n",
      "Epoch 5/100\n",
      "15/15 [==============================] - 2s 116ms/step - loss: 5.8032 - auc: 0.6475\n",
      "Epoch 6/100\n",
      "15/15 [==============================] - 1s 96ms/step - loss: 3.5955 - auc: 0.6773\n",
      "Epoch 7/100\n",
      "15/15 [==============================] - 2s 109ms/step - loss: 2.4229 - auc: 0.6763\n",
      "Epoch 8/100\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 1.7859 - auc: 0.6966\n",
      "Epoch 9/100\n",
      "15/15 [==============================] - 1s 96ms/step - loss: 1.4773 - auc: 0.6875\n",
      "Epoch 10/100\n",
      "15/15 [==============================] - 1s 97ms/step - loss: 1.3469 - auc: 0.6827\n",
      "Epoch 11/100\n",
      "15/15 [==============================] - 1s 91ms/step - loss: 1.2324 - auc: 0.7036\n",
      "Epoch 12/100\n",
      "15/15 [==============================] - 1s 92ms/step - loss: 1.1966 - auc: 0.6901\n",
      "Epoch 13/100\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 1.1818 - auc: 0.6871\n",
      "Epoch 14/100\n",
      "15/15 [==============================] - 1s 98ms/step - loss: 1.1725 - auc: 0.6976\n",
      "Epoch 15/100\n",
      "15/15 [==============================] - 1s 101ms/step - loss: 1.1906 - auc: 0.6941\n",
      "Epoch 16/100\n",
      "15/15 [==============================] - 1s 98ms/step - loss: 1.1711 - auc: 0.7005\n",
      "Epoch 17/100\n",
      "15/15 [==============================] - 1s 94ms/step - loss: 1.1065 - auc: 0.7322\n",
      "Epoch 18/100\n",
      "15/15 [==============================] - 2s 108ms/step - loss: 1.1923 - auc: 0.7176\n",
      "Epoch 19/100\n",
      "15/15 [==============================] - 1s 97ms/step - loss: 1.2653 - auc: 0.7336\n",
      "Epoch 20/100\n",
      "15/15 [==============================] - 1s 96ms/step - loss: 1.2934 - auc: 0.7355\n",
      "Epoch 21/100\n",
      "15/15 [==============================] - 1s 92ms/step - loss: 1.2507 - auc: 0.7535\n",
      "Epoch 22/100\n",
      "15/15 [==============================] - 2s 110ms/step - loss: 1.2297 - auc: 0.7365\n",
      "Epoch 23/100\n",
      "15/15 [==============================] - 1s 100ms/step - loss: 1.2310 - auc: 0.7298\n",
      "Epoch 24/100\n",
      "15/15 [==============================] - 1s 90ms/step - loss: 1.1269 - auc: 0.7719\n",
      "Epoch 25/100\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 1.1858 - auc: 0.7484\n",
      "Epoch 26/100\n",
      "15/15 [==============================] - 2s 100ms/step - loss: 1.2207 - auc: 0.7537\n",
      "Epoch 27/100\n",
      "15/15 [==============================] - 1s 94ms/step - loss: 1.1227 - auc: 0.8083\n",
      "Epoch 28/100\n",
      "15/15 [==============================] - 1s 100ms/step - loss: 1.1451 - auc: 0.7905\n",
      "Epoch 29/100\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 1.0844 - auc: 0.7933\n",
      "Epoch 30/100\n",
      "15/15 [==============================] - 1s 94ms/step - loss: 1.0455 - auc: 0.8048\n",
      "Epoch 31/100\n",
      "15/15 [==============================] - 2s 110ms/step - loss: 1.0189 - auc: 0.8102\n",
      "Epoch 32/100\n",
      "15/15 [==============================] - 1s 98ms/step - loss: 0.9967 - auc: 0.8160\n",
      "Epoch 33/100\n",
      "15/15 [==============================] - 1s 94ms/step - loss: 0.9904 - auc: 0.8233\n",
      "Epoch 34/100\n",
      "15/15 [==============================] - 1s 95ms/step - loss: 0.9886 - auc: 0.8282\n",
      "Epoch 35/100\n",
      "15/15 [==============================] - 2s 101ms/step - loss: 0.9630 - auc: 0.8364\n",
      "Epoch 36/100\n",
      "15/15 [==============================] - 1s 94ms/step - loss: 0.9744 - auc: 0.8232\n",
      "Epoch 37/100\n",
      "15/15 [==============================] - 2s 107ms/step - loss: 1.0361 - auc: 0.8056\n",
      "Epoch 38/100\n",
      "15/15 [==============================] - 1s 99ms/step - loss: 1.0091 - auc: 0.8332\n",
      "Epoch 39/100\n",
      "15/15 [==============================] - 2s 108ms/step - loss: 0.9436 - auc: 0.8601\n",
      "Epoch 40/100\n",
      "15/15 [==============================] - 2s 109ms/step - loss: 0.9010 - auc: 0.8623\n",
      "Epoch 41/100\n",
      "15/15 [==============================] - 2s 122ms/step - loss: 0.9444 - auc: 0.8389\n",
      "Epoch 42/100\n",
      "15/15 [==============================] - 2s 130ms/step - loss: 0.8751 - auc: 0.8560\n",
      "Epoch 43/100\n",
      "15/15 [==============================] - 1s 93ms/step - loss: 0.9372 - auc: 0.8338\n",
      "Epoch 44/100\n",
      "15/15 [==============================] - 1s 93ms/step - loss: 0.9608 - auc: 0.8131\n",
      "Epoch 45/100\n",
      "15/15 [==============================] - 1s 97ms/step - loss: 0.9986 - auc: 0.8112\n",
      "Epoch 46/100\n",
      "15/15 [==============================] - 2s 130ms/step - loss: 0.9124 - auc: 0.8516\n",
      "Epoch 47/100\n",
      "15/15 [==============================] - 2s 164ms/step - loss: 0.9052 - auc: 0.8530\n",
      "Epoch 48/100\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.9144 - auc: 0.8416\n",
      "Epoch 49/100\n",
      "15/15 [==============================] - 1s 88ms/step - loss: 0.9146 - auc: 0.8365\n",
      "Epoch 50/100\n",
      "15/15 [==============================] - 2s 114ms/step - loss: 0.8909 - auc: 0.8566\n",
      "Epoch 51/100\n",
      "15/15 [==============================] - 2s 128ms/step - loss: 0.9188 - auc: 0.8381\n",
      "Epoch 52/100\n",
      "15/15 [==============================] - 2s 109ms/step - loss: 0.8462 - auc: 0.8645\n",
      "Epoch 53/100\n",
      "15/15 [==============================] - 2s 114ms/step - loss: 0.9163 - auc: 0.8357\n",
      "Epoch 54/100\n",
      "15/15 [==============================] - 2s 117ms/step - loss: 0.9379 - auc: 0.8271\n",
      "Epoch 55/100\n",
      "15/15 [==============================] - 2s 122ms/step - loss: 0.8462 - auc: 0.8617\n",
      "Epoch 56/100\n",
      "15/15 [==============================] - 2s 132ms/step - loss: 0.8537 - auc: 0.8562\n",
      "Epoch 57/100\n",
      "15/15 [==============================] - 2s 130ms/step - loss: 0.8685 - auc: 0.8454\n",
      "Epoch 58/100\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 0.9048 - auc: 0.8263\n",
      "Epoch 59/100\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.7884 - auc: 0.8830\n",
      "Epoch 60/100\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 0.8098 - auc: 0.8694\n",
      "Epoch 61/100\n",
      "15/15 [==============================] - 2s 111ms/step - loss: 0.7951 - auc: 0.8709\n",
      "Epoch 62/100\n",
      "15/15 [==============================] - 2s 118ms/step - loss: 0.7960 - auc: 0.8688\n",
      "Epoch 63/100\n",
      "15/15 [==============================] - 2s 110ms/step - loss: 0.8158 - auc: 0.8658\n",
      "Epoch 64/100\n",
      "15/15 [==============================] - 2s 113ms/step - loss: 0.7973 - auc: 0.8701\n",
      "Epoch 65/100\n",
      "15/15 [==============================] - 1s 97ms/step - loss: 0.9178 - auc: 0.8292\n",
      "Epoch 66/100\n",
      "15/15 [==============================] - 2s 132ms/step - loss: 0.8192 - auc: 0.8651\n",
      "Epoch 67/100\n",
      "15/15 [==============================] - 1s 95ms/step - loss: 0.8513 - auc: 0.8544\n",
      "Epoch 68/100\n",
      "15/15 [==============================] - 2s 101ms/step - loss: 0.7972 - auc: 0.8713\n",
      "Epoch 69/100\n",
      "15/15 [==============================] - 1s 97ms/step - loss: 0.8187 - auc: 0.8557\n",
      "Epoch 70/100\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 0.7353 - auc: 0.8805\n",
      "Epoch 71/100\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.7667 - auc: 0.8687\n",
      "Epoch 72/100\n",
      "15/15 [==============================] - 1s 91ms/step - loss: 0.7732 - auc: 0.8637\n",
      "Epoch 73/100\n",
      "15/15 [==============================] - 1s 99ms/step - loss: 0.8054 - auc: 0.8589\n",
      "Epoch 74/100\n",
      "15/15 [==============================] - 2s 106ms/step - loss: 0.8054 - auc: 0.8568\n",
      "Epoch 75/100\n",
      "15/15 [==============================] - 1s 101ms/step - loss: 0.7683 - auc: 0.8713\n",
      "Epoch 76/100\n",
      "15/15 [==============================] - 1s 96ms/step - loss: 0.7783 - auc: 0.8640\n",
      "Epoch 77/100\n",
      "15/15 [==============================] - 1s 89ms/step - loss: 0.7587 - auc: 0.8754\n",
      "Epoch 78/100\n",
      "15/15 [==============================] - 1s 86ms/step - loss: 0.7376 - auc: 0.8865\n",
      "Epoch 79/100\n",
      "15/15 [==============================] - 1s 90ms/step - loss: 0.7939 - auc: 0.8609\n",
      "Epoch 80/100\n",
      "15/15 [==============================] - 1s 91ms/step - loss: 0.8658 - auc: 0.8311\n",
      "Epoch 81/100\n",
      "15/15 [==============================] - 2s 109ms/step - loss: 0.7617 - auc: 0.8764\n",
      "Epoch 82/100\n",
      "15/15 [==============================] - 1s 99ms/step - loss: 0.7949 - auc: 0.8631\n",
      "Epoch 83/100\n",
      "15/15 [==============================] - 1s 90ms/step - loss: 0.7484 - auc: 0.8729\n",
      "Epoch 84/100\n",
      "15/15 [==============================] - 2s 106ms/step - loss: 0.7451 - auc: 0.8749\n",
      "Epoch 85/100\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.7868 - auc: 0.8595\n",
      "Epoch 86/100\n",
      "15/15 [==============================] - 1s 95ms/step - loss: 0.7981 - auc: 0.8644\n",
      "Epoch 87/100\n",
      "15/15 [==============================] - 2s 109ms/step - loss: 0.7889 - auc: 0.8708\n",
      "Epoch 88/100\n",
      "15/15 [==============================] - 1s 90ms/step - loss: 0.7774 - auc: 0.8758\n",
      "Epoch 89/100\n",
      "15/15 [==============================] - 2s 108ms/step - loss: 0.7463 - auc: 0.8811\n",
      "Epoch 90/100\n",
      "15/15 [==============================] - 1s 95ms/step - loss: 0.7094 - auc: 0.8910\n",
      "Epoch 91/100\n",
      "15/15 [==============================] - 1s 100ms/step - loss: 0.7175 - auc: 0.8840\n",
      "Epoch 92/100\n",
      "15/15 [==============================] - 2s 109ms/step - loss: 0.7153 - auc: 0.8857\n",
      "Epoch 93/100\n",
      "15/15 [==============================] - 2s 107ms/step - loss: 0.7978 - auc: 0.8567\n",
      "Epoch 94/100\n",
      "15/15 [==============================] - 3s 168ms/step - loss: 0.7611 - auc: 0.8679\n",
      "Epoch 95/100\n",
      "15/15 [==============================] - 2s 122ms/step - loss: 0.7283 - auc: 0.8829\n",
      "Epoch 96/100\n",
      "15/15 [==============================] - 2s 150ms/step - loss: 0.7056 - auc: 0.8867\n",
      "Epoch 97/100\n",
      "15/15 [==============================] - 2s 135ms/step - loss: 0.7546 - auc: 0.8682\n",
      "Epoch 98/100\n",
      "15/15 [==============================] - 2s 126ms/step - loss: 0.7403 - auc: 0.8763\n",
      "Epoch 99/100\n",
      "15/15 [==============================] - 2s 169ms/step - loss: 0.7145 - auc: 0.8898\n",
      "Epoch 100/100\n",
      "15/15 [==============================] - 2s 133ms/step - loss: 0.6763 - auc: 0.8978\n"
     ]
    }
   ],
   "source": [
    "ensemble = MultivariableVotingClassifier(models)\n",
    "ensemble.fit(X_scaled, y_one_hot)\n",
    "# ensemble.fit(X, y_one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Datos originales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open_time</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Number of trades</th>\n",
       "      <th>Close_BTCUSDT</th>\n",
       "      <th>Volume_BTCUSDT</th>\n",
       "      <th>Number_of_trades_BTCUSDT</th>\n",
       "      <th>Close_ETHUSDT</th>\n",
       "      <th>Volume_ETHUSDT</th>\n",
       "      <th>Number_of_trades_ETHUSDT</th>\n",
       "      <th>Close_BNBUSDT</th>\n",
       "      <th>Volume_BNBUSDT</th>\n",
       "      <th>Number_of_trades_BNBUSDT</th>\n",
       "      <th>SMA_20</th>\n",
       "      <th>EMA_20</th>\n",
       "      <th>Upper_Band</th>\n",
       "      <th>Middle_Band</th>\n",
       "      <th>Lower_Band</th>\n",
       "      <th>RSI</th>\n",
       "      <th>buy_1000x_high_coinbase</th>\n",
       "      <th>sell_1000x_high_coinbase</th>\n",
       "      <th>total_trades_coinbase</th>\n",
       "      <th>Tweets_Utilizados</th>\n",
       "      <th>Tweets_Utilizados_coin</th>\n",
       "      <th>Tweets_Utilizados_referentes</th>\n",
       "      <th>Tweets_Utilizados_whale_alert</th>\n",
       "      <th>Buy_1000x_high</th>\n",
       "      <th>sell_1000x_high</th>\n",
       "      <th>total_trades_binance</th>\n",
       "      <th>Tendencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>946</th>\n",
       "      <td>2024-04-30</td>\n",
       "      <td>6.59</td>\n",
       "      <td>6.67</td>\n",
       "      <td>6.04</td>\n",
       "      <td>115512.00</td>\n",
       "      <td>60672.00</td>\n",
       "      <td>54947.66</td>\n",
       "      <td>1985671.00</td>\n",
       "      <td>3014.05</td>\n",
       "      <td>561717.49</td>\n",
       "      <td>1292873.00</td>\n",
       "      <td>578.40</td>\n",
       "      <td>766513.45</td>\n",
       "      <td>486465.00</td>\n",
       "      <td>6.93</td>\n",
       "      <td>7.13</td>\n",
       "      <td>7.80</td>\n",
       "      <td>6.93</td>\n",
       "      <td>6.05</td>\n",
       "      <td>34.18</td>\n",
       "      <td>51.00</td>\n",
       "      <td>55.00</td>\n",
       "      <td>48709.00</td>\n",
       "      <td>142</td>\n",
       "      <td>187</td>\n",
       "      <td>1.00</td>\n",
       "      <td>23.00</td>\n",
       "      <td>379.00</td>\n",
       "      <td>377.00</td>\n",
       "      <td>70000.00</td>\n",
       "      <td>Bajista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>947</th>\n",
       "      <td>2024-05-01</td>\n",
       "      <td>6.42</td>\n",
       "      <td>6.93</td>\n",
       "      <td>6.13</td>\n",
       "      <td>175570.00</td>\n",
       "      <td>58364.97</td>\n",
       "      <td>81166.47</td>\n",
       "      <td>2401089.00</td>\n",
       "      <td>2972.46</td>\n",
       "      <td>624963.78</td>\n",
       "      <td>1365039.00</td>\n",
       "      <td>561.80</td>\n",
       "      <td>669027.32</td>\n",
       "      <td>427425.00</td>\n",
       "      <td>6.85</td>\n",
       "      <td>7.11</td>\n",
       "      <td>7.41</td>\n",
       "      <td>6.85</td>\n",
       "      <td>6.29</td>\n",
       "      <td>43.30</td>\n",
       "      <td>42.00</td>\n",
       "      <td>50.00</td>\n",
       "      <td>83718.00</td>\n",
       "      <td>130</td>\n",
       "      <td>177</td>\n",
       "      <td>0.00</td>\n",
       "      <td>36.00</td>\n",
       "      <td>327.00</td>\n",
       "      <td>340.00</td>\n",
       "      <td>107000.00</td>\n",
       "      <td>Alcista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948</th>\n",
       "      <td>2024-05-02</td>\n",
       "      <td>6.90</td>\n",
       "      <td>7.41</td>\n",
       "      <td>6.69</td>\n",
       "      <td>109002.00</td>\n",
       "      <td>59060.61</td>\n",
       "      <td>47583.82</td>\n",
       "      <td>1572898.00</td>\n",
       "      <td>2986.19</td>\n",
       "      <td>365939.72</td>\n",
       "      <td>880167.00</td>\n",
       "      <td>560.50</td>\n",
       "      <td>359794.32</td>\n",
       "      <td>250921.00</td>\n",
       "      <td>6.85</td>\n",
       "      <td>7.12</td>\n",
       "      <td>7.42</td>\n",
       "      <td>6.85</td>\n",
       "      <td>6.29</td>\n",
       "      <td>49.27</td>\n",
       "      <td>87.00</td>\n",
       "      <td>57.00</td>\n",
       "      <td>61208.00</td>\n",
       "      <td>461</td>\n",
       "      <td>374</td>\n",
       "      <td>1.00</td>\n",
       "      <td>25.00</td>\n",
       "      <td>270.00</td>\n",
       "      <td>282.00</td>\n",
       "      <td>71000.00</td>\n",
       "      <td>Alcista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949</th>\n",
       "      <td>2024-05-03</td>\n",
       "      <td>7.27</td>\n",
       "      <td>7.39</td>\n",
       "      <td>7.00</td>\n",
       "      <td>109634.00</td>\n",
       "      <td>62882.01</td>\n",
       "      <td>43628.40</td>\n",
       "      <td>1558661.00</td>\n",
       "      <td>3102.61</td>\n",
       "      <td>355825.84</td>\n",
       "      <td>859542.00</td>\n",
       "      <td>587.00</td>\n",
       "      <td>342906.43</td>\n",
       "      <td>257575.00</td>\n",
       "      <td>6.90</td>\n",
       "      <td>7.14</td>\n",
       "      <td>7.44</td>\n",
       "      <td>6.90</td>\n",
       "      <td>6.35</td>\n",
       "      <td>48.86</td>\n",
       "      <td>52.00</td>\n",
       "      <td>40.00</td>\n",
       "      <td>46255.00</td>\n",
       "      <td>573</td>\n",
       "      <td>474</td>\n",
       "      <td>1.00</td>\n",
       "      <td>22.00</td>\n",
       "      <td>386.00</td>\n",
       "      <td>635.00</td>\n",
       "      <td>69000.00</td>\n",
       "      <td>Lateral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>2024-05-04</td>\n",
       "      <td>7.24</td>\n",
       "      <td>7.28</td>\n",
       "      <td>7.11</td>\n",
       "      <td>71120.00</td>\n",
       "      <td>63892.04</td>\n",
       "      <td>24368.69</td>\n",
       "      <td>1113509.00</td>\n",
       "      <td>3117.23</td>\n",
       "      <td>196263.95</td>\n",
       "      <td>575026.00</td>\n",
       "      <td>585.70</td>\n",
       "      <td>197129.25</td>\n",
       "      <td>210303.00</td>\n",
       "      <td>6.91</td>\n",
       "      <td>7.13</td>\n",
       "      <td>7.46</td>\n",
       "      <td>6.91</td>\n",
       "      <td>6.36</td>\n",
       "      <td>46.98</td>\n",
       "      <td>68.00</td>\n",
       "      <td>50.00</td>\n",
       "      <td>34251.00</td>\n",
       "      <td>407</td>\n",
       "      <td>472</td>\n",
       "      <td>0.00</td>\n",
       "      <td>14.00</td>\n",
       "      <td>203.00</td>\n",
       "      <td>232.00</td>\n",
       "      <td>49000.00</td>\n",
       "      <td>Bajista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951</th>\n",
       "      <td>2024-05-05</td>\n",
       "      <td>7.12</td>\n",
       "      <td>7.40</td>\n",
       "      <td>7.01</td>\n",
       "      <td>72928.00</td>\n",
       "      <td>64012.00</td>\n",
       "      <td>18526.75</td>\n",
       "      <td>992921.00</td>\n",
       "      <td>3136.41</td>\n",
       "      <td>218760.27</td>\n",
       "      <td>600693.00</td>\n",
       "      <td>592.00</td>\n",
       "      <td>180458.24</td>\n",
       "      <td>180794.00</td>\n",
       "      <td>6.94</td>\n",
       "      <td>7.15</td>\n",
       "      <td>7.51</td>\n",
       "      <td>6.94</td>\n",
       "      <td>6.37</td>\n",
       "      <td>50.00</td>\n",
       "      <td>37.00</td>\n",
       "      <td>52.00</td>\n",
       "      <td>29197.00</td>\n",
       "      <td>417</td>\n",
       "      <td>499</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>320.00</td>\n",
       "      <td>284.00</td>\n",
       "      <td>47000.00</td>\n",
       "      <td>Alcista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>2024-05-06</td>\n",
       "      <td>7.30</td>\n",
       "      <td>7.47</td>\n",
       "      <td>7.06</td>\n",
       "      <td>94264.00</td>\n",
       "      <td>63165.19</td>\n",
       "      <td>34674.92</td>\n",
       "      <td>1392557.00</td>\n",
       "      <td>3062.60</td>\n",
       "      <td>355135.30</td>\n",
       "      <td>873200.00</td>\n",
       "      <td>588.20</td>\n",
       "      <td>278669.01</td>\n",
       "      <td>248490.00</td>\n",
       "      <td>6.96</td>\n",
       "      <td>7.15</td>\n",
       "      <td>7.53</td>\n",
       "      <td>6.96</td>\n",
       "      <td>6.39</td>\n",
       "      <td>47.10</td>\n",
       "      <td>49.00</td>\n",
       "      <td>71.00</td>\n",
       "      <td>40027.00</td>\n",
       "      <td>482</td>\n",
       "      <td>531</td>\n",
       "      <td>0.00</td>\n",
       "      <td>25.00</td>\n",
       "      <td>339.00</td>\n",
       "      <td>249.00</td>\n",
       "      <td>59000.00</td>\n",
       "      <td>Bajista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>2024-05-07</td>\n",
       "      <td>7.12</td>\n",
       "      <td>7.29</td>\n",
       "      <td>6.98</td>\n",
       "      <td>64947.00</td>\n",
       "      <td>62312.08</td>\n",
       "      <td>25598.79</td>\n",
       "      <td>1272898.00</td>\n",
       "      <td>3005.69</td>\n",
       "      <td>298796.68</td>\n",
       "      <td>815246.00</td>\n",
       "      <td>576.50</td>\n",
       "      <td>289488.71</td>\n",
       "      <td>266127.00</td>\n",
       "      <td>6.98</td>\n",
       "      <td>7.13</td>\n",
       "      <td>7.52</td>\n",
       "      <td>6.98</td>\n",
       "      <td>6.44</td>\n",
       "      <td>45.10</td>\n",
       "      <td>21.00</td>\n",
       "      <td>25.00</td>\n",
       "      <td>31028.00</td>\n",
       "      <td>495</td>\n",
       "      <td>494</td>\n",
       "      <td>0.00</td>\n",
       "      <td>28.00</td>\n",
       "      <td>296.00</td>\n",
       "      <td>205.00</td>\n",
       "      <td>42000.00</td>\n",
       "      <td>Bajista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>2024-05-08</td>\n",
       "      <td>6.99</td>\n",
       "      <td>7.19</td>\n",
       "      <td>6.89</td>\n",
       "      <td>75550.00</td>\n",
       "      <td>61193.03</td>\n",
       "      <td>26121.19</td>\n",
       "      <td>1415152.00</td>\n",
       "      <td>2974.21</td>\n",
       "      <td>266934.81</td>\n",
       "      <td>830635.00</td>\n",
       "      <td>588.60</td>\n",
       "      <td>297016.62</td>\n",
       "      <td>249379.00</td>\n",
       "      <td>6.99</td>\n",
       "      <td>7.12</td>\n",
       "      <td>7.52</td>\n",
       "      <td>6.99</td>\n",
       "      <td>6.46</td>\n",
       "      <td>44.94</td>\n",
       "      <td>17.00</td>\n",
       "      <td>24.00</td>\n",
       "      <td>32040.00</td>\n",
       "      <td>426</td>\n",
       "      <td>494</td>\n",
       "      <td>0.00</td>\n",
       "      <td>24.00</td>\n",
       "      <td>230.00</td>\n",
       "      <td>177.00</td>\n",
       "      <td>49000.00</td>\n",
       "      <td>Lateral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955</th>\n",
       "      <td>2024-05-09</td>\n",
       "      <td>6.98</td>\n",
       "      <td>7.09</td>\n",
       "      <td>6.78</td>\n",
       "      <td>75016.00</td>\n",
       "      <td>63074.01</td>\n",
       "      <td>30660.81</td>\n",
       "      <td>1381957.00</td>\n",
       "      <td>3036.23</td>\n",
       "      <td>238561.75</td>\n",
       "      <td>686147.00</td>\n",
       "      <td>596.80</td>\n",
       "      <td>464857.60</td>\n",
       "      <td>332988.00</td>\n",
       "      <td>7.01</td>\n",
       "      <td>7.11</td>\n",
       "      <td>7.52</td>\n",
       "      <td>7.01</td>\n",
       "      <td>6.50</td>\n",
       "      <td>46.32</td>\n",
       "      <td>18.00</td>\n",
       "      <td>17.00</td>\n",
       "      <td>29314.00</td>\n",
       "      <td>475</td>\n",
       "      <td>464</td>\n",
       "      <td>0.00</td>\n",
       "      <td>16.00</td>\n",
       "      <td>188.00</td>\n",
       "      <td>257.00</td>\n",
       "      <td>50000.00</td>\n",
       "      <td>Lateral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Open_time  Open  High  Low  Number of trades  Close_BTCUSDT  \\\n",
       "946 2024-04-30  6.59  6.67 6.04         115512.00       60672.00   \n",
       "947 2024-05-01  6.42  6.93 6.13         175570.00       58364.97   \n",
       "948 2024-05-02  6.90  7.41 6.69         109002.00       59060.61   \n",
       "949 2024-05-03  7.27  7.39 7.00         109634.00       62882.01   \n",
       "950 2024-05-04  7.24  7.28 7.11          71120.00       63892.04   \n",
       "951 2024-05-05  7.12  7.40 7.01          72928.00       64012.00   \n",
       "952 2024-05-06  7.30  7.47 7.06          94264.00       63165.19   \n",
       "953 2024-05-07  7.12  7.29 6.98          64947.00       62312.08   \n",
       "954 2024-05-08  6.99  7.19 6.89          75550.00       61193.03   \n",
       "955 2024-05-09  6.98  7.09 6.78          75016.00       63074.01   \n",
       "\n",
       "     Volume_BTCUSDT  Number_of_trades_BTCUSDT  Close_ETHUSDT  Volume_ETHUSDT  \\\n",
       "946        54947.66                1985671.00        3014.05       561717.49   \n",
       "947        81166.47                2401089.00        2972.46       624963.78   \n",
       "948        47583.82                1572898.00        2986.19       365939.72   \n",
       "949        43628.40                1558661.00        3102.61       355825.84   \n",
       "950        24368.69                1113509.00        3117.23       196263.95   \n",
       "951        18526.75                 992921.00        3136.41       218760.27   \n",
       "952        34674.92                1392557.00        3062.60       355135.30   \n",
       "953        25598.79                1272898.00        3005.69       298796.68   \n",
       "954        26121.19                1415152.00        2974.21       266934.81   \n",
       "955        30660.81                1381957.00        3036.23       238561.75   \n",
       "\n",
       "     Number_of_trades_ETHUSDT  Close_BNBUSDT  Volume_BNBUSDT  \\\n",
       "946                1292873.00         578.40       766513.45   \n",
       "947                1365039.00         561.80       669027.32   \n",
       "948                 880167.00         560.50       359794.32   \n",
       "949                 859542.00         587.00       342906.43   \n",
       "950                 575026.00         585.70       197129.25   \n",
       "951                 600693.00         592.00       180458.24   \n",
       "952                 873200.00         588.20       278669.01   \n",
       "953                 815246.00         576.50       289488.71   \n",
       "954                 830635.00         588.60       297016.62   \n",
       "955                 686147.00         596.80       464857.60   \n",
       "\n",
       "     Number_of_trades_BNBUSDT  SMA_20  EMA_20  Upper_Band  Middle_Band  \\\n",
       "946                 486465.00    6.93    7.13        7.80         6.93   \n",
       "947                 427425.00    6.85    7.11        7.41         6.85   \n",
       "948                 250921.00    6.85    7.12        7.42         6.85   \n",
       "949                 257575.00    6.90    7.14        7.44         6.90   \n",
       "950                 210303.00    6.91    7.13        7.46         6.91   \n",
       "951                 180794.00    6.94    7.15        7.51         6.94   \n",
       "952                 248490.00    6.96    7.15        7.53         6.96   \n",
       "953                 266127.00    6.98    7.13        7.52         6.98   \n",
       "954                 249379.00    6.99    7.12        7.52         6.99   \n",
       "955                 332988.00    7.01    7.11        7.52         7.01   \n",
       "\n",
       "     Lower_Band   RSI  buy_1000x_high_coinbase  sell_1000x_high_coinbase  \\\n",
       "946        6.05 34.18                    51.00                     55.00   \n",
       "947        6.29 43.30                    42.00                     50.00   \n",
       "948        6.29 49.27                    87.00                     57.00   \n",
       "949        6.35 48.86                    52.00                     40.00   \n",
       "950        6.36 46.98                    68.00                     50.00   \n",
       "951        6.37 50.00                    37.00                     52.00   \n",
       "952        6.39 47.10                    49.00                     71.00   \n",
       "953        6.44 45.10                    21.00                     25.00   \n",
       "954        6.46 44.94                    17.00                     24.00   \n",
       "955        6.50 46.32                    18.00                     17.00   \n",
       "\n",
       "     total_trades_coinbase  Tweets_Utilizados  Tweets_Utilizados_coin  \\\n",
       "946               48709.00                142                     187   \n",
       "947               83718.00                130                     177   \n",
       "948               61208.00                461                     374   \n",
       "949               46255.00                573                     474   \n",
       "950               34251.00                407                     472   \n",
       "951               29197.00                417                     499   \n",
       "952               40027.00                482                     531   \n",
       "953               31028.00                495                     494   \n",
       "954               32040.00                426                     494   \n",
       "955               29314.00                475                     464   \n",
       "\n",
       "     Tweets_Utilizados_referentes  Tweets_Utilizados_whale_alert  \\\n",
       "946                          1.00                          23.00   \n",
       "947                          0.00                          36.00   \n",
       "948                          1.00                          25.00   \n",
       "949                          1.00                          22.00   \n",
       "950                          0.00                          14.00   \n",
       "951                          0.00                           6.00   \n",
       "952                          0.00                          25.00   \n",
       "953                          0.00                          28.00   \n",
       "954                          0.00                          24.00   \n",
       "955                          0.00                          16.00   \n",
       "\n",
       "     Buy_1000x_high  sell_1000x_high  total_trades_binance Tendencia  \n",
       "946          379.00           377.00              70000.00   Bajista  \n",
       "947          327.00           340.00             107000.00   Alcista  \n",
       "948          270.00           282.00              71000.00   Alcista  \n",
       "949          386.00           635.00              69000.00   Lateral  \n",
       "950          203.00           232.00              49000.00   Bajista  \n",
       "951          320.00           284.00              47000.00   Alcista  \n",
       "952          339.00           249.00              59000.00   Bajista  \n",
       "953          296.00           205.00              42000.00   Bajista  \n",
       "954          230.00           177.00              49000.00   Lateral  \n",
       "955          188.00           257.00              50000.00   Lateral  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(clasifier_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 175ms/step\n",
      "1/1 [==============================] - 0s 181ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 2, 0, 0, 0, 0, 0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['Alcista', 'Bajista', 'Lateral'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Bajista',\n",
       " 'Alcista',\n",
       " 'Alcista',\n",
       " 'Alcista',\n",
       " 'Lateral',\n",
       " 'Alcista',\n",
       " 'Alcista',\n",
       " 'Alcista',\n",
       " 'Alcista',\n",
       " 'Alcista']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_clases = 3 \n",
    "\n",
    "# DESCOMENTAR PARA NORMALIZACION\n",
    "clasifier_validation_normalized = scaler.transform(clasifier_validation[columns].drop(columns=[\"Open_time\"]))\n",
    "validation_predictions = ensemble.predict(clasifier_validation_normalized)\n",
    "\n",
    "# validation_predictions = clasifier_validation[columns].drop(columns=[\"Open_time\"])\n",
    "# validation_predictions = ensemble.predict(validation_predictions)\n",
    "\n",
    "predicciones_one_hot = to_categorical(validation_predictions, num_classes=n_clases)\n",
    "etiquetas_numericas = np.argmax(predicciones_one_hot, axis=1)\n",
    "categorias_clases = onehot_encoder.categories_[0]\n",
    "nombres_clases = [categorias_clases[indice] for indice in etiquetas_numericas]\n",
    "\n",
    "display(validation_predictions)\n",
    "display(categorias_clases)\n",
    "display(nombres_clases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Datos generados por auto ml con prophet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 121ms/step\n",
      "1/1 [==============================] - 0s 134ms/step\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 107ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 2, 2, 1, 1, 1, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['Alcista', 'Bajista', 'Lateral'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Bajista',\n",
       " 'Bajista',\n",
       " 'Bajista',\n",
       " 'Bajista',\n",
       " 'Lateral',\n",
       " 'Lateral',\n",
       " 'Bajista',\n",
       " 'Bajista',\n",
       " 'Bajista',\n",
       " 'Bajista']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "auto_ml_prophet_df = pd.read_csv('auto_timeseries_models_prophet/predicciones.csv')\n",
    "auto_ml_prophet_df = auto_ml_prophet_df[columns].drop(columns=[\"Open_time\"])\n",
    "\n",
    "# DESCOMENTAR PARA NORMALIZACION\n",
    "auto_ml_prophet_df_normalized = scaler.transform(auto_ml_prophet_df)\n",
    "auto_mp_prophet_predictions = ensemble.predict(auto_ml_prophet_df_normalized)\n",
    "\n",
    "# auto_mp_prophet_predictions = ensemble.predict(auto_ml_prophet_df)\n",
    "\n",
    "predicciones_one_hot = to_categorical(auto_mp_prophet_predictions, num_classes=n_clases)\n",
    "etiquetas_numericas = np.argmax(predicciones_one_hot, axis=1)\n",
    "categorias_clases = onehot_encoder.categories_[0]\n",
    "nombres_clases = [categorias_clases[indice] for indice in etiquetas_numericas]\n",
    "\n",
    "display(auto_mp_prophet_predictions)\n",
    "display(categorias_clases)\n",
    "display(nombres_clases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Datos generados por auto ml con stats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 161ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['Alcista', 'Bajista', 'Lateral'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Alcista',\n",
       " 'Alcista',\n",
       " 'Alcista',\n",
       " 'Alcista',\n",
       " 'Bajista',\n",
       " 'Bajista',\n",
       " 'Bajista',\n",
       " 'Bajista',\n",
       " 'Bajista',\n",
       " 'Bajista']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "auto_ml_stats_df = pd.read_csv('auto_timeseries_models/predicciones.csv')\n",
    "auto_ml_stats_df = auto_ml_stats_df[columns].drop(columns=[\"Open_time\"])\n",
    "\n",
    "# DESCOMENTAR PARA NORMALIZACION\n",
    "auto_ml_stats_df_normalized = scaler.transform(auto_ml_stats_df)\n",
    "auto_mp_stats_predictions = ensemble.predict(auto_ml_stats_df_normalized)\n",
    "\n",
    "# auto_mp_stats_predictions = ensemble.predict(auto_ml_stats_df)\n",
    "\n",
    "predicciones_one_hot = to_categorical(auto_mp_stats_predictions, num_classes=n_clases)\n",
    "etiquetas_numericas = np.argmax(predicciones_one_hot, axis=1)\n",
    "categorias_clases = onehot_encoder.categories_[0]\n",
    "nombres_clases = [categorias_clases[indice] for indice in etiquetas_numericas]\n",
    "\n",
    "display(auto_mp_stats_predictions)\n",
    "display(categorias_clases)\n",
    "display(nombres_clases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Datos generados por auto ml con modelos clasicos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 151ms/step\n",
      "1/1 [==============================] - 0s 144ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 122ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 2, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['Alcista', 'Bajista', 'Lateral'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Bajista',\n",
       " 'Lateral',\n",
       " 'Bajista',\n",
       " 'Bajista',\n",
       " 'Bajista',\n",
       " 'Bajista',\n",
       " 'Bajista',\n",
       " 'Bajista',\n",
       " 'Bajista',\n",
       " 'Bajista']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "auto_ml_df = pd.read_csv('h2o_models/predicciones.csv')\n",
    "auto_ml_df = auto_ml_df[columns].drop(columns=[\"Open_time\"])\n",
    "\n",
    "# DESCOMENTAR PARA NORMALIZACION\n",
    "auto_ml_df_normalized = scaler.transform(auto_ml_df)\n",
    "auto_mp_predictions = ensemble.predict(auto_ml_df_normalized)\n",
    "\n",
    "# auto_mp_predictions = ensemble.predict(auto_ml_df)\n",
    "\n",
    "predicciones_one_hot = to_categorical(auto_mp_predictions, num_classes=n_clases)\n",
    "etiquetas_numericas = np.argmax(predicciones_one_hot, axis=1)\n",
    "categorias_clases = onehot_encoder.categories_[0]\n",
    "nombres_clases = [categorias_clases[indice] for indice in etiquetas_numericas]\n",
    "\n",
    "display(auto_mp_predictions)\n",
    "display(categorias_clases)\n",
    "display(nombres_clases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Datos generados por auto ml con skforecast:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "1/1 [==============================] - 0s 178ms/step\n",
      "1/1 [==============================] - 0s 328ms/step\n",
      "1/1 [==============================] - 0s 176ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['Alcista', 'Bajista', 'Lateral'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Bajista',\n",
       " 'Bajista',\n",
       " 'Bajista',\n",
       " 'Bajista',\n",
       " 'Bajista',\n",
       " 'Bajista',\n",
       " 'Bajista',\n",
       " 'Bajista',\n",
       " 'Bajista',\n",
       " 'Bajista']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "skforecast_df = pd.read_csv('skforecast/predicciones.csv')\n",
    "skforecast_df = skforecast_df[columns[1:]]\n",
    "\n",
    "# DESCOMENTAR PARA NORMALIZACION\n",
    "skforecast_df_normalized = scaler.transform(skforecast_df)\n",
    "skforecast_predictions = ensemble.predict(skforecast_df_normalized)\n",
    "\n",
    "# skforecast_predictions = ensemble.predict(skforecast_df)\n",
    "\n",
    "predicciones_one_hot = to_categorical(skforecast_predictions, num_classes=n_clases)\n",
    "etiquetas_numericas = np.argmax(predicciones_one_hot, axis=1)\n",
    "categorias_clases = onehot_encoder.categories_[0]\n",
    "nombres_clases = [categorias_clases[indice] for indice in etiquetas_numericas]\n",
    "\n",
    "display(skforecast_predictions)\n",
    "display(categorias_clases)\n",
    "display(nombres_clases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analizo los features mas importantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Permutation Feature Importance\n",
    "\n",
    "El método de Permutation Feature Importance implica permutar las características del dataset y medir el impacto en el rendimiento del modelo. Esto se hace una característica a la vez. Puedes utilizar el siguiente código para calcular la importancia de las características usando este método:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# def permutation_feature_importance(model, X, y, metric=accuracy_score, n_repeats=10):\n",
    "#     base_score = metric(y, model.predict(X))\n",
    "#     scores = np.zeros((X.shape[1], n_repeats))\n",
    "    \n",
    "#     for i in range(X.shape[1]):\n",
    "#         for n in range(n_repeats):\n",
    "#             X_permuted = X.copy()\n",
    "#             np.random.shuffle(X_permuted[:, i])\n",
    "#             permuted_score = metric(y, model.predict(X_permuted))\n",
    "#             scores[i, n] = base_score - permuted_score\n",
    "            \n",
    "#     importances = np.mean(scores, axis=1)\n",
    "#     return importances\n",
    "\n",
    "# # Usa el ensemble model para obtener las importancias\n",
    "# importances = permutation_feature_importance(ensemble, X.values, y)\n",
    "\n",
    "# # Ordena las características por importancia\n",
    "# feature_importances = sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# # Imprime las importancias\n",
    "# for feature, importance in feature_importances:\n",
    "#     print(f'Feature: {feature}, Importance: {importance}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SHAP (SHapley Additive exPlanations)\n",
    "\n",
    "SHAP es una técnica avanzada que explica las predicciones de cualquier modelo basado en el cálculo de los valores de Shapley de la teoría de juegos. Puedes usar la librería shap para calcular las importancias:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shap\n",
    "\n",
    "# # Crear un background dataset, usualmente se usa una muestra aleatoria de los datos de entrenamiento\n",
    "# background = X.sample(n=100)\n",
    "\n",
    "# # Crear el objeto explainer de SHAP\n",
    "# explainer = shap.KernelExplainer(ensemble.predict, background)\n",
    "\n",
    "# # Calcular los valores SHAP para el conjunto de datos completo\n",
    "# shap_values = explainer.shap_values(X)\n",
    "\n",
    "# # Visualizar las importancias de las características\n",
    "# shap.summary_plot(shap_values, X)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
