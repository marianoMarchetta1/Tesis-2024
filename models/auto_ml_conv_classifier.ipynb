{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from auto_ts import auto_timeseries\n",
    "import dill\n",
    "import talib\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from keras.callbacks import EarlyStopping\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.losses import categorical_crossentropy\n",
    "from sklearn.metrics import log_loss\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evito que ciertas columnas se transformen a notacion cientifica en las predicciones\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    'Open_time',\n",
    "    'Open',\n",
    "    'High',\n",
    "    'Low',\n",
    "    'Close',\n",
    "    'Number of trades',\n",
    "    'Close_BTCUSDT',\n",
    "    'Volume_BTCUSDT',\n",
    "    'Number_of_trades_BTCUSDT',\n",
    "    'Close_ETHUSDT',\n",
    "    'Volume_ETHUSDT',\n",
    "    'Number_of_trades_ETHUSDT',\n",
    "    'Close_BNBUSDT',\n",
    "    'Volume_BNBUSDT',\n",
    "    'Number_of_trades_BNBUSDT',\n",
    "    'SMA_20',\n",
    "    'EMA_20',\n",
    "    'Upper_Band',\n",
    "    'Middle_Band',\n",
    "    'Lower_Band',\n",
    "    'RSI',\n",
    "    'buy_1000x_high_coinbase',\n",
    "    'sell_1000x_high_coinbase',\n",
    "    'total_trades_coinbase',\t\n",
    "    'Tweets_Utilizados',\n",
    "    'Tweets_Utilizados_coin',\n",
    "    'Tweets_Utilizados_referentes',\n",
    "    'Tweets_Utilizados_whale_alert',\n",
    "    'Buy_1000x_high',\n",
    "    'sell_1000x_high',\n",
    "    'total_trades_binance'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Armado y entrenamiento de un clasificador a partir de los datos originales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelo convolucional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_dataset = pd.read_csv('/Users/mmarchetta/Desktop/Tesis-2024/data-visualization/final_dataset.csv') \n",
    "classifier_dataset = complete_dataset[columns]\n",
    "classifier_dataset['Open_time'] = pd.to_datetime(classifier_dataset['Open_time'])\n",
    "classifier_dataset['Tendencia'] = complete_dataset['Tendencia']\n",
    "\n",
    "clasifier_validation = classifier_dataset[-5:]\n",
    "classifier_dataset = classifier_dataset[:-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open_time</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Number of trades</th>\n",
       "      <th>Close_BTCUSDT</th>\n",
       "      <th>Volume_BTCUSDT</th>\n",
       "      <th>Number_of_trades_BTCUSDT</th>\n",
       "      <th>Close_ETHUSDT</th>\n",
       "      <th>Volume_ETHUSDT</th>\n",
       "      <th>Number_of_trades_ETHUSDT</th>\n",
       "      <th>Close_BNBUSDT</th>\n",
       "      <th>Volume_BNBUSDT</th>\n",
       "      <th>Number_of_trades_BNBUSDT</th>\n",
       "      <th>SMA_20</th>\n",
       "      <th>EMA_20</th>\n",
       "      <th>Upper_Band</th>\n",
       "      <th>Middle_Band</th>\n",
       "      <th>Lower_Band</th>\n",
       "      <th>RSI</th>\n",
       "      <th>buy_1000x_high_coinbase</th>\n",
       "      <th>sell_1000x_high_coinbase</th>\n",
       "      <th>total_trades_coinbase</th>\n",
       "      <th>Tweets_Utilizados</th>\n",
       "      <th>Tweets_Utilizados_coin</th>\n",
       "      <th>Tweets_Utilizados_referentes</th>\n",
       "      <th>Tweets_Utilizados_whale_alert</th>\n",
       "      <th>Buy_1000x_high</th>\n",
       "      <th>sell_1000x_high</th>\n",
       "      <th>total_trades_binance</th>\n",
       "      <th>Tendencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>2024-03-13</td>\n",
       "      <td>10.74</td>\n",
       "      <td>11.46</td>\n",
       "      <td>10.64</td>\n",
       "      <td>11.37</td>\n",
       "      <td>276468.00</td>\n",
       "      <td>73072.41</td>\n",
       "      <td>52659.71</td>\n",
       "      <td>2501197.00</td>\n",
       "      <td>4004.79</td>\n",
       "      <td>482305.78</td>\n",
       "      <td>1536498.00</td>\n",
       "      <td>630.50</td>\n",
       "      <td>2526002.56</td>\n",
       "      <td>1265237.00</td>\n",
       "      <td>9.45</td>\n",
       "      <td>9.70</td>\n",
       "      <td>11.82</td>\n",
       "      <td>9.45</td>\n",
       "      <td>7.07</td>\n",
       "      <td>73.43</td>\n",
       "      <td>64.00</td>\n",
       "      <td>81.00</td>\n",
       "      <td>92576.00</td>\n",
       "      <td>275</td>\n",
       "      <td>205</td>\n",
       "      <td>1.00</td>\n",
       "      <td>73.00</td>\n",
       "      <td>578.00</td>\n",
       "      <td>553.00</td>\n",
       "      <td>164000.00</td>\n",
       "      <td>Alcista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>2024-03-14</td>\n",
       "      <td>11.37</td>\n",
       "      <td>11.89</td>\n",
       "      <td>10.68</td>\n",
       "      <td>11.56</td>\n",
       "      <td>536988.00</td>\n",
       "      <td>71388.94</td>\n",
       "      <td>71757.63</td>\n",
       "      <td>2994869.00</td>\n",
       "      <td>3881.70</td>\n",
       "      <td>648237.52</td>\n",
       "      <td>1919963.00</td>\n",
       "      <td>603.20</td>\n",
       "      <td>2119540.30</td>\n",
       "      <td>1038297.00</td>\n",
       "      <td>9.65</td>\n",
       "      <td>9.88</td>\n",
       "      <td>12.03</td>\n",
       "      <td>9.65</td>\n",
       "      <td>7.27</td>\n",
       "      <td>74.51</td>\n",
       "      <td>102.00</td>\n",
       "      <td>133.00</td>\n",
       "      <td>145727.00</td>\n",
       "      <td>211</td>\n",
       "      <td>181</td>\n",
       "      <td>4.00</td>\n",
       "      <td>29.00</td>\n",
       "      <td>754.00</td>\n",
       "      <td>677.00</td>\n",
       "      <td>327000.00</td>\n",
       "      <td>Alcista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>2024-03-15</td>\n",
       "      <td>11.56</td>\n",
       "      <td>11.71</td>\n",
       "      <td>9.97</td>\n",
       "      <td>10.81</td>\n",
       "      <td>557152.00</td>\n",
       "      <td>69499.85</td>\n",
       "      <td>103334.04</td>\n",
       "      <td>3904445.00</td>\n",
       "      <td>3742.19</td>\n",
       "      <td>947537.41</td>\n",
       "      <td>2487337.00</td>\n",
       "      <td>632.70</td>\n",
       "      <td>3066312.79</td>\n",
       "      <td>1365283.00</td>\n",
       "      <td>9.80</td>\n",
       "      <td>9.97</td>\n",
       "      <td>12.07</td>\n",
       "      <td>9.80</td>\n",
       "      <td>7.52</td>\n",
       "      <td>63.27</td>\n",
       "      <td>88.00</td>\n",
       "      <td>83.00</td>\n",
       "      <td>147460.00</td>\n",
       "      <td>238</td>\n",
       "      <td>106</td>\n",
       "      <td>0.00</td>\n",
       "      <td>25.00</td>\n",
       "      <td>493.00</td>\n",
       "      <td>430.00</td>\n",
       "      <td>360000.00</td>\n",
       "      <td>Bajista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901</th>\n",
       "      <td>2024-03-16</td>\n",
       "      <td>10.81</td>\n",
       "      <td>10.90</td>\n",
       "      <td>9.50</td>\n",
       "      <td>9.68</td>\n",
       "      <td>330505.00</td>\n",
       "      <td>65300.63</td>\n",
       "      <td>55926.95</td>\n",
       "      <td>2729019.00</td>\n",
       "      <td>3523.09</td>\n",
       "      <td>548288.16</td>\n",
       "      <td>1798939.00</td>\n",
       "      <td>576.40</td>\n",
       "      <td>1811838.04</td>\n",
       "      <td>1025452.00</td>\n",
       "      <td>9.89</td>\n",
       "      <td>9.94</td>\n",
       "      <td>11.99</td>\n",
       "      <td>9.89</td>\n",
       "      <td>7.78</td>\n",
       "      <td>50.82</td>\n",
       "      <td>30.00</td>\n",
       "      <td>49.00</td>\n",
       "      <td>88095.00</td>\n",
       "      <td>670</td>\n",
       "      <td>471</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>513.00</td>\n",
       "      <td>403.00</td>\n",
       "      <td>209000.00</td>\n",
       "      <td>Bajista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>2024-03-17</td>\n",
       "      <td>9.68</td>\n",
       "      <td>10.25</td>\n",
       "      <td>9.19</td>\n",
       "      <td>10.08</td>\n",
       "      <td>229683.00</td>\n",
       "      <td>68393.48</td>\n",
       "      <td>49742.22</td>\n",
       "      <td>2449156.00</td>\n",
       "      <td>3644.71</td>\n",
       "      <td>517790.99</td>\n",
       "      <td>1721355.00</td>\n",
       "      <td>571.70</td>\n",
       "      <td>1712920.34</td>\n",
       "      <td>802297.00</td>\n",
       "      <td>9.98</td>\n",
       "      <td>9.95</td>\n",
       "      <td>11.93</td>\n",
       "      <td>9.98</td>\n",
       "      <td>8.04</td>\n",
       "      <td>54.27</td>\n",
       "      <td>36.00</td>\n",
       "      <td>48.00</td>\n",
       "      <td>71390.00</td>\n",
       "      <td>693</td>\n",
       "      <td>413</td>\n",
       "      <td>0.00</td>\n",
       "      <td>21.00</td>\n",
       "      <td>295.00</td>\n",
       "      <td>277.00</td>\n",
       "      <td>150000.00</td>\n",
       "      <td>Alcista</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Open_time  Open  High   Low  Close  Number of trades  Close_BTCUSDT  \\\n",
       "898 2024-03-13 10.74 11.46 10.64  11.37         276468.00       73072.41   \n",
       "899 2024-03-14 11.37 11.89 10.68  11.56         536988.00       71388.94   \n",
       "900 2024-03-15 11.56 11.71  9.97  10.81         557152.00       69499.85   \n",
       "901 2024-03-16 10.81 10.90  9.50   9.68         330505.00       65300.63   \n",
       "902 2024-03-17  9.68 10.25  9.19  10.08         229683.00       68393.48   \n",
       "\n",
       "     Volume_BTCUSDT  Number_of_trades_BTCUSDT  Close_ETHUSDT  Volume_ETHUSDT  \\\n",
       "898        52659.71                2501197.00        4004.79       482305.78   \n",
       "899        71757.63                2994869.00        3881.70       648237.52   \n",
       "900       103334.04                3904445.00        3742.19       947537.41   \n",
       "901        55926.95                2729019.00        3523.09       548288.16   \n",
       "902        49742.22                2449156.00        3644.71       517790.99   \n",
       "\n",
       "     Number_of_trades_ETHUSDT  Close_BNBUSDT  Volume_BNBUSDT  \\\n",
       "898                1536498.00         630.50      2526002.56   \n",
       "899                1919963.00         603.20      2119540.30   \n",
       "900                2487337.00         632.70      3066312.79   \n",
       "901                1798939.00         576.40      1811838.04   \n",
       "902                1721355.00         571.70      1712920.34   \n",
       "\n",
       "     Number_of_trades_BNBUSDT  SMA_20  EMA_20  Upper_Band  Middle_Band  \\\n",
       "898                1265237.00    9.45    9.70       11.82         9.45   \n",
       "899                1038297.00    9.65    9.88       12.03         9.65   \n",
       "900                1365283.00    9.80    9.97       12.07         9.80   \n",
       "901                1025452.00    9.89    9.94       11.99         9.89   \n",
       "902                 802297.00    9.98    9.95       11.93         9.98   \n",
       "\n",
       "     Lower_Band   RSI  buy_1000x_high_coinbase  sell_1000x_high_coinbase  \\\n",
       "898        7.07 73.43                    64.00                     81.00   \n",
       "899        7.27 74.51                   102.00                    133.00   \n",
       "900        7.52 63.27                    88.00                     83.00   \n",
       "901        7.78 50.82                    30.00                     49.00   \n",
       "902        8.04 54.27                    36.00                     48.00   \n",
       "\n",
       "     total_trades_coinbase  Tweets_Utilizados  Tweets_Utilizados_coin  \\\n",
       "898               92576.00                275                     205   \n",
       "899              145727.00                211                     181   \n",
       "900              147460.00                238                     106   \n",
       "901               88095.00                670                     471   \n",
       "902               71390.00                693                     413   \n",
       "\n",
       "     Tweets_Utilizados_referentes  Tweets_Utilizados_whale_alert  \\\n",
       "898                          1.00                          73.00   \n",
       "899                          4.00                          29.00   \n",
       "900                          0.00                          25.00   \n",
       "901                          0.00                          20.00   \n",
       "902                          0.00                          21.00   \n",
       "\n",
       "     Buy_1000x_high  sell_1000x_high  total_trades_binance Tendencia  \n",
       "898          578.00           553.00             164000.00   Alcista  \n",
       "899          754.00           677.00             327000.00   Alcista  \n",
       "900          493.00           430.00             360000.00   Bajista  \n",
       "901          513.00           403.00             209000.00   Bajista  \n",
       "902          295.00           277.00             150000.00   Alcista  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(classifier_dataset.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(903, 32)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = classifier_dataset.drop(columns=[\"Tendencia\", \"Open_time\"])\n",
    "y = classifier_dataset[\"Tendencia\"]\n",
    "\n",
    "# label_encoder = LabelEncoder()\n",
    "# y_encoded = label_encoder.fit_transform(classifier_dataset[\"Tendencia\"])\n",
    "\n",
    "y = y.to_numpy().reshape(-1, 1)\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "y_one_hot = onehot_encoder.fit_transform(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(y_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    }
   ],
   "source": [
    "# Function to create the model\n",
    "def create_model(activation, units, dropout, learning_rate, l2_penalty, depth, optimizer='adam'):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(units, kernel_size=3, activation=activation, input_shape=(len(X.columns), 1)))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    for _ in range(depth - 1):\n",
    "        model.add(Conv1D(units, kernel_size=3, activation=activation))\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units, activation=activation, kernel_regularizer=l2(l2_penalty)))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(units=3, activation='softmax'))\n",
    "\n",
    "    if optimizer == 'adam':\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == 'rmsprop':\n",
    "        optimizer = RMSprop(learning_rate=learning_rate)\n",
    "    elif optimizer == 'sgd':\n",
    "        optimizer = SGD(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "regressor = KerasRegressor(build_fn=create_model, verbose=0, activation='relu', units=50, dropout=0.2, learning_rate=0.1, l2_penalty=0.001, depth=2, optimizer='adam')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# Define cross-validation\n",
    "cv = TimeSeriesSplit(n_splits=10).split(X)\n",
    "\n",
    "# Define parameter space\n",
    "param_space = {\n",
    "    'depth': [2, 3],\n",
    "    'activation': ['relu', 'tanh', 'swish', 'selu'],\n",
    "    'units': [64, 128, 256, 512],\n",
    "    'dropout': [0.1, 0.2, 0.3, 0.4],\n",
    "    'learning_rate': [0.01, 0.001, 0.0001],\n",
    "    'epochs': [10, 20, 30, 50, 100],\n",
    "    'batch_size': [32, 64, 128],\n",
    "    'optimizer': ['adam', 'rmsprop', 'sgd'],\n",
    "    'l2_penalty': [0.001, 0.01, 0.1]\n",
    "}\n",
    "\n",
    "def categorical_crossentropy_loss(estimator, X_test, y_test):\n",
    "    y_pred = estimator.predict(X_test)\n",
    "    loss = log_loss(y_test, y_pred)\n",
    "    return loss\n",
    "\n",
    "# Perform Bayesian optimization\n",
    "bayes_search = BayesSearchCV(regressor, param_space, scoring=categorical_crossentropy_loss, cv=cv,verbose=1)\n",
    "bayes_result = bayes_search.fit(X, y_one_hot, callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 7.433947600348458\n",
      "Best parameters: OrderedDict([('activation', 'swish'), ('batch_size', 128), ('depth', 3), ('dropout', 0.1), ('epochs', 10), ('l2_penalty', 0.001), ('learning_rate', 0.01), ('optimizer', 'adam'), ('units', 512)])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KerasRegressor(\n",
       "\tmodel=None\n",
       "\tbuild_fn=&lt;function create_model at 0x7fe86ff3e820&gt;\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=adam\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=128\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=0\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=10\n",
       "\tactivation=swish\n",
       "\tunits=512\n",
       "\tdropout=0.1\n",
       "\tlearning_rate=0.01\n",
       "\tl2_penalty=0.001\n",
       "\tdepth=3\n",
       ")</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KerasRegressor</label><div class=\"sk-toggleable__content\"><pre>KerasRegressor(\n",
       "\tmodel=None\n",
       "\tbuild_fn=&lt;function create_model at 0x7fe86ff3e820&gt;\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=adam\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=128\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=0\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=10\n",
       "\tactivation=swish\n",
       "\tunits=512\n",
       "\tdropout=0.1\n",
       "\tlearning_rate=0.01\n",
       "\tl2_penalty=0.001\n",
       "\tdepth=3\n",
       ")</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KerasRegressor(\n",
       "\tmodel=None\n",
       "\tbuild_fn=<function create_model at 0x7fe86ff3e820>\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=adam\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=128\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=0\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=10\n",
       "\tactivation=swish\n",
       "\tunits=512\n",
       "\tdropout=0.1\n",
       "\tlearning_rate=0.01\n",
       "\tl2_penalty=0.001\n",
       "\tdepth=3\n",
       ")"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show best results\n",
    "print(\"Best score:\", bayes_result.best_score_)\n",
    "print(\"Best parameters:\", bayes_result.best_params_)\n",
    "\n",
    "# Entrenar el modelo con los mejores hiperparámetros\n",
    "best_model = bayes_result.best_estimator_\n",
    "best_model.fit(X, y_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 mejores modelos:\n",
      "Modelo 1\n",
      "Hiperparámetros: OrderedDict([('activation', 'selu'), ('batch_size', 32), ('depth', 2), ('dropout', 0.4), ('epochs', 50), ('l2_penalty', 0.1), ('learning_rate', 0.0001), ('optimizer', 'sgd'), ('units', 256)])\n",
      "Puntaje: 1.3145139783581825\n",
      "Modelo 2\n",
      "Hiperparámetros: OrderedDict([('activation', 'tanh'), ('batch_size', 64), ('depth', 2), ('dropout', 0.4), ('epochs', 10), ('l2_penalty', 0.001), ('learning_rate', 0.001), ('optimizer', 'rmsprop'), ('units', 128)])\n",
      "Puntaje: 1.0971109449652734\n",
      "Modelo 3\n",
      "Hiperparámetros: OrderedDict([('activation', 'selu'), ('batch_size', 32), ('depth', 2), ('dropout', 0.2), ('epochs', 20), ('l2_penalty', 0.1), ('learning_rate', 0.001), ('optimizer', 'adam'), ('units', 256)])\n",
      "Puntaje: 2.448404133680249\n",
      "Modelo 4\n",
      "Hiperparámetros: OrderedDict([('activation', 'selu'), ('batch_size', 32), ('depth', 2), ('dropout', 0.3), ('epochs', 100), ('l2_penalty', 0.1), ('learning_rate', 0.01), ('optimizer', 'rmsprop'), ('units', 128)])\n",
      "Puntaje: 1.1231212524842908\n",
      "Modelo 5\n",
      "Hiperparámetros: OrderedDict([('activation', 'swish'), ('batch_size', 64), ('depth', 2), ('dropout', 0.4), ('epochs', 20), ('l2_penalty', 0.1), ('learning_rate', 0.0001), ('optimizer', 'adam'), ('units', 64)])\n",
      "Puntaje: 1.2827424770689713\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Obtener los hiperparámetros y puntajes de los 5 mejores modelos\n",
    "top_n_models = 5\n",
    "best_params_list = []\n",
    "best_scores_list = []\n",
    "\n",
    "for i in range(min(top_n_models, len(bayes_search.cv_results_['params']))):\n",
    "    best_params_list.append(bayes_search.cv_results_['params'][i])\n",
    "    best_scores_list.append(bayes_search.cv_results_['mean_test_score'][i])\n",
    "\n",
    "# Guardar los hiperparámetros de los 5 mejores modelos en un archivo JSON\n",
    "with open('conv_classifier/top_5_hyperparameters.json', 'w') as f:\n",
    "    json.dump({'best_params': best_params_list, 'best_scores': best_scores_list}, f)\n",
    "\n",
    "# O imprimir los hiperparámetros\n",
    "print(\"Top 5 mejores modelos:\")\n",
    "for i in range(len(best_params_list)):\n",
    "    print(\"Modelo\", i+1)\n",
    "    print(\"Hiperparámetros:\", best_params_list[i])\n",
    "    print(\"Puntaje:\", best_scores_list[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Armado del ensamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prime_seeds(n):\n",
    "    seeds = []\n",
    "    num = 70001  # Comenzamos desde el primer número primo mayor que 70000\n",
    "    while len(seeds) < n:\n",
    "        is_prime = True\n",
    "        for i in range(2, int(num**0.5) + 1):\n",
    "            if num % i == 0:\n",
    "                is_prime = False\n",
    "                break\n",
    "        if is_prime:\n",
    "            seeds.append(num)\n",
    "        num += 1\n",
    "    return seeds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clase personalizada para hacer el ensamble, dado que sklearn no provee ninguna clase que permita hacer ensmble\n",
    "## de modelos re regresion multivariados\n",
    "class MultivariableVotingRegressor:\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for model in self.models:\n",
    "            model.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Hacer predicciones con cada modelo\n",
    "        predictions = np.array([model.predict(X) for model in self.models])\n",
    "        \n",
    "        # Calcular la moda de las predicciones\n",
    "        mode_predictions = np.argmax(np.sum(predictions, axis=0), axis=1)\n",
    "        \n",
    "        return mode_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "# Leer los hiperparámetros desde el archivo JSON\n",
    "with open('conv_classifier/top_5_hyperparameters.json', 'r') as f:\n",
    "    top_hyperparameters = json.load(f)\n",
    "\n",
    "prime_seeds = generate_prime_seeds(300)\n",
    "models = []\n",
    "best_seeds= {}\n",
    "\n",
    "# Train models with different seeds for each set of hyperparameters\n",
    "for mode_number, params in enumerate(top_hyperparameters['best_params']):\n",
    "    best_validation_errors = {}\n",
    "    \n",
    "    for seed_number, seed in enumerate(prime_seeds):\n",
    "        model = KerasRegressor(build_fn=create_model, random_state=seed, verbose=0, **params)\n",
    "        model.fit(X, y_one_hot)\n",
    "        \n",
    "        # Make predictions with the model\n",
    "        model_predictions = model.predict(X)\n",
    "        \n",
    "        # Calculate error (training error)\n",
    "        train_error = categorical_crossentropy(y_one_hot, model_predictions)\n",
    "        \n",
    "        mean_train_error = np.mean(train_error)\n",
    "\n",
    "        # Update best validation error for this seed\n",
    "        best_validation_errors[seed] = mean_train_error\n",
    "    \n",
    "    # print(\"Best validation errors:\", best_validation_errors)\n",
    "\n",
    "    # Find the best seed for this set of hyperparameters\n",
    "    best_seed_for_params = min(best_validation_errors, key=lambda k: best_validation_errors[k])\n",
    "    best_seeds[str(params)] = best_seed_for_params\n",
    "    \n",
    "    # Create and train the model with the best seed\n",
    "    model = KerasRegressor(build_fn=create_model, random_state=best_seed_for_params, verbose=0, **params)\n",
    "    model.fit(X, y_one_hot)\n",
    "    models.append(model)\n",
    "\n",
    "# Define and train the ensemble model\n",
    "ensemble = MultivariableVotingRegressor(models)\n",
    "ensemble.fit(X, y_one_hot)\n",
    "\n",
    "# Save the best seeds to a JSON file\n",
    "with open('conv_classifier/best_seeds.json', 'w') as f:\n",
    "    json.dump(best_seeds, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clasificacion con el ensamble sobre las redicciones de los modelos generativos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "29/29 [==============================] - 4s 16ms/step - loss: 43.2721 - accuracy: 0.3953\n",
      "Epoch 2/50\n",
      "29/29 [==============================] - 0s 14ms/step - loss: 38.9668 - accuracy: 0.4352\n",
      "Epoch 3/50\n",
      "29/29 [==============================] - 1s 18ms/step - loss: 35.1119 - accuracy: 0.4075\n",
      "Epoch 4/50\n",
      "29/29 [==============================] - 1s 17ms/step - loss: 31.6139 - accuracy: 0.4109\n",
      "Epoch 5/50\n",
      "29/29 [==============================] - 0s 15ms/step - loss: 28.4705 - accuracy: 0.4341\n",
      "Epoch 6/50\n",
      "29/29 [==============================] - 0s 14ms/step - loss: 25.6703 - accuracy: 0.4585\n",
      "Epoch 7/50\n",
      "29/29 [==============================] - 1s 22ms/step - loss: 23.2328 - accuracy: 0.4385\n",
      "Epoch 8/50\n",
      "29/29 [==============================] - 0s 14ms/step - loss: 20.9785 - accuracy: 0.4341\n",
      "Epoch 9/50\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 18.9250 - accuracy: 0.4297\n",
      "Epoch 10/50\n",
      "29/29 [==============================] - 0s 16ms/step - loss: 17.1654 - accuracy: 0.4485\n",
      "Epoch 11/50\n",
      "29/29 [==============================] - 1s 18ms/step - loss: 15.4941 - accuracy: 0.4640\n",
      "Epoch 12/50\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 14.0830 - accuracy: 0.4330\n",
      "Epoch 13/50\n",
      "29/29 [==============================] - 0s 16ms/step - loss: 12.7858 - accuracy: 0.4551\n",
      "Epoch 14/50\n",
      "29/29 [==============================] - 1s 19ms/step - loss: 11.6151 - accuracy: 0.4718\n",
      "Epoch 15/50\n",
      "29/29 [==============================] - 0s 16ms/step - loss: 10.6191 - accuracy: 0.4529\n",
      "Epoch 16/50\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 9.7203 - accuracy: 0.4496\n",
      "Epoch 17/50\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 8.9167 - accuracy: 0.4408\n",
      "Epoch 18/50\n",
      "29/29 [==============================] - 0s 17ms/step - loss: 8.1843 - accuracy: 0.4385\n",
      "Epoch 19/50\n",
      "29/29 [==============================] - 1s 17ms/step - loss: 7.4983 - accuracy: 0.4441\n",
      "Epoch 20/50\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 6.9075 - accuracy: 0.4551\n",
      "Epoch 21/50\n",
      "29/29 [==============================] - 0s 16ms/step - loss: 6.3863 - accuracy: 0.4607\n",
      "Epoch 22/50\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 5.8900 - accuracy: 0.4596\n",
      "Epoch 23/50\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 5.4532 - accuracy: 0.4806\n",
      "Epoch 24/50\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 5.0971 - accuracy: 0.4729\n",
      "Epoch 25/50\n",
      "29/29 [==============================] - 0s 16ms/step - loss: 4.7442 - accuracy: 0.4518\n",
      "Epoch 26/50\n",
      "29/29 [==============================] - 0s 16ms/step - loss: 4.4985 - accuracy: 0.4341\n",
      "Epoch 27/50\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 4.1911 - accuracy: 0.4563\n",
      "Epoch 28/50\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 3.9588 - accuracy: 0.4529\n",
      "Epoch 29/50\n",
      "29/29 [==============================] - 0s 16ms/step - loss: 3.6966 - accuracy: 0.4430\n",
      "Epoch 30/50\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 3.5014 - accuracy: 0.4419\n",
      "Epoch 31/50\n",
      "29/29 [==============================] - 0s 15ms/step - loss: 3.2663 - accuracy: 0.4983\n",
      "Epoch 32/50\n",
      "29/29 [==============================] - 0s 16ms/step - loss: 3.1187 - accuracy: 0.4762\n",
      "Epoch 33/50\n",
      "29/29 [==============================] - 1s 18ms/step - loss: 2.9753 - accuracy: 0.4707\n",
      "Epoch 34/50\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 2.8292 - accuracy: 0.4806\n",
      "Epoch 35/50\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 2.7108 - accuracy: 0.4718\n",
      "Epoch 36/50\n",
      "29/29 [==============================] - 0s 14ms/step - loss: 2.6034 - accuracy: 0.4707\n",
      "Epoch 37/50\n",
      "29/29 [==============================] - 1s 19ms/step - loss: 2.4914 - accuracy: 0.4596\n",
      "Epoch 38/50\n",
      "29/29 [==============================] - 0s 17ms/step - loss: 2.3481 - accuracy: 0.4928\n",
      "Epoch 39/50\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 2.3252 - accuracy: 0.4485\n",
      "Epoch 40/50\n",
      "29/29 [==============================] - 0s 17ms/step - loss: 2.2174 - accuracy: 0.4961\n",
      "Epoch 41/50\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 2.1712 - accuracy: 0.4740\n",
      "Epoch 42/50\n",
      "29/29 [==============================] - 0s 14ms/step - loss: 2.0753 - accuracy: 0.4828\n",
      "Epoch 43/50\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 1.9623 - accuracy: 0.5116\n",
      "Epoch 44/50\n",
      "29/29 [==============================] - 0s 17ms/step - loss: 1.9195 - accuracy: 0.5116\n",
      "Epoch 45/50\n",
      "29/29 [==============================] - 1s 18ms/step - loss: 1.8609 - accuracy: 0.5072\n",
      "Epoch 46/50\n",
      "29/29 [==============================] - 0s 16ms/step - loss: 1.8562 - accuracy: 0.4839\n",
      "Epoch 47/50\n",
      "29/29 [==============================] - 0s 14ms/step - loss: 1.8438 - accuracy: 0.4695\n",
      "Epoch 48/50\n",
      "29/29 [==============================] - 0s 16ms/step - loss: 1.7645 - accuracy: 0.4939\n",
      "Epoch 49/50\n",
      "29/29 [==============================] - 1s 18ms/step - loss: 1.7019 - accuracy: 0.4961\n",
      "Epoch 50/50\n",
      "29/29 [==============================] - 0s 14ms/step - loss: 1.6534 - accuracy: 0.4762\n",
      "Epoch 1/10\n",
      "15/15 [==============================] - 3s 10ms/step - loss: 1.6427 - accuracy: 0.4009\n",
      "Epoch 2/10\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 1.5384 - accuracy: 0.4219\n",
      "Epoch 3/10\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 1.4088 - accuracy: 0.4884\n",
      "Epoch 4/10\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 1.3818 - accuracy: 0.4917\n",
      "Epoch 5/10\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 1.3740 - accuracy: 0.4806\n",
      "Epoch 6/10\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 1.3170 - accuracy: 0.5205\n",
      "Epoch 7/10\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 1.2735 - accuracy: 0.5415\n",
      "Epoch 8/10\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 1.2963 - accuracy: 0.4994\n",
      "Epoch 9/10\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 1.2484 - accuracy: 0.5271\n",
      "Epoch 10/10\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 1.2480 - accuracy: 0.5050\n",
      "Epoch 1/20\n",
      "29/29 [==============================] - 3s 14ms/step - loss: 28.6197 - accuracy: 0.4241\n",
      "Epoch 2/20\n",
      "29/29 [==============================] - 1s 17ms/step - loss: 8.1069 - accuracy: 0.4230\n",
      "Epoch 3/20\n",
      "29/29 [==============================] - 0s 17ms/step - loss: 3.0059 - accuracy: 0.4330\n",
      "Epoch 4/20\n",
      "29/29 [==============================] - 0s 14ms/step - loss: 1.8378 - accuracy: 0.4241\n",
      "Epoch 5/20\n",
      "29/29 [==============================] - 0s 15ms/step - loss: 1.5310 - accuracy: 0.3843\n",
      "Epoch 6/20\n",
      "29/29 [==============================] - 1s 19ms/step - loss: 1.4279 - accuracy: 0.4385\n",
      "Epoch 7/20\n",
      "29/29 [==============================] - 0s 17ms/step - loss: 1.4075 - accuracy: 0.4352\n",
      "Epoch 8/20\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 1.3693 - accuracy: 0.4341\n",
      "Epoch 9/20\n",
      "29/29 [==============================] - 1s 18ms/step - loss: 1.3039 - accuracy: 0.4574\n",
      "Epoch 10/20\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 1.3245 - accuracy: 0.4618\n",
      "Epoch 11/20\n",
      "29/29 [==============================] - 0s 15ms/step - loss: 1.3241 - accuracy: 0.4662\n",
      "Epoch 12/20\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 1.3926 - accuracy: 0.4551\n",
      "Epoch 13/20\n",
      "29/29 [==============================] - 0s 14ms/step - loss: 1.3664 - accuracy: 0.4474\n",
      "Epoch 14/20\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 1.2781 - accuracy: 0.4474\n",
      "Epoch 15/20\n",
      "29/29 [==============================] - 0s 14ms/step - loss: 1.2836 - accuracy: 0.4385\n",
      "Epoch 16/20\n",
      "29/29 [==============================] - 0s 16ms/step - loss: 1.2282 - accuracy: 0.4640\n",
      "Epoch 17/20\n",
      "29/29 [==============================] - 0s 15ms/step - loss: 1.2391 - accuracy: 0.4452\n",
      "Epoch 18/20\n",
      "29/29 [==============================] - 1s 19ms/step - loss: 1.2141 - accuracy: 0.4529\n",
      "Epoch 19/20\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 1.2467 - accuracy: 0.4474\n",
      "Epoch 20/20\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 1.1998 - accuracy: 0.4596\n",
      "Epoch 1/100\n",
      "29/29 [==============================] - 3s 13ms/step - loss: 6.7040 - accuracy: 0.3854\n",
      "Epoch 2/100\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 1.8167 - accuracy: 0.3953\n",
      "Epoch 3/100\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 1.5883 - accuracy: 0.4109\n",
      "Epoch 4/100\n",
      "29/29 [==============================] - 0s 9ms/step - loss: 1.4481 - accuracy: 0.3965\n",
      "Epoch 5/100\n",
      "29/29 [==============================] - 0s 9ms/step - loss: 1.4236 - accuracy: 0.3898\n",
      "Epoch 6/100\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 1.4116 - accuracy: 0.4097\n",
      "Epoch 7/100\n",
      "29/29 [==============================] - 0s 10ms/step - loss: 1.4778 - accuracy: 0.3998\n",
      "Epoch 8/100\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 1.4728 - accuracy: 0.4009\n",
      "Epoch 9/100\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 1.3975 - accuracy: 0.4430\n",
      "Epoch 10/100\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 1.4225 - accuracy: 0.4219\n",
      "Epoch 11/100\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 1.3942 - accuracy: 0.4319\n",
      "Epoch 12/100\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 1.4852 - accuracy: 0.4053\n",
      "Epoch 13/100\n",
      "29/29 [==============================] - 0s 9ms/step - loss: 1.4127 - accuracy: 0.4186\n",
      "Epoch 14/100\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 1.3782 - accuracy: 0.4286\n",
      "Epoch 15/100\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 1.3766 - accuracy: 0.4297\n",
      "Epoch 16/100\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 1.4782 - accuracy: 0.3876\n",
      "Epoch 17/100\n",
      "29/29 [==============================] - 0s 15ms/step - loss: 1.3720 - accuracy: 0.4286\n",
      "Epoch 18/100\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 1.4141 - accuracy: 0.4020\n",
      "Epoch 19/100\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 1.4464 - accuracy: 0.4042\n",
      "Epoch 20/100\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 1.4123 - accuracy: 0.4020\n",
      "Epoch 21/100\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 1.3456 - accuracy: 0.4219\n",
      "Epoch 22/100\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 1.3469 - accuracy: 0.4097\n",
      "Epoch 23/100\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 1.3456 - accuracy: 0.4031\n",
      "Epoch 24/100\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 1.3671 - accuracy: 0.4275\n",
      "Epoch 25/100\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 1.3591 - accuracy: 0.4075\n",
      "Epoch 26/100\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 1.3599 - accuracy: 0.3876\n",
      "Epoch 27/100\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 1.3482 - accuracy: 0.3942\n",
      "Epoch 28/100\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 1.2920 - accuracy: 0.4252\n",
      "Epoch 29/100\n",
      "29/29 [==============================] - 0s 10ms/step - loss: 1.3432 - accuracy: 0.4264\n",
      "Epoch 30/100\n",
      "29/29 [==============================] - 0s 9ms/step - loss: 1.4039 - accuracy: 0.4164\n",
      "Epoch 31/100\n",
      "29/29 [==============================] - 0s 9ms/step - loss: 1.2862 - accuracy: 0.4131\n",
      "Epoch 32/100\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 1.2924 - accuracy: 0.4241\n",
      "Epoch 33/100\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 1.2734 - accuracy: 0.4241\n",
      "Epoch 34/100\n",
      "29/29 [==============================] - 0s 10ms/step - loss: 1.2867 - accuracy: 0.3976\n",
      "Epoch 35/100\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 1.2428 - accuracy: 0.4175\n",
      "Epoch 36/100\n",
      "29/29 [==============================] - 0s 9ms/step - loss: 1.2201 - accuracy: 0.4097\n",
      "Epoch 37/100\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 1.2350 - accuracy: 0.4308\n",
      "Epoch 38/100\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 1.1957 - accuracy: 0.4120\n",
      "Epoch 39/100\n",
      "29/29 [==============================] - 0s 9ms/step - loss: 1.2613 - accuracy: 0.4175\n",
      "Epoch 40/100\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 1.2837 - accuracy: 0.4363\n",
      "Epoch 41/100\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 1.2786 - accuracy: 0.4186\n",
      "Epoch 42/100\n",
      "29/29 [==============================] - 0s 9ms/step - loss: 1.2794 - accuracy: 0.3732\n",
      "Epoch 43/100\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 1.2624 - accuracy: 0.3821\n",
      "Epoch 44/100\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 1.2414 - accuracy: 0.4408\n",
      "Epoch 45/100\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 1.2383 - accuracy: 0.4064\n",
      "Epoch 46/100\n",
      "29/29 [==============================] - 0s 9ms/step - loss: 1.2602 - accuracy: 0.4241\n",
      "Epoch 47/100\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 1.2271 - accuracy: 0.4507\n",
      "Epoch 48/100\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 1.2296 - accuracy: 0.4197\n",
      "Epoch 49/100\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 1.2829 - accuracy: 0.4164\n",
      "Epoch 50/100\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 1.2337 - accuracy: 0.4430\n",
      "Epoch 51/100\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 1.2408 - accuracy: 0.4341\n",
      "Epoch 52/100\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 1.2345 - accuracy: 0.4053\n",
      "Epoch 53/100\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 1.1843 - accuracy: 0.4485\n",
      "Epoch 54/100\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 1.1866 - accuracy: 0.4297\n",
      "Epoch 55/100\n",
      "29/29 [==============================] - 0s 10ms/step - loss: 1.1919 - accuracy: 0.4408\n",
      "Epoch 56/100\n",
      "29/29 [==============================] - 0s 10ms/step - loss: 1.1770 - accuracy: 0.4352\n",
      "Epoch 57/100\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 1.1687 - accuracy: 0.4042\n",
      "Epoch 58/100\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 1.2032 - accuracy: 0.4430\n",
      "Epoch 59/100\n",
      "29/29 [==============================] - 0s 9ms/step - loss: 1.2237 - accuracy: 0.4264\n",
      "Epoch 60/100\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 1.1630 - accuracy: 0.4175\n",
      "Epoch 61/100\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 1.2279 - accuracy: 0.4352\n",
      "Epoch 62/100\n",
      "29/29 [==============================] - 1s 17ms/step - loss: 1.1939 - accuracy: 0.4252\n",
      "Epoch 63/100\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 1.1887 - accuracy: 0.4219\n",
      "Epoch 64/100\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 1.1769 - accuracy: 0.4430\n",
      "Epoch 65/100\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 1.1808 - accuracy: 0.4563\n",
      "Epoch 66/100\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 1.1445 - accuracy: 0.4374\n",
      "Epoch 67/100\n",
      "29/29 [==============================] - 0s 9ms/step - loss: 1.1375 - accuracy: 0.4385\n",
      "Epoch 68/100\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 1.1191 - accuracy: 0.4441\n",
      "Epoch 69/100\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 1.1612 - accuracy: 0.4109\n",
      "Epoch 70/100\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 1.1571 - accuracy: 0.4264\n",
      "Epoch 71/100\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 1.1409 - accuracy: 0.4186\n",
      "Epoch 72/100\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 1.1112 - accuracy: 0.4441\n",
      "Epoch 73/100\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 1.1455 - accuracy: 0.4264\n",
      "Epoch 74/100\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 1.1013 - accuracy: 0.4396\n",
      "Epoch 75/100\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 1.1191 - accuracy: 0.4053\n",
      "Epoch 76/100\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 1.1122 - accuracy: 0.4585\n",
      "Epoch 77/100\n",
      "29/29 [==============================] - 0s 9ms/step - loss: 1.1024 - accuracy: 0.4740\n",
      "Epoch 78/100\n",
      "29/29 [==============================] - 0s 9ms/step - loss: 1.1358 - accuracy: 0.4396\n",
      "Epoch 79/100\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 1.1357 - accuracy: 0.4275\n",
      "Epoch 80/100\n",
      "29/29 [==============================] - 0s 9ms/step - loss: 1.1115 - accuracy: 0.4374\n",
      "Epoch 81/100\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 1.1069 - accuracy: 0.4551\n",
      "Epoch 82/100\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 1.0995 - accuracy: 0.4607\n",
      "Epoch 83/100\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 1.1081 - accuracy: 0.4264\n",
      "Epoch 84/100\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 1.1033 - accuracy: 0.4363\n",
      "Epoch 85/100\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 1.0842 - accuracy: 0.4463\n",
      "Epoch 86/100\n",
      "29/29 [==============================] - 0s 10ms/step - loss: 1.1174 - accuracy: 0.4297\n",
      "Epoch 87/100\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 1.1006 - accuracy: 0.4563\n",
      "Epoch 88/100\n",
      "29/29 [==============================] - 0s 10ms/step - loss: 1.1078 - accuracy: 0.4396\n",
      "Epoch 89/100\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 1.0983 - accuracy: 0.4452\n",
      "Epoch 90/100\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 1.0814 - accuracy: 0.4452\n",
      "Epoch 91/100\n",
      "29/29 [==============================] - 0s 9ms/step - loss: 1.0796 - accuracy: 0.4430\n",
      "Epoch 92/100\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 1.0774 - accuracy: 0.4385\n",
      "Epoch 93/100\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 1.0636 - accuracy: 0.4551\n",
      "Epoch 94/100\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 1.0828 - accuracy: 0.4452\n",
      "Epoch 95/100\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 1.0701 - accuracy: 0.4230\n",
      "Epoch 96/100\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 1.0956 - accuracy: 0.4219\n",
      "Epoch 97/100\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 1.0810 - accuracy: 0.4374\n",
      "Epoch 98/100\n",
      "29/29 [==============================] - 0s 17ms/step - loss: 1.0635 - accuracy: 0.4463\n",
      "Epoch 99/100\n",
      "29/29 [==============================] - 0s 10ms/step - loss: 1.0592 - accuracy: 0.4607\n",
      "Epoch 100/100\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 1.0658 - accuracy: 0.4529\n",
      "Epoch 1/20\n",
      "15/15 [==============================] - 4s 22ms/step - loss: 12.3548 - accuracy: 0.3367\n",
      "Epoch 2/20\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 11.9332 - accuracy: 0.3721\n",
      "Epoch 3/20\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 11.5538 - accuracy: 0.4120\n",
      "Epoch 4/20\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 11.2191 - accuracy: 0.4208\n",
      "Epoch 5/20\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 10.9774 - accuracy: 0.4164\n",
      "Epoch 6/20\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 10.6536 - accuracy: 0.4330\n",
      "Epoch 7/20\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 10.3503 - accuracy: 0.4452\n",
      "Epoch 8/20\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 10.1157 - accuracy: 0.4540\n",
      "Epoch 9/20\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 9.8612 - accuracy: 0.4252\n",
      "Epoch 10/20\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 9.6327 - accuracy: 0.4330\n",
      "Epoch 11/20\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 9.3936 - accuracy: 0.4430\n",
      "Epoch 12/20\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 9.1484 - accuracy: 0.4662\n",
      "Epoch 13/20\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 8.9603 - accuracy: 0.4408\n",
      "Epoch 14/20\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 8.7734 - accuracy: 0.4286\n",
      "Epoch 15/20\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 8.5629 - accuracy: 0.4585\n",
      "Epoch 16/20\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 8.2935 - accuracy: 0.4618\n",
      "Epoch 17/20\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 8.1642 - accuracy: 0.4563\n",
      "Epoch 18/20\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 7.9603 - accuracy: 0.4241\n",
      "Epoch 19/20\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 7.7618 - accuracy: 0.4596\n",
      "Epoch 20/20\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 7.5583 - accuracy: 0.4574\n"
     ]
    }
   ],
   "source": [
    "with open('conv_classifier/best_seeds.json', 'r') as f:\n",
    "    best_seeds = json.load(f)\n",
    "\n",
    "# 21 Crear y entrenar los modelos con los hiperparámetros y semillas guardados\n",
    "models = []\n",
    "for params_str, seed in best_seeds.items():\n",
    "    params = json.loads(params_str.replace(\"'\", \"\\\"\"))\n",
    "    model = KerasRegressor(build_fn=create_model, random_state=seed, **params)\n",
    "    model.fit(X, y_one_hot)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "29/29 [==============================] - 8s 13ms/step - loss: 43.2721 - accuracy: 0.3953\n",
      "Epoch 2/50\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 38.9668 - accuracy: 0.4352\n",
      "Epoch 3/50\n",
      "29/29 [==============================] - 1s 19ms/step - loss: 35.1119 - accuracy: 0.4075\n",
      "Epoch 4/50\n",
      "29/29 [==============================] - 0s 14ms/step - loss: 31.6139 - accuracy: 0.4109\n",
      "Epoch 5/50\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 28.4705 - accuracy: 0.4341\n",
      "Epoch 6/50\n",
      "29/29 [==============================] - 1s 22ms/step - loss: 25.6703 - accuracy: 0.4585\n",
      "Epoch 7/50\n",
      "29/29 [==============================] - 0s 15ms/step - loss: 23.2328 - accuracy: 0.4385\n",
      "Epoch 8/50\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 20.9785 - accuracy: 0.4341\n",
      "Epoch 9/50\n",
      "29/29 [==============================] - 0s 14ms/step - loss: 18.9250 - accuracy: 0.4297\n",
      "Epoch 10/50\n",
      "29/29 [==============================] - 1s 19ms/step - loss: 17.1654 - accuracy: 0.4485\n",
      "Epoch 11/50\n",
      "29/29 [==============================] - 0s 16ms/step - loss: 15.4941 - accuracy: 0.4640\n",
      "Epoch 12/50\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 14.0830 - accuracy: 0.4330\n",
      "Epoch 13/50\n",
      "29/29 [==============================] - 0s 16ms/step - loss: 12.7858 - accuracy: 0.4551\n",
      "Epoch 14/50\n",
      "29/29 [==============================] - 0s 16ms/step - loss: 11.6151 - accuracy: 0.4718\n",
      "Epoch 15/50\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 10.6191 - accuracy: 0.4529\n",
      "Epoch 16/50\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 9.7203 - accuracy: 0.4496\n",
      "Epoch 17/50\n",
      "29/29 [==============================] - 1s 18ms/step - loss: 8.9167 - accuracy: 0.4408\n",
      "Epoch 18/50\n",
      "29/29 [==============================] - 0s 16ms/step - loss: 8.1843 - accuracy: 0.4385\n",
      "Epoch 19/50\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 7.4983 - accuracy: 0.4441\n",
      "Epoch 20/50\n",
      "29/29 [==============================] - 0s 15ms/step - loss: 6.9075 - accuracy: 0.4551\n",
      "Epoch 21/50\n",
      "29/29 [==============================] - 1s 17ms/step - loss: 6.3863 - accuracy: 0.4607\n",
      "Epoch 22/50\n",
      "29/29 [==============================] - 0s 15ms/step - loss: 5.8900 - accuracy: 0.4596\n",
      "Epoch 23/50\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 5.4532 - accuracy: 0.4806\n",
      "Epoch 24/50\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 5.0971 - accuracy: 0.4729\n",
      "Epoch 25/50\n",
      "29/29 [==============================] - 1s 19ms/step - loss: 4.7442 - accuracy: 0.4518\n",
      "Epoch 26/50\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 4.4985 - accuracy: 0.4341\n",
      "Epoch 27/50\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 4.1911 - accuracy: 0.4563\n",
      "Epoch 28/50\n",
      "29/29 [==============================] - 0s 17ms/step - loss: 3.9588 - accuracy: 0.4529\n",
      "Epoch 29/50\n",
      "29/29 [==============================] - 1s 18ms/step - loss: 3.6966 - accuracy: 0.4430\n",
      "Epoch 30/50\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 3.5014 - accuracy: 0.4419\n",
      "Epoch 31/50\n",
      "29/29 [==============================] - 0s 16ms/step - loss: 3.2663 - accuracy: 0.4983\n",
      "Epoch 32/50\n",
      "29/29 [==============================] - 0s 17ms/step - loss: 3.1187 - accuracy: 0.4762\n",
      "Epoch 33/50\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 2.9753 - accuracy: 0.4707\n",
      "Epoch 34/50\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 2.8292 - accuracy: 0.4806\n",
      "Epoch 35/50\n",
      "29/29 [==============================] - 0s 16ms/step - loss: 2.7108 - accuracy: 0.4718\n",
      "Epoch 36/50\n",
      "29/29 [==============================] - 1s 17ms/step - loss: 2.6034 - accuracy: 0.4707\n",
      "Epoch 37/50\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 2.4914 - accuracy: 0.4596\n",
      "Epoch 38/50\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 2.3481 - accuracy: 0.4928\n",
      "Epoch 39/50\n",
      "29/29 [==============================] - 1s 18ms/step - loss: 2.3252 - accuracy: 0.4485\n",
      "Epoch 40/50\n",
      "29/29 [==============================] - 0s 17ms/step - loss: 2.2174 - accuracy: 0.4961\n",
      "Epoch 41/50\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 2.1712 - accuracy: 0.4740\n",
      "Epoch 42/50\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 2.0753 - accuracy: 0.4828\n",
      "Epoch 43/50\n",
      "29/29 [==============================] - 1s 17ms/step - loss: 1.9623 - accuracy: 0.5116\n",
      "Epoch 44/50\n",
      "29/29 [==============================] - 0s 14ms/step - loss: 1.9195 - accuracy: 0.5116\n",
      "Epoch 45/50\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 1.8609 - accuracy: 0.5072\n",
      "Epoch 46/50\n",
      "29/29 [==============================] - 0s 17ms/step - loss: 1.8562 - accuracy: 0.4839\n",
      "Epoch 47/50\n",
      "29/29 [==============================] - 1s 19ms/step - loss: 1.8438 - accuracy: 0.4695\n",
      "Epoch 48/50\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 1.7645 - accuracy: 0.4939\n",
      "Epoch 49/50\n",
      "29/29 [==============================] - 0s 15ms/step - loss: 1.7019 - accuracy: 0.4961\n",
      "Epoch 50/50\n",
      "29/29 [==============================] - 1s 17ms/step - loss: 1.6534 - accuracy: 0.4762\n",
      "Epoch 1/10\n",
      "15/15 [==============================] - 3s 20ms/step - loss: 1.6427 - accuracy: 0.4009\n",
      "Epoch 2/10\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.5384 - accuracy: 0.4219\n",
      "Epoch 3/10\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 1.4088 - accuracy: 0.4884\n",
      "Epoch 4/10\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 1.3818 - accuracy: 0.4917\n",
      "Epoch 5/10\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 1.3740 - accuracy: 0.4806\n",
      "Epoch 6/10\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 1.3170 - accuracy: 0.5205\n",
      "Epoch 7/10\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 1.2735 - accuracy: 0.5415\n",
      "Epoch 8/10\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 1.2963 - accuracy: 0.4994\n",
      "Epoch 9/10\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 1.2484 - accuracy: 0.5271\n",
      "Epoch 10/10\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 1.2480 - accuracy: 0.5050\n",
      "Epoch 1/20\n",
      "29/29 [==============================] - 3s 15ms/step - loss: 28.6197 - accuracy: 0.4241\n",
      "Epoch 2/20\n",
      "29/29 [==============================] - 1s 19ms/step - loss: 8.1069 - accuracy: 0.4230\n",
      "Epoch 3/20\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 3.0059 - accuracy: 0.4330\n",
      "Epoch 4/20\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 1.8378 - accuracy: 0.4241\n",
      "Epoch 5/20\n",
      "29/29 [==============================] - 1s 18ms/step - loss: 1.5310 - accuracy: 0.3843\n",
      "Epoch 6/20\n",
      "29/29 [==============================] - 0s 15ms/step - loss: 1.4279 - accuracy: 0.4385\n",
      "Epoch 7/20\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 1.4075 - accuracy: 0.4352\n",
      "Epoch 8/20\n",
      "29/29 [==============================] - 0s 17ms/step - loss: 1.3693 - accuracy: 0.4341\n",
      "Epoch 9/20\n",
      "29/29 [==============================] - 1s 19ms/step - loss: 1.3039 - accuracy: 0.4574\n",
      "Epoch 10/20\n",
      "29/29 [==============================] - 0s 14ms/step - loss: 1.3245 - accuracy: 0.4618\n",
      "Epoch 11/20\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 1.3241 - accuracy: 0.4662\n",
      "Epoch 12/20\n",
      "29/29 [==============================] - 0s 15ms/step - loss: 1.3926 - accuracy: 0.4551\n",
      "Epoch 13/20\n",
      "29/29 [==============================] - 1s 18ms/step - loss: 1.3664 - accuracy: 0.4474\n",
      "Epoch 14/20\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 1.2781 - accuracy: 0.4474\n",
      "Epoch 15/20\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 1.2836 - accuracy: 0.4385\n",
      "Epoch 16/20\n",
      "29/29 [==============================] - 1s 18ms/step - loss: 1.2282 - accuracy: 0.4640\n",
      "Epoch 17/20\n",
      "29/29 [==============================] - 1s 17ms/step - loss: 1.2391 - accuracy: 0.4452\n",
      "Epoch 18/20\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 1.2141 - accuracy: 0.4529\n",
      "Epoch 19/20\n",
      "29/29 [==============================] - 0s 14ms/step - loss: 1.2467 - accuracy: 0.4474\n",
      "Epoch 20/20\n",
      "29/29 [==============================] - 1s 18ms/step - loss: 1.1998 - accuracy: 0.4596\n",
      "Epoch 1/100\n",
      "29/29 [==============================] - 3s 12ms/step - loss: 6.7040 - accuracy: 0.3854\n",
      "Epoch 2/100\n",
      "29/29 [==============================] - 0s 9ms/step - loss: 1.8167 - accuracy: 0.3953\n",
      "Epoch 3/100\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 1.5883 - accuracy: 0.4109\n",
      "Epoch 4/100\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 1.4481 - accuracy: 0.3965\n",
      "Epoch 5/100\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 1.4236 - accuracy: 0.3898\n",
      "Epoch 6/100\n",
      "29/29 [==============================] - 0s 10ms/step - loss: 1.4116 - accuracy: 0.4097\n",
      "Epoch 7/100\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 1.4778 - accuracy: 0.3998\n",
      "Epoch 8/100\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 1.4728 - accuracy: 0.4009\n",
      "Epoch 9/100\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 1.3975 - accuracy: 0.4430\n",
      "Epoch 10/100\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 1.4225 - accuracy: 0.4219\n",
      "Epoch 11/100\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 1.3942 - accuracy: 0.4319\n",
      "Epoch 12/100\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 1.4852 - accuracy: 0.4053\n",
      "Epoch 13/100\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 1.4127 - accuracy: 0.4186\n",
      "Epoch 14/100\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 1.3782 - accuracy: 0.4286\n",
      "Epoch 15/100\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 1.3766 - accuracy: 0.4297\n",
      "Epoch 16/100\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 1.4782 - accuracy: 0.3876\n",
      "Epoch 17/100\n",
      "29/29 [==============================] - 0s 10ms/step - loss: 1.3720 - accuracy: 0.4286\n",
      "Epoch 18/100\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 1.4141 - accuracy: 0.4020\n",
      "Epoch 19/100\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 1.4464 - accuracy: 0.4042\n",
      "Epoch 20/100\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 1.4123 - accuracy: 0.4020\n",
      "Epoch 21/100\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 1.3456 - accuracy: 0.4219\n",
      "Epoch 22/100\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 1.3469 - accuracy: 0.4097\n",
      "Epoch 23/100\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 1.3456 - accuracy: 0.4031\n",
      "Epoch 24/100\n",
      "29/29 [==============================] - 0s 10ms/step - loss: 1.3671 - accuracy: 0.4275\n",
      "Epoch 25/100\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 1.3591 - accuracy: 0.4075\n",
      "Epoch 26/100\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 1.3599 - accuracy: 0.3876\n",
      "Epoch 27/100\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 1.3482 - accuracy: 0.3942\n",
      "Epoch 28/100\n",
      "29/29 [==============================] - 0s 10ms/step - loss: 1.2920 - accuracy: 0.4252\n",
      "Epoch 29/100\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 1.3432 - accuracy: 0.4264\n",
      "Epoch 30/100\n",
      "29/29 [==============================] - 0s 9ms/step - loss: 1.4039 - accuracy: 0.4164\n",
      "Epoch 31/100\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 1.2862 - accuracy: 0.4131\n",
      "Epoch 32/100\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 1.2924 - accuracy: 0.4241\n",
      "Epoch 33/100\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 1.2734 - accuracy: 0.4241\n",
      "Epoch 34/100\n",
      "29/29 [==============================] - 0s 14ms/step - loss: 1.2867 - accuracy: 0.3976\n",
      "Epoch 35/100\n",
      "29/29 [==============================] - 1s 18ms/step - loss: 1.2428 - accuracy: 0.4175\n",
      "Epoch 36/100\n",
      "29/29 [==============================] - 1s 19ms/step - loss: 1.2201 - accuracy: 0.4097\n",
      "Epoch 37/100\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 1.2350 - accuracy: 0.4308\n",
      "Epoch 38/100\n",
      "29/29 [==============================] - 1s 18ms/step - loss: 1.1957 - accuracy: 0.4120\n",
      "Epoch 39/100\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 1.2613 - accuracy: 0.4175\n",
      "Epoch 40/100\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 1.2837 - accuracy: 0.4363\n",
      "Epoch 41/100\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 1.2786 - accuracy: 0.4186\n",
      "Epoch 42/100\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 1.2794 - accuracy: 0.3732\n",
      "Epoch 43/100\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 1.2624 - accuracy: 0.3821\n",
      "Epoch 44/100\n",
      "29/29 [==============================] - 0s 9ms/step - loss: 1.2414 - accuracy: 0.4408\n",
      "Epoch 45/100\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 1.2383 - accuracy: 0.4064\n",
      "Epoch 46/100\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 1.2602 - accuracy: 0.4241\n",
      "Epoch 47/100\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 1.2271 - accuracy: 0.4507\n",
      "Epoch 48/100\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 1.2296 - accuracy: 0.4197\n",
      "Epoch 49/100\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 1.2829 - accuracy: 0.4164\n",
      "Epoch 50/100\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 1.2337 - accuracy: 0.4430\n",
      "Epoch 51/100\n",
      "29/29 [==============================] - 0s 9ms/step - loss: 1.2408 - accuracy: 0.4341\n",
      "Epoch 52/100\n",
      "29/29 [==============================] - 0s 10ms/step - loss: 1.2345 - accuracy: 0.4053\n",
      "Epoch 53/100\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 1.1843 - accuracy: 0.4485\n",
      "Epoch 54/100\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 1.1866 - accuracy: 0.4297\n",
      "Epoch 55/100\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 1.1919 - accuracy: 0.4408\n",
      "Epoch 56/100\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 1.1770 - accuracy: 0.4352\n",
      "Epoch 57/100\n",
      "29/29 [==============================] - 0s 10ms/step - loss: 1.1687 - accuracy: 0.4042\n",
      "Epoch 58/100\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 1.2032 - accuracy: 0.4430\n",
      "Epoch 59/100\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 1.2237 - accuracy: 0.4264\n",
      "Epoch 60/100\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 1.1630 - accuracy: 0.4175\n",
      "Epoch 61/100\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 1.2279 - accuracy: 0.4352\n",
      "Epoch 62/100\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 1.1939 - accuracy: 0.4252\n",
      "Epoch 63/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 1.1887 - accuracy: 0.4219\n",
      "Epoch 64/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 1.1769 - accuracy: 0.4430\n",
      "Epoch 65/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 1.1808 - accuracy: 0.4563\n",
      "Epoch 66/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 1.1445 - accuracy: 0.4374\n",
      "Epoch 67/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 1.1375 - accuracy: 0.4385\n",
      "Epoch 68/100\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 1.1191 - accuracy: 0.4441\n",
      "Epoch 69/100\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 1.1612 - accuracy: 0.4109\n",
      "Epoch 70/100\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 1.1571 - accuracy: 0.4264\n",
      "Epoch 71/100\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 1.1409 - accuracy: 0.4186\n",
      "Epoch 72/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 1.1112 - accuracy: 0.4441\n",
      "Epoch 73/100\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 1.1455 - accuracy: 0.4264\n",
      "Epoch 74/100\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 1.1013 - accuracy: 0.4396\n",
      "Epoch 75/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 1.1191 - accuracy: 0.4053\n",
      "Epoch 76/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 1.1122 - accuracy: 0.4585\n",
      "Epoch 77/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 1.1024 - accuracy: 0.4740\n",
      "Epoch 78/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 1.1358 - accuracy: 0.4396\n",
      "Epoch 79/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 1.1357 - accuracy: 0.4275\n",
      "Epoch 80/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 1.1115 - accuracy: 0.4374\n",
      "Epoch 81/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 1.1069 - accuracy: 0.4551\n",
      "Epoch 82/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 1.0995 - accuracy: 0.4607\n",
      "Epoch 83/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 1.1081 - accuracy: 0.4264\n",
      "Epoch 84/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 1.1033 - accuracy: 0.4363\n",
      "Epoch 85/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 1.0842 - accuracy: 0.4463\n",
      "Epoch 86/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 1.1174 - accuracy: 0.4297\n",
      "Epoch 87/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 1.1006 - accuracy: 0.4563\n",
      "Epoch 88/100\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 1.1078 - accuracy: 0.4396\n",
      "Epoch 89/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 1.0983 - accuracy: 0.4452\n",
      "Epoch 90/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 1.0814 - accuracy: 0.4452\n",
      "Epoch 91/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 1.0796 - accuracy: 0.4430\n",
      "Epoch 92/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 1.0774 - accuracy: 0.4385\n",
      "Epoch 93/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 1.0636 - accuracy: 0.4551\n",
      "Epoch 94/100\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 1.0828 - accuracy: 0.4452\n",
      "Epoch 95/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 1.0701 - accuracy: 0.4230\n",
      "Epoch 96/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 1.0956 - accuracy: 0.4219\n",
      "Epoch 97/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 1.0810 - accuracy: 0.4374\n",
      "Epoch 98/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 1.0635 - accuracy: 0.4463\n",
      "Epoch 99/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 1.0592 - accuracy: 0.4607\n",
      "Epoch 100/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 1.0658 - accuracy: 0.4529\n",
      "Epoch 1/20\n",
      "15/15 [==============================] - 2s 6ms/step - loss: 12.3548 - accuracy: 0.3367\n",
      "Epoch 2/20\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 11.9332 - accuracy: 0.3721\n",
      "Epoch 3/20\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.5538 - accuracy: 0.4120\n",
      "Epoch 4/20\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.2191 - accuracy: 0.4208\n",
      "Epoch 5/20\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 10.9774 - accuracy: 0.4164\n",
      "Epoch 6/20\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 10.6536 - accuracy: 0.4330\n",
      "Epoch 7/20\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 10.3503 - accuracy: 0.4452\n",
      "Epoch 8/20\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 10.1157 - accuracy: 0.4540\n",
      "Epoch 9/20\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 9.8612 - accuracy: 0.4252\n",
      "Epoch 10/20\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 9.6327 - accuracy: 0.4330\n",
      "Epoch 11/20\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 9.3936 - accuracy: 0.4430\n",
      "Epoch 12/20\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 9.1484 - accuracy: 0.4662\n",
      "Epoch 13/20\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 8.9603 - accuracy: 0.4408\n",
      "Epoch 14/20\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 8.7734 - accuracy: 0.4286\n",
      "Epoch 15/20\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 8.5629 - accuracy: 0.4585\n",
      "Epoch 16/20\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 8.2935 - accuracy: 0.4618\n",
      "Epoch 17/20\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 8.1642 - accuracy: 0.4563\n",
      "Epoch 18/20\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 7.9603 - accuracy: 0.4241\n",
      "Epoch 19/20\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 7.7618 - accuracy: 0.4596\n",
      "Epoch 20/20\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 7.5583 - accuracy: 0.4574\n"
     ]
    }
   ],
   "source": [
    "ensemble = MultivariableVotingRegressor(models)\n",
    "ensemble.fit(X, y_one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Datos originales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open_time</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Number of trades</th>\n",
       "      <th>Close_BTCUSDT</th>\n",
       "      <th>Volume_BTCUSDT</th>\n",
       "      <th>Number_of_trades_BTCUSDT</th>\n",
       "      <th>Close_ETHUSDT</th>\n",
       "      <th>Volume_ETHUSDT</th>\n",
       "      <th>Number_of_trades_ETHUSDT</th>\n",
       "      <th>Close_BNBUSDT</th>\n",
       "      <th>Volume_BNBUSDT</th>\n",
       "      <th>Number_of_trades_BNBUSDT</th>\n",
       "      <th>SMA_20</th>\n",
       "      <th>EMA_20</th>\n",
       "      <th>Upper_Band</th>\n",
       "      <th>Middle_Band</th>\n",
       "      <th>Lower_Band</th>\n",
       "      <th>RSI</th>\n",
       "      <th>buy_1000x_high_coinbase</th>\n",
       "      <th>sell_1000x_high_coinbase</th>\n",
       "      <th>total_trades_coinbase</th>\n",
       "      <th>Tweets_Utilizados</th>\n",
       "      <th>Tweets_Utilizados_coin</th>\n",
       "      <th>Tweets_Utilizados_referentes</th>\n",
       "      <th>Tweets_Utilizados_whale_alert</th>\n",
       "      <th>Buy_1000x_high</th>\n",
       "      <th>sell_1000x_high</th>\n",
       "      <th>total_trades_binance</th>\n",
       "      <th>Tendencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>903</th>\n",
       "      <td>2024-03-18</td>\n",
       "      <td>10.08</td>\n",
       "      <td>10.46</td>\n",
       "      <td>9.60</td>\n",
       "      <td>9.90</td>\n",
       "      <td>245319.00</td>\n",
       "      <td>67609.99</td>\n",
       "      <td>55691.08</td>\n",
       "      <td>2464515.00</td>\n",
       "      <td>3520.46</td>\n",
       "      <td>570901.29</td>\n",
       "      <td>1906387.00</td>\n",
       "      <td>555.40</td>\n",
       "      <td>2284301.81</td>\n",
       "      <td>994512.00</td>\n",
       "      <td>10.06</td>\n",
       "      <td>9.95</td>\n",
       "      <td>11.86</td>\n",
       "      <td>10.06</td>\n",
       "      <td>8.26</td>\n",
       "      <td>52.48</td>\n",
       "      <td>34.00</td>\n",
       "      <td>43.00</td>\n",
       "      <td>84706.00</td>\n",
       "      <td>696</td>\n",
       "      <td>471</td>\n",
       "      <td>0.00</td>\n",
       "      <td>43.00</td>\n",
       "      <td>343.00</td>\n",
       "      <td>228.00</td>\n",
       "      <td>154000.00</td>\n",
       "      <td>Bajista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>2024-03-19</td>\n",
       "      <td>9.90</td>\n",
       "      <td>9.99</td>\n",
       "      <td>8.60</td>\n",
       "      <td>8.77</td>\n",
       "      <td>341363.00</td>\n",
       "      <td>61937.40</td>\n",
       "      <td>101005.32</td>\n",
       "      <td>3593832.00</td>\n",
       "      <td>3158.64</td>\n",
       "      <td>1049629.69</td>\n",
       "      <td>2647385.00</td>\n",
       "      <td>507.70</td>\n",
       "      <td>2551361.51</td>\n",
       "      <td>1213572.00</td>\n",
       "      <td>10.08</td>\n",
       "      <td>9.84</td>\n",
       "      <td>11.81</td>\n",
       "      <td>10.08</td>\n",
       "      <td>8.35</td>\n",
       "      <td>42.93</td>\n",
       "      <td>120.00</td>\n",
       "      <td>126.00</td>\n",
       "      <td>135180.00</td>\n",
       "      <td>961</td>\n",
       "      <td>509</td>\n",
       "      <td>1.00</td>\n",
       "      <td>56.00</td>\n",
       "      <td>534.00</td>\n",
       "      <td>433.00</td>\n",
       "      <td>221000.00</td>\n",
       "      <td>Bajista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>2024-03-20</td>\n",
       "      <td>8.77</td>\n",
       "      <td>9.57</td>\n",
       "      <td>8.49</td>\n",
       "      <td>9.48</td>\n",
       "      <td>267797.00</td>\n",
       "      <td>67840.51</td>\n",
       "      <td>90420.59</td>\n",
       "      <td>3549793.00</td>\n",
       "      <td>3516.53</td>\n",
       "      <td>1207322.82</td>\n",
       "      <td>2987953.00</td>\n",
       "      <td>556.80</td>\n",
       "      <td>1425296.58</td>\n",
       "      <td>809335.00</td>\n",
       "      <td>10.14</td>\n",
       "      <td>9.80</td>\n",
       "      <td>11.68</td>\n",
       "      <td>10.14</td>\n",
       "      <td>8.60</td>\n",
       "      <td>49.21</td>\n",
       "      <td>185.00</td>\n",
       "      <td>117.00</td>\n",
       "      <td>112997.00</td>\n",
       "      <td>866</td>\n",
       "      <td>555</td>\n",
       "      <td>1.00</td>\n",
       "      <td>40.00</td>\n",
       "      <td>473.00</td>\n",
       "      <td>386.00</td>\n",
       "      <td>171000.00</td>\n",
       "      <td>Alcista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>2024-03-21</td>\n",
       "      <td>9.48</td>\n",
       "      <td>9.58</td>\n",
       "      <td>9.07</td>\n",
       "      <td>9.18</td>\n",
       "      <td>156774.00</td>\n",
       "      <td>65501.27</td>\n",
       "      <td>53357.48</td>\n",
       "      <td>2388390.00</td>\n",
       "      <td>3492.85</td>\n",
       "      <td>602755.21</td>\n",
       "      <td>1791989.00</td>\n",
       "      <td>553.80</td>\n",
       "      <td>953921.37</td>\n",
       "      <td>563996.00</td>\n",
       "      <td>10.17</td>\n",
       "      <td>9.74</td>\n",
       "      <td>11.63</td>\n",
       "      <td>10.17</td>\n",
       "      <td>8.71</td>\n",
       "      <td>46.85</td>\n",
       "      <td>64.00</td>\n",
       "      <td>81.00</td>\n",
       "      <td>66543.00</td>\n",
       "      <td>692</td>\n",
       "      <td>533</td>\n",
       "      <td>0.00</td>\n",
       "      <td>24.00</td>\n",
       "      <td>350.00</td>\n",
       "      <td>290.00</td>\n",
       "      <td>101000.00</td>\n",
       "      <td>Bajista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>2024-03-22</td>\n",
       "      <td>9.18</td>\n",
       "      <td>9.37</td>\n",
       "      <td>8.69</td>\n",
       "      <td>8.94</td>\n",
       "      <td>147578.00</td>\n",
       "      <td>63796.64</td>\n",
       "      <td>51482.38</td>\n",
       "      <td>2492881.00</td>\n",
       "      <td>3336.35</td>\n",
       "      <td>558848.89</td>\n",
       "      <td>1747756.00</td>\n",
       "      <td>553.80</td>\n",
       "      <td>1181298.51</td>\n",
       "      <td>712381.00</td>\n",
       "      <td>10.14</td>\n",
       "      <td>9.67</td>\n",
       "      <td>11.67</td>\n",
       "      <td>10.14</td>\n",
       "      <td>8.62</td>\n",
       "      <td>45.00</td>\n",
       "      <td>57.00</td>\n",
       "      <td>66.00</td>\n",
       "      <td>68616.00</td>\n",
       "      <td>681</td>\n",
       "      <td>546</td>\n",
       "      <td>0.00</td>\n",
       "      <td>41.00</td>\n",
       "      <td>252.00</td>\n",
       "      <td>206.00</td>\n",
       "      <td>92000.00</td>\n",
       "      <td>Bajista</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Open_time  Open  High  Low  Close  Number of trades  Close_BTCUSDT  \\\n",
       "903 2024-03-18 10.08 10.46 9.60   9.90         245319.00       67609.99   \n",
       "904 2024-03-19  9.90  9.99 8.60   8.77         341363.00       61937.40   \n",
       "905 2024-03-20  8.77  9.57 8.49   9.48         267797.00       67840.51   \n",
       "906 2024-03-21  9.48  9.58 9.07   9.18         156774.00       65501.27   \n",
       "907 2024-03-22  9.18  9.37 8.69   8.94         147578.00       63796.64   \n",
       "\n",
       "     Volume_BTCUSDT  Number_of_trades_BTCUSDT  Close_ETHUSDT  Volume_ETHUSDT  \\\n",
       "903        55691.08                2464515.00        3520.46       570901.29   \n",
       "904       101005.32                3593832.00        3158.64      1049629.69   \n",
       "905        90420.59                3549793.00        3516.53      1207322.82   \n",
       "906        53357.48                2388390.00        3492.85       602755.21   \n",
       "907        51482.38                2492881.00        3336.35       558848.89   \n",
       "\n",
       "     Number_of_trades_ETHUSDT  Close_BNBUSDT  Volume_BNBUSDT  \\\n",
       "903                1906387.00         555.40      2284301.81   \n",
       "904                2647385.00         507.70      2551361.51   \n",
       "905                2987953.00         556.80      1425296.58   \n",
       "906                1791989.00         553.80       953921.37   \n",
       "907                1747756.00         553.80      1181298.51   \n",
       "\n",
       "     Number_of_trades_BNBUSDT  SMA_20  EMA_20  Upper_Band  Middle_Band  \\\n",
       "903                 994512.00   10.06    9.95       11.86        10.06   \n",
       "904                1213572.00   10.08    9.84       11.81        10.08   \n",
       "905                 809335.00   10.14    9.80       11.68        10.14   \n",
       "906                 563996.00   10.17    9.74       11.63        10.17   \n",
       "907                 712381.00   10.14    9.67       11.67        10.14   \n",
       "\n",
       "     Lower_Band   RSI  buy_1000x_high_coinbase  sell_1000x_high_coinbase  \\\n",
       "903        8.26 52.48                    34.00                     43.00   \n",
       "904        8.35 42.93                   120.00                    126.00   \n",
       "905        8.60 49.21                   185.00                    117.00   \n",
       "906        8.71 46.85                    64.00                     81.00   \n",
       "907        8.62 45.00                    57.00                     66.00   \n",
       "\n",
       "     total_trades_coinbase  Tweets_Utilizados  Tweets_Utilizados_coin  \\\n",
       "903               84706.00                696                     471   \n",
       "904              135180.00                961                     509   \n",
       "905              112997.00                866                     555   \n",
       "906               66543.00                692                     533   \n",
       "907               68616.00                681                     546   \n",
       "\n",
       "     Tweets_Utilizados_referentes  Tweets_Utilizados_whale_alert  \\\n",
       "903                          0.00                          43.00   \n",
       "904                          1.00                          56.00   \n",
       "905                          1.00                          40.00   \n",
       "906                          0.00                          24.00   \n",
       "907                          0.00                          41.00   \n",
       "\n",
       "     Buy_1000x_high  sell_1000x_high  total_trades_binance Tendencia  \n",
       "903          343.00           228.00             154000.00   Bajista  \n",
       "904          534.00           433.00             221000.00   Bajista  \n",
       "905          473.00           386.00             171000.00   Alcista  \n",
       "906          350.00           290.00             101000.00   Bajista  \n",
       "907          252.00           206.00              92000.00   Bajista  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(clasifier_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 235ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 184ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['Alcista', 'Bajista', 'Lateral'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Bajista', 'Bajista', 'Bajista', 'Alcista', 'Bajista']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_clases = 3 \n",
    "\n",
    "validation_predictions = ensemble.predict(clasifier_validation.drop(columns=[\"Open_time\", \"Tendencia\"]))\n",
    "predicciones_one_hot = to_categorical(validation_predictions, num_classes=n_clases)\n",
    "etiquetas_numericas = np.argmax(predicciones_one_hot, axis=1)\n",
    "categorias_clases = onehot_encoder.categories_[0]\n",
    "nombres_clases = [categorias_clases[indice] for indice in etiquetas_numericas]\n",
    "\n",
    "display(validation_predictions)\n",
    "display(categorias_clases)\n",
    "display(nombres_clases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Datos generados por auto ml con prophet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['Alcista', 'Bajista', 'Lateral'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Alcista', 'Alcista', 'Alcista', 'Alcista', 'Alcista']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "auto_ml_prophet_df = pd.read_csv('auto_timeseries_models_prophet/predicciones.csv')\n",
    "auto_mp_prophet_predictions = ensemble.predict(auto_ml_prophet_df.drop(columns=[\"Open_time\"]))\n",
    "\n",
    "predicciones_one_hot = to_categorical(auto_mp_prophet_predictions, num_classes=n_clases)\n",
    "etiquetas_numericas = np.argmax(predicciones_one_hot, axis=1)\n",
    "categorias_clases = onehot_encoder.categories_[0]\n",
    "nombres_clases = [categorias_clases[indice] for indice in etiquetas_numericas]\n",
    "\n",
    "display(auto_mp_prophet_predictions)\n",
    "display(categorias_clases)\n",
    "display(nombres_clases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Datos generados por auto ml con stats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['Alcista', 'Bajista', 'Lateral'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Alcista', 'Alcista', 'Alcista', 'Alcista', 'Alcista']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "auto_ml_stats_df = pd.read_csv('auto_timeseries_models/predicciones.csv')\n",
    "auto_mp_stats_predictions = ensemble.predict(auto_ml_stats_df.drop(columns=[\"Open_time\"]))\n",
    "\n",
    "predicciones_one_hot = to_categorical(auto_mp_stats_predictions, num_classes=n_clases)\n",
    "etiquetas_numericas = np.argmax(predicciones_one_hot, axis=1)\n",
    "categorias_clases = onehot_encoder.categories_[0]\n",
    "nombres_clases = [categorias_clases[indice] for indice in etiquetas_numericas]\n",
    "\n",
    "display(auto_mp_stats_predictions)\n",
    "display(categorias_clases)\n",
    "display(nombres_clases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Datos generados por auto ml con modelos clasicos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['Alcista', 'Bajista', 'Lateral'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Bajista', 'Bajista', 'Bajista', 'Alcista', 'Alcista']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "auto_ml_df = pd.read_csv('h2o_models/predicciones.csv')\n",
    "auto_mp_predictions = ensemble.predict(auto_ml_df.drop(columns=[\"Open_time\", \"Next_Day_Target\"]))\n",
    "\n",
    "predicciones_one_hot = to_categorical(auto_mp_predictions, num_classes=n_clases)\n",
    "etiquetas_numericas = np.argmax(predicciones_one_hot, axis=1)\n",
    "categorias_clases = onehot_encoder.categories_[0]\n",
    "nombres_clases = [categorias_clases[indice] for indice in etiquetas_numericas]\n",
    "\n",
    "display(auto_mp_predictions)\n",
    "display(categorias_clases)\n",
    "display(nombres_clases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
