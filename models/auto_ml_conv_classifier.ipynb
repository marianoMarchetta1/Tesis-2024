{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam, RMSprop, SGD, AdamW, Nadam\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import log_loss\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from keras.utils import to_categorical\n",
    "from adabound import AdaBound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evito que ciertas columnas se transformen a notacion cientifica en las predicciones\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    'Open_time',\n",
    "    'Open',\n",
    "    'High',\n",
    "    'Low',\n",
    "    # 'Close',\n",
    "    'Number of trades',\n",
    "    # 'Close_BTCUSDT',\n",
    "    'Volume_BTCUSDT',\n",
    "    'Number_of_trades_BTCUSDT',\n",
    "    # 'Close_ETHUSDT',\n",
    "    'Volume_ETHUSDT',\n",
    "    # 'Number_of_trades_ETHUSDT',\n",
    "    # 'Close_BNBUSDT',\n",
    "    'Volume_BNBUSDT',\n",
    "    # 'Number_of_trades_BNBUSDT',\n",
    "    # 'SMA_20',\n",
    "    'EMA_20',\n",
    "    'Upper_Band',\n",
    "    'Middle_Band',\n",
    "    # 'Lower_Band',\n",
    "    'RSI',\n",
    "    # 'buy_1000x_high_coinbase',\n",
    "    # 'sell_1000x_high_coinbase',\n",
    "    'total_trades_coinbase',\t\n",
    "    'Tweets_Utilizados',\n",
    "    'Tweets_Utilizados_coin',\n",
    "    # 'Tweets_Utilizados_referentes',\n",
    "    'Tweets_Utilizados_whale_alert',\n",
    "    'Buy_1000x_high',\n",
    "    'sell_1000x_high',\n",
    "    # 'total_trades_binance'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Armado y entrenamiento de un clasificador a partir de los datos originales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelo convolucional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_dataset = pd.read_csv('/Users/mmarchetta/Desktop/Tesis-2024/data-visualization/final_dataset.csv') \n",
    "classifier_dataset = complete_dataset[columns]\n",
    "classifier_dataset['Open_time'] = pd.to_datetime(classifier_dataset['Open_time'])\n",
    "classifier_dataset['Tendencia'] = complete_dataset['Tendencia']\n",
    "\n",
    "clasifier_validation = classifier_dataset[-10:]\n",
    "classifier_dataset = classifier_dataset[:-10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open_time</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Number of trades</th>\n",
       "      <th>Volume_BTCUSDT</th>\n",
       "      <th>Number_of_trades_BTCUSDT</th>\n",
       "      <th>Volume_ETHUSDT</th>\n",
       "      <th>Volume_BNBUSDT</th>\n",
       "      <th>EMA_20</th>\n",
       "      <th>Upper_Band</th>\n",
       "      <th>Middle_Band</th>\n",
       "      <th>RSI</th>\n",
       "      <th>total_trades_coinbase</th>\n",
       "      <th>Tweets_Utilizados</th>\n",
       "      <th>Tweets_Utilizados_coin</th>\n",
       "      <th>Tweets_Utilizados_whale_alert</th>\n",
       "      <th>Buy_1000x_high</th>\n",
       "      <th>sell_1000x_high</th>\n",
       "      <th>Tendencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>941</th>\n",
       "      <td>2024-04-25</td>\n",
       "      <td>6.93</td>\n",
       "      <td>7.00</td>\n",
       "      <td>6.70</td>\n",
       "      <td>71088.00</td>\n",
       "      <td>31341.46</td>\n",
       "      <td>1375324.00</td>\n",
       "      <td>352288.55</td>\n",
       "      <td>453745.52</td>\n",
       "      <td>7.45</td>\n",
       "      <td>9.08</td>\n",
       "      <td>7.43</td>\n",
       "      <td>38.83</td>\n",
       "      <td>33468.00</td>\n",
       "      <td>151</td>\n",
       "      <td>114</td>\n",
       "      <td>22.00</td>\n",
       "      <td>242.00</td>\n",
       "      <td>219.00</td>\n",
       "      <td>Lateral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942</th>\n",
       "      <td>2024-04-26</td>\n",
       "      <td>6.86</td>\n",
       "      <td>6.95</td>\n",
       "      <td>6.71</td>\n",
       "      <td>67383.00</td>\n",
       "      <td>27085.19</td>\n",
       "      <td>1025561.00</td>\n",
       "      <td>252522.65</td>\n",
       "      <td>302119.88</td>\n",
       "      <td>7.38</td>\n",
       "      <td>8.94</td>\n",
       "      <td>7.34</td>\n",
       "      <td>37.81</td>\n",
       "      <td>26619.00</td>\n",
       "      <td>117</td>\n",
       "      <td>106</td>\n",
       "      <td>14.00</td>\n",
       "      <td>292.00</td>\n",
       "      <td>324.00</td>\n",
       "      <td>Lateral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>943</th>\n",
       "      <td>2024-04-27</td>\n",
       "      <td>6.76</td>\n",
       "      <td>6.87</td>\n",
       "      <td>6.51</td>\n",
       "      <td>64779.00</td>\n",
       "      <td>20933.06</td>\n",
       "      <td>912422.00</td>\n",
       "      <td>323811.19</td>\n",
       "      <td>268783.91</td>\n",
       "      <td>7.33</td>\n",
       "      <td>8.73</td>\n",
       "      <td>7.24</td>\n",
       "      <td>38.57</td>\n",
       "      <td>25565.00</td>\n",
       "      <td>101</td>\n",
       "      <td>138</td>\n",
       "      <td>7.00</td>\n",
       "      <td>248.00</td>\n",
       "      <td>179.00</td>\n",
       "      <td>Lateral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>944</th>\n",
       "      <td>2024-04-28</td>\n",
       "      <td>6.81</td>\n",
       "      <td>6.95</td>\n",
       "      <td>6.69</td>\n",
       "      <td>43208.00</td>\n",
       "      <td>16949.20</td>\n",
       "      <td>790652.00</td>\n",
       "      <td>304766.01</td>\n",
       "      <td>258059.43</td>\n",
       "      <td>7.27</td>\n",
       "      <td>8.38</td>\n",
       "      <td>7.13</td>\n",
       "      <td>37.66</td>\n",
       "      <td>20954.00</td>\n",
       "      <td>82</td>\n",
       "      <td>106</td>\n",
       "      <td>13.00</td>\n",
       "      <td>173.00</td>\n",
       "      <td>165.00</td>\n",
       "      <td>Lateral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>945</th>\n",
       "      <td>2024-04-29</td>\n",
       "      <td>6.73</td>\n",
       "      <td>6.83</td>\n",
       "      <td>6.47</td>\n",
       "      <td>63006.00</td>\n",
       "      <td>28150.23</td>\n",
       "      <td>1152296.00</td>\n",
       "      <td>421831.29</td>\n",
       "      <td>330474.01</td>\n",
       "      <td>7.20</td>\n",
       "      <td>8.08</td>\n",
       "      <td>7.03</td>\n",
       "      <td>36.02</td>\n",
       "      <td>33959.00</td>\n",
       "      <td>115</td>\n",
       "      <td>125</td>\n",
       "      <td>24.00</td>\n",
       "      <td>260.00</td>\n",
       "      <td>188.00</td>\n",
       "      <td>Bajista</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Open_time  Open  High  Low  Number of trades  Volume_BTCUSDT  \\\n",
       "941 2024-04-25  6.93  7.00 6.70          71088.00        31341.46   \n",
       "942 2024-04-26  6.86  6.95 6.71          67383.00        27085.19   \n",
       "943 2024-04-27  6.76  6.87 6.51          64779.00        20933.06   \n",
       "944 2024-04-28  6.81  6.95 6.69          43208.00        16949.20   \n",
       "945 2024-04-29  6.73  6.83 6.47          63006.00        28150.23   \n",
       "\n",
       "     Number_of_trades_BTCUSDT  Volume_ETHUSDT  Volume_BNBUSDT  EMA_20  \\\n",
       "941                1375324.00       352288.55       453745.52    7.45   \n",
       "942                1025561.00       252522.65       302119.88    7.38   \n",
       "943                 912422.00       323811.19       268783.91    7.33   \n",
       "944                 790652.00       304766.01       258059.43    7.27   \n",
       "945                1152296.00       421831.29       330474.01    7.20   \n",
       "\n",
       "     Upper_Band  Middle_Band   RSI  total_trades_coinbase  Tweets_Utilizados  \\\n",
       "941        9.08         7.43 38.83               33468.00                151   \n",
       "942        8.94         7.34 37.81               26619.00                117   \n",
       "943        8.73         7.24 38.57               25565.00                101   \n",
       "944        8.38         7.13 37.66               20954.00                 82   \n",
       "945        8.08         7.03 36.02               33959.00                115   \n",
       "\n",
       "     Tweets_Utilizados_coin  Tweets_Utilizados_whale_alert  Buy_1000x_high  \\\n",
       "941                     114                          22.00          242.00   \n",
       "942                     106                          14.00          292.00   \n",
       "943                     138                           7.00          248.00   \n",
       "944                     106                          13.00          173.00   \n",
       "945                     125                          24.00          260.00   \n",
       "\n",
       "     sell_1000x_high Tendencia  \n",
       "941           219.00   Lateral  \n",
       "942           324.00   Lateral  \n",
       "943           179.00   Lateral  \n",
       "944           165.00   Lateral  \n",
       "945           188.00   Bajista  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(classifier_dataset.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(946, 20)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = classifier_dataset.drop(columns=[\"Tendencia\", \"Open_time\"])\n",
    "y = classifier_dataset[\"Tendencia\"]\n",
    "\n",
    "y = y.to_numpy().reshape(-1, 1)\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "y_one_hot = onehot_encoder.fit_transform(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DESCOMENTAR PARA NORMALIZACION\n",
    "\n",
    "# scaler = MinMaxScaler()\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_validation = clasifier_validation.drop(columns=[\"Tendencia\", \"Open_time\"])\n",
    "y_validation = clasifier_validation[\"Tendencia\"]\n",
    "\n",
    "X_validation_scaled = scaler.transform(X_validation)\n",
    "y_validation = y_validation.to_numpy().reshape(-1, 1)\n",
    "y_validation_one_hot = onehot_encoder.transform(y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(y_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create the model\n",
    "def create_model(activation, units, dropout, learning_rate, l2_penalty, depth, optimizer='adam'):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(units, kernel_size=3, activation=activation, input_shape=(len(X.columns), 1)))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    for _ in range(depth - 1):\n",
    "        model.add(Conv1D(units, kernel_size=3, activation=activation))\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units, activation=activation, kernel_regularizer=l2(l2_penalty)))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(units=3, activation='softmax'))\n",
    "\n",
    "    if optimizer == 'adam':\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == 'rmsprop':\n",
    "        optimizer = RMSprop(learning_rate=learning_rate)\n",
    "    elif optimizer == 'sgd':\n",
    "        optimizer = SGD(learning_rate=learning_rate)\n",
    "    elif optimizer == 'adamw':\n",
    "        optimizer = AdamW(learning_rate=learning_rate)\n",
    "    # elif optimizer == 'nadam':\n",
    "    #     optimizer = Nadam(learning_rate=learning_rate)\n",
    "    # elif optimizer == 'adabound':\n",
    "    #     optimizer = AdaBound(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=[AUC(name='auc')])\n",
    "    return model\n",
    "\n",
    "\n",
    "classifier = KerasClassifier(build_fn=create_model, verbose=0, activation='relu', units=50, dropout=0.2, learning_rate=0.1, l2_penalty=0.001, depth=2, optimizer='adam')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=0.00001)\n",
    "\n",
    "# Define cross-validation\n",
    "cv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Define parameter space\n",
    "param_space = {\n",
    "    'depth': [2],\n",
    "    'activation': ['relu', 'tanh', 'swish', 'selu'],\n",
    "    'units': [64, 128, 256, 512],\n",
    "    'dropout': [0.1, 0.2, 0.3, 0.4],\n",
    "    'learning_rate': [0.01, 0.001, 0.0001],\n",
    "    'epochs': [10, 20, 30, 50, 100],\n",
    "    'batch_size': [32, 64, 128],\n",
    "    'optimizer': ['adam', 'adamw', 'rmsprop', 'sgd'],\n",
    "    # 'optimizer': ['adam', , 'nadam', 'adabound'],\n",
    "    'l2_penalty': [0.001, 0.01, 0.1]\n",
    "}\n",
    "\n",
    "def categorical_crossentropy_loss(estimator, X_test, y_test):\n",
    "    y_pred = estimator.predict_proba(X_test)\n",
    "    loss = log_loss(y_test, y_pred)\n",
    "    return loss\n",
    "\n",
    "# Perform Bayesian optimization\n",
    "bayes_search = BayesSearchCV(classifier, param_space, scoring=categorical_crossentropy_loss, verbose=0, cv=cv)\n",
    "bayes_result = bayes_search.fit(X_scaled, y_one_hot, callbacks=[early_stopping, reduce_lr])\n",
    "# bayes_result = bayes_search.fit(X, y_one_hot, callbacks=[early_stopping, reduce_lr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 4.5214545894719125\n",
      "Best parameters: OrderedDict([('activation', 'swish'), ('batch_size', 128), ('depth', 2), ('dropout', 0.1), ('epochs', 100), ('l2_penalty', 0.001), ('learning_rate', 0.001), ('optimizer', 'rmsprop'), ('units', 512)])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KerasClassifier(\n",
       "\tmodel=None\n",
       "\tbuild_fn=&lt;function create_model at 0x7f83dc53ff70&gt;\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=rmsprop\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=128\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=0\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=100\n",
       "\tactivation=swish\n",
       "\tunits=512\n",
       "\tdropout=0.1\n",
       "\tlearning_rate=0.001\n",
       "\tl2_penalty=0.001\n",
       "\tdepth=2\n",
       "\tclass_weight=None\n",
       ")</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KerasClassifier</label><div class=\"sk-toggleable__content\"><pre>KerasClassifier(\n",
       "\tmodel=None\n",
       "\tbuild_fn=&lt;function create_model at 0x7f83dc53ff70&gt;\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=rmsprop\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=128\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=0\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=100\n",
       "\tactivation=swish\n",
       "\tunits=512\n",
       "\tdropout=0.1\n",
       "\tlearning_rate=0.001\n",
       "\tl2_penalty=0.001\n",
       "\tdepth=2\n",
       "\tclass_weight=None\n",
       ")</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KerasClassifier(\n",
       "\tmodel=None\n",
       "\tbuild_fn=<function create_model at 0x7f83dc53ff70>\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=rmsprop\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=128\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=0\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=100\n",
       "\tactivation=swish\n",
       "\tunits=512\n",
       "\tdropout=0.1\n",
       "\tlearning_rate=0.001\n",
       "\tl2_penalty=0.001\n",
       "\tdepth=2\n",
       "\tclass_weight=None\n",
       ")"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show best results\n",
    "print(\"Best score:\", bayes_result.best_score_)\n",
    "print(\"Best parameters:\", bayes_result.best_params_)\n",
    "\n",
    "# Entrenar el modelo con los mejores hiperparámetros\n",
    "best_model = bayes_result.best_estimator_\n",
    "best_model.fit(X_scaled, y_one_hot)\n",
    "# best_model.fit(X, y_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history = bayes_result.best_estimator_.model_.history\n",
    "\n",
    "plt.plot(history.history['auc'])\n",
    "# plt.plot(history.history['val_auc'])\n",
    "plt.title('Model AUC')\n",
    "plt.ylabel('AUC')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 mejores modelos:\n",
      "Modelo 1\n",
      "Hiperparámetros: OrderedDict([('activation', 'swish'), ('batch_size', 128), ('depth', 2), ('dropout', 0.1), ('epochs', 100), ('l2_penalty', 0.001), ('learning_rate', 0.001), ('optimizer', 'rmsprop'), ('units', 512)])\n",
      "Puntaje: 4.5214545894719125\n",
      "Modelo 2\n",
      "Hiperparámetros: OrderedDict([('activation', 'swish'), ('batch_size', 32), ('depth', 2), ('dropout', 0.2), ('epochs', 100), ('l2_penalty', 0.001), ('learning_rate', 0.001), ('optimizer', 'rmsprop'), ('units', 256)])\n",
      "Puntaje: 4.410993374685522\n",
      "Modelo 3\n",
      "Hiperparámetros: OrderedDict([('activation', 'swish'), ('batch_size', 128), ('depth', 2), ('dropout', 0.1), ('epochs', 100), ('l2_penalty', 0.001), ('learning_rate', 0.001), ('optimizer', 'adamw'), ('units', 512)])\n",
      "Puntaje: 4.328679919882833\n",
      "Modelo 4\n",
      "Hiperparámetros: OrderedDict([('activation', 'swish'), ('batch_size', 128), ('depth', 2), ('dropout', 0.2), ('epochs', 100), ('l2_penalty', 0.001), ('learning_rate', 0.001), ('optimizer', 'rmsprop'), ('units', 512)])\n",
      "Puntaje: 4.1826028046105845\n",
      "Modelo 5\n",
      "Hiperparámetros: OrderedDict([('activation', 'swish'), ('batch_size', 32), ('depth', 2), ('dropout', 0.2), ('epochs', 100), ('l2_penalty', 0.001), ('learning_rate', 0.001), ('optimizer', 'adamw'), ('units', 256)])\n",
      "Puntaje: 4.111566334376233\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Obtener los hiperparámetros y puntajes de los 5 mejores modelos\n",
    "top_n_models = 5\n",
    "best_params_list = []\n",
    "best_scores_list = []\n",
    "\n",
    "if 'rank_test_score' in bayes_search.cv_results_:\n",
    "    sorted_indices = np.argsort(bayes_search.cv_results_['rank_test_score'])\n",
    "\n",
    "    for i in range(min(top_n_models, len(sorted_indices))):\n",
    "        best_params_list.append(bayes_search.cv_results_['params'][sorted_indices[i]])\n",
    "        best_scores_list.append(bayes_search.cv_results_['mean_test_score'][sorted_indices[i]])\n",
    "\n",
    "    # Guardar los hiperparámetros de los 5 mejores modelos en un archivo JSON\n",
    "    with open('conv_classifier/top_5_hyperparameters.json', 'w') as f:\n",
    "        json.dump({'best_params': best_params_list, 'best_scores': best_scores_list}, f)\n",
    "\n",
    "    # O imprimir los hiperparámetros\n",
    "    print(\"Top 5 mejores modelos:\")\n",
    "    for i in range(len(best_params_list)):\n",
    "        print(\"Modelo\", i + 1)\n",
    "        print(\"Hiperparámetros:\", best_params_list[i])\n",
    "        print(\"Puntaje:\", best_scores_list[i])\n",
    "\n",
    "else:\n",
    "    print(\"Error: 'rank_test_score' no encontrado en cv_results_\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Armado del ensamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prime_seeds(n):\n",
    "    seeds = []\n",
    "    num = 70001  # Comenzamos desde el primer número primo mayor que 70000\n",
    "    while len(seeds) < n:\n",
    "        is_prime = True\n",
    "        for i in range(2, int(num**0.5) + 1):\n",
    "            if num % i == 0:\n",
    "                is_prime = False\n",
    "                break\n",
    "        if is_prime:\n",
    "            seeds.append(num)\n",
    "        num += 1\n",
    "    return seeds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clase personalizada para hacer el ensamble, dado que sklearn no provee ninguna clase que permita hacer ensmble\n",
    "## de modelos re regresion multivariados\n",
    "class MultivariableVotingClassifier:\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for model in self.models:\n",
    "            model.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Hacer predicciones con cada modelo\n",
    "        predictions = np.array([model.predict(X) for model in self.models])\n",
    "        \n",
    "        # Calcular la moda de las predicciones\n",
    "        mode_predictions = np.argmax(np.sum(predictions, axis=0), axis=1)\n",
    "        \n",
    "        return mode_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model number 0, seed number 0\n",
      "model number 0, seed number 1\n",
      "model number 0, seed number 2\n",
      "model number 0, seed number 3\n",
      "model number 0, seed number 4\n",
      "model number 0, seed number 5\n",
      "model number 0, seed number 6\n",
      "model number 0, seed number 7\n",
      "model number 0, seed number 8\n",
      "model number 0, seed number 9\n",
      "model number 0, seed number 10\n",
      "model number 0, seed number 11\n",
      "model number 0, seed number 12\n",
      "model number 0, seed number 13\n",
      "model number 0, seed number 14\n",
      "model number 0, seed number 15\n",
      "model number 0, seed number 16\n",
      "model number 0, seed number 17\n",
      "model number 0, seed number 18\n",
      "model number 0, seed number 19\n",
      "model number 1, seed number 0\n",
      "model number 1, seed number 1\n",
      "model number 1, seed number 2\n",
      "model number 1, seed number 3\n",
      "model number 1, seed number 4\n",
      "model number 1, seed number 5\n",
      "model number 1, seed number 6\n",
      "model number 1, seed number 7\n",
      "model number 1, seed number 8\n",
      "model number 1, seed number 9\n",
      "model number 1, seed number 10\n",
      "model number 1, seed number 11\n",
      "model number 1, seed number 12\n",
      "model number 1, seed number 13\n",
      "model number 1, seed number 14\n",
      "model number 1, seed number 15\n",
      "model number 1, seed number 16\n",
      "model number 1, seed number 17\n",
      "model number 1, seed number 18\n",
      "model number 1, seed number 19\n",
      "model number 2, seed number 0\n",
      "model number 2, seed number 1\n",
      "model number 2, seed number 2\n",
      "model number 2, seed number 3\n",
      "model number 2, seed number 4\n",
      "model number 2, seed number 5\n",
      "model number 2, seed number 6\n",
      "model number 2, seed number 7\n",
      "model number 2, seed number 8\n",
      "model number 2, seed number 9\n",
      "model number 2, seed number 10\n",
      "model number 2, seed number 11\n",
      "model number 2, seed number 12\n",
      "model number 2, seed number 13\n",
      "model number 2, seed number 14\n",
      "model number 2, seed number 15\n",
      "model number 2, seed number 16\n",
      "model number 2, seed number 17\n",
      "model number 2, seed number 18\n",
      "model number 2, seed number 19\n",
      "model number 3, seed number 0\n",
      "model number 3, seed number 1\n",
      "model number 3, seed number 2\n",
      "model number 3, seed number 3\n",
      "model number 3, seed number 4\n",
      "model number 3, seed number 5\n",
      "model number 3, seed number 6\n",
      "model number 3, seed number 7\n",
      "model number 3, seed number 8\n",
      "model number 3, seed number 9\n",
      "model number 3, seed number 10\n",
      "model number 3, seed number 11\n",
      "model number 3, seed number 12\n",
      "model number 3, seed number 13\n",
      "model number 3, seed number 14\n",
      "model number 3, seed number 15\n",
      "model number 3, seed number 16\n",
      "model number 3, seed number 17\n",
      "model number 3, seed number 18\n",
      "model number 3, seed number 19\n",
      "model number 4, seed number 0\n",
      "model number 4, seed number 1\n",
      "model number 4, seed number 2\n",
      "model number 4, seed number 3\n",
      "model number 4, seed number 4\n",
      "model number 4, seed number 5\n",
      "model number 4, seed number 6\n",
      "model number 4, seed number 7\n",
      "model number 4, seed number 8\n",
      "model number 4, seed number 9\n",
      "model number 4, seed number 10\n",
      "model number 4, seed number 11\n",
      "model number 4, seed number 12\n",
      "model number 4, seed number 13\n",
      "model number 4, seed number 14\n",
      "model number 4, seed number 15\n",
      "model number 4, seed number 16\n",
      "model number 4, seed number 17\n",
      "model number 4, seed number 18\n",
      "model number 4, seed number 19\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "# Leer los hiperparámetros desde el archivo JSON\n",
    "with open('conv_classifier/top_5_hyperparameters.json', 'r') as f:\n",
    "    top_hyperparameters = json.load(f)\n",
    "\n",
    "prime_seeds = generate_prime_seeds(20)\n",
    "models = []\n",
    "best_seeds= {}\n",
    "\n",
    "# Train models with different seeds for each set of hyperparameters\n",
    "for mode_number, params in enumerate(top_hyperparameters['best_params']):\n",
    "    best_validation_errors = {}\n",
    "    \n",
    "    for seed_number, seed in enumerate(prime_seeds):\n",
    "        model = KerasClassifier(build_fn=create_model, random_state=seed, verbose=0, **params)\n",
    "        model.fit(X_scaled, y_one_hot)\n",
    "        # model.fit(X, y_one_hot)\n",
    "        \n",
    "        train_error = categorical_crossentropy_loss(model, X_scaled, y_one_hot)\n",
    "        # train_error = categorical_crossentropy_loss(model, X, y_one_hot)\n",
    "        \n",
    "        mean_train_error = np.mean(train_error)\n",
    "        \n",
    "        # Update best validation error for this seed\n",
    "        best_validation_errors[seed] = mean_train_error\n",
    "        \n",
    "        print(f\"model number {mode_number}, seed number {seed_number}\")\n",
    "    \n",
    "    # print(\"Best validation errors:\", best_validation_errors)\n",
    "\n",
    "    # Find the best seed for this set of hyperparameters\n",
    "    best_seed_for_params = min(best_validation_errors, key=lambda k: best_validation_errors[k])\n",
    "    best_seeds[str(params)] = best_seed_for_params\n",
    "    \n",
    "    # Create and train the model with the best seed\n",
    "    model = KerasClassifier(build_fn=create_model, random_state=best_seed_for_params, verbose=0, **params)\n",
    "    model.fit(X_scaled, y_one_hot)\n",
    "    # model.fit(X, y_one_hot)\n",
    "    \n",
    "    models.append(model)\n",
    "\n",
    "# Define and train the ensemble model\n",
    "ensemble = MultivariableVotingClassifier(models)\n",
    "ensemble.fit(X_scaled, y_one_hot)\n",
    "# ensemble.fit(X, y_one_hot)\n",
    "\n",
    "# Save the best seeds to a JSON file\n",
    "with open('conv_classifier/best_seeds.json', 'w') as f:\n",
    "    json.dump(best_seeds, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clasificacion con el ensamble sobre las redicciones de los modelos generativos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 3s 53ms/step - loss: 2.8787 - auc: 0.6067\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 1s 76ms/step - loss: 1.9066 - auc: 0.6499\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 1.7757 - auc: 0.7072\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 1.7456 - auc: 0.7107\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 1.6936 - auc: 0.7364\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 1.6719 - auc: 0.7418\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 1.6272 - auc: 0.7623\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 1.6261 - auc: 0.7473\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 1.5958 - auc: 0.7618\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 1.5170 - auc: 0.7915\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 1.5050 - auc: 0.7899\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 1.4540 - auc: 0.8086\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 1.4185 - auc: 0.8207\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 1.4196 - auc: 0.8180\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 1s 74ms/step - loss: 1.4602 - auc: 0.7972\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 1.3848 - auc: 0.8149\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 1.3177 - auc: 0.8419\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 1.2742 - auc: 0.8552\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 1.2574 - auc: 0.8607\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 1.2598 - auc: 0.8524\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 1.2274 - auc: 0.8576\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 1.2509 - auc: 0.8449\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 1.2217 - auc: 0.8531\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 1.1957 - auc: 0.8601\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 1.1779 - auc: 0.8608\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 1s 78ms/step - loss: 1.1643 - auc: 0.8662\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 1.0960 - auc: 0.8855\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 64ms/step - loss: 1.0958 - auc: 0.8831\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 1.0760 - auc: 0.8871\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 1.0728 - auc: 0.8839\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 1s 71ms/step - loss: 1.0361 - auc: 0.8949\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.9734 - auc: 0.9141\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 1.0181 - auc: 0.8949\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 1.0004 - auc: 0.8980\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.9853 - auc: 0.9005\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.9742 - auc: 0.9025\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.9193 - auc: 0.9212\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.8939 - auc: 0.9249\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.8689 - auc: 0.9294\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.8492 - auc: 0.9327\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 0.8586 - auc: 0.9292\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 1s 61ms/step - loss: 0.8492 - auc: 0.9291\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.8375 - auc: 0.9308\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.8454 - auc: 0.9304\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.8398 - auc: 0.9285\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.7865 - auc: 0.9440\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.8278 - auc: 0.9289\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.7805 - auc: 0.9407\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 0.8258 - auc: 0.9291\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.7926 - auc: 0.9357\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.7432 - auc: 0.9486\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 1s 74ms/step - loss: 0.7353 - auc: 0.9480\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 0.6866 - auc: 0.9594\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.7079 - auc: 0.9533\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.6831 - auc: 0.9570\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.6683 - auc: 0.9610\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.6150 - auc: 0.9719\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 0.6806 - auc: 0.9537\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 1s 69ms/step - loss: 0.6835 - auc: 0.9530\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.6676 - auc: 0.9564\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.6651 - auc: 0.9569\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.6047 - auc: 0.9694\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.5716 - auc: 0.9761\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.5397 - auc: 0.9798\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 0.5211 - auc: 0.9829\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.4779 - auc: 0.9889\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.4942 - auc: 0.9836\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 1s 73ms/step - loss: 0.6043 - auc: 0.9639\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.5636 - auc: 0.9708\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.5357 - auc: 0.9772\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.4996 - auc: 0.9825\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.4949 - auc: 0.9831\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.4878 - auc: 0.9829\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.4742 - auc: 0.9847\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.4657 - auc: 0.9849\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.5086 - auc: 0.9781\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.5362 - auc: 0.9732\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.5055 - auc: 0.9784\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 1s 73ms/step - loss: 0.5034 - auc: 0.9796\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 0.4430 - auc: 0.9876\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 0.4490 - auc: 0.9864\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.4138 - auc: 0.9910\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.3816 - auc: 0.9937\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 1s 74ms/step - loss: 0.3886 - auc: 0.9922\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 0.3931 - auc: 0.9896\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.3684 - auc: 0.9940\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.3589 - auc: 0.9943\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.4004 - auc: 0.9896\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.3922 - auc: 0.9907\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.3965 - auc: 0.9896\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.3753 - auc: 0.9919\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.4379 - auc: 0.9832\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.4410 - auc: 0.9835\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.4288 - auc: 0.9857\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 1s 79ms/step - loss: 0.4071 - auc: 0.9882\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.3622 - auc: 0.9933\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 0.3690 - auc: 0.9923\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 0.3512 - auc: 0.9944\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 0.3796 - auc: 0.9910\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 0.3333 - auc: 0.9957\n",
      "Epoch 1/100\n",
      "30/30 [==============================] - 3s 11ms/step - loss: 1.8130 - auc: 0.6039\n",
      "Epoch 2/100\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 1.4647 - auc: 0.6753\n",
      "Epoch 3/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 1.4307 - auc: 0.6861\n",
      "Epoch 4/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 1.3718 - auc: 0.7091\n",
      "Epoch 5/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 1.3885 - auc: 0.7020\n",
      "Epoch 6/100\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 1.3461 - auc: 0.7219\n",
      "Epoch 7/100\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.3136 - auc: 0.7301\n",
      "Epoch 8/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 1.2792 - auc: 0.7372\n",
      "Epoch 9/100\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.2510 - auc: 0.7449\n",
      "Epoch 10/100\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.2190 - auc: 0.7630\n",
      "Epoch 11/100\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.2098 - auc: 0.7623\n",
      "Epoch 12/100\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.2099 - auc: 0.7541\n",
      "Epoch 13/100\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.1533 - auc: 0.7780\n",
      "Epoch 14/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 1.1511 - auc: 0.7782\n",
      "Epoch 15/100\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 1.1250 - auc: 0.7883\n",
      "Epoch 16/100\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 1.1130 - auc: 0.7942\n",
      "Epoch 17/100\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.1064 - auc: 0.7920\n",
      "Epoch 18/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 1.0766 - auc: 0.8059\n",
      "Epoch 19/100\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 1.0582 - auc: 0.8088\n",
      "Epoch 20/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 1.0477 - auc: 0.8113\n",
      "Epoch 21/100\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.0022 - auc: 0.8300\n",
      "Epoch 22/100\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.0143 - auc: 0.8216\n",
      "Epoch 23/100\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.9746 - auc: 0.8377\n",
      "Epoch 24/100\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.9832 - auc: 0.8301\n",
      "Epoch 25/100\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.9556 - auc: 0.8405\n",
      "Epoch 26/100\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.9635 - auc: 0.8364\n",
      "Epoch 27/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.9523 - auc: 0.8385\n",
      "Epoch 28/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.8997 - auc: 0.8607\n",
      "Epoch 29/100\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.9219 - auc: 0.8493\n",
      "Epoch 30/100\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.8847 - auc: 0.8635\n",
      "Epoch 31/100\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.8584 - auc: 0.8726\n",
      "Epoch 32/100\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.8583 - auc: 0.8724\n",
      "Epoch 33/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.8657 - auc: 0.8679\n",
      "Epoch 34/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.8173 - auc: 0.8877\n",
      "Epoch 35/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.8151 - auc: 0.8847\n",
      "Epoch 36/100\n",
      "30/30 [==============================] - 0s 17ms/step - loss: 0.7899 - auc: 0.8943\n",
      "Epoch 37/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.7642 - auc: 0.9017\n",
      "Epoch 38/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.7946 - auc: 0.8892\n",
      "Epoch 39/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.7606 - auc: 0.9032\n",
      "Epoch 40/100\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.7606 - auc: 0.9008\n",
      "Epoch 41/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.7539 - auc: 0.9034\n",
      "Epoch 42/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.7375 - auc: 0.9082\n",
      "Epoch 43/100\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.7356 - auc: 0.9078\n",
      "Epoch 44/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.6982 - auc: 0.9204\n",
      "Epoch 45/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.7279 - auc: 0.9093\n",
      "Epoch 46/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.6519 - auc: 0.9372\n",
      "Epoch 47/100\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.6927 - auc: 0.9214\n",
      "Epoch 48/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.6479 - auc: 0.9342\n",
      "Epoch 49/100\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.6413 - auc: 0.9359\n",
      "Epoch 50/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.6992 - auc: 0.9203\n",
      "Epoch 51/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.6726 - auc: 0.9276\n",
      "Epoch 52/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.6113 - auc: 0.9444\n",
      "Epoch 53/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.6493 - auc: 0.9336\n",
      "Epoch 54/100\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.6195 - auc: 0.9420\n",
      "Epoch 55/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.6020 - auc: 0.9468\n",
      "Epoch 56/100\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.5730 - auc: 0.9539\n",
      "Epoch 57/100\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.5608 - auc: 0.9560\n",
      "Epoch 58/100\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.5622 - auc: 0.9568\n",
      "Epoch 59/100\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.6053 - auc: 0.9444\n",
      "Epoch 60/100\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.4995 - auc: 0.9707\n",
      "Epoch 61/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.5030 - auc: 0.9678\n",
      "Epoch 62/100\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.5475 - auc: 0.9581\n",
      "Epoch 63/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.5218 - auc: 0.9639\n",
      "Epoch 64/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.5230 - auc: 0.9635\n",
      "Epoch 65/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.5176 - auc: 0.9650\n",
      "Epoch 66/100\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.4889 - auc: 0.9704\n",
      "Epoch 67/100\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.5200 - auc: 0.9642\n",
      "Epoch 68/100\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.5013 - auc: 0.9685\n",
      "Epoch 69/100\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.4868 - auc: 0.9712\n",
      "Epoch 70/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.4890 - auc: 0.9707\n",
      "Epoch 71/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.4978 - auc: 0.9699\n",
      "Epoch 72/100\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.4908 - auc: 0.9703\n",
      "Epoch 73/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.4503 - auc: 0.9778\n",
      "Epoch 74/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.4598 - auc: 0.9757\n",
      "Epoch 75/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.4767 - auc: 0.9729\n",
      "Epoch 76/100\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.4913 - auc: 0.9699\n",
      "Epoch 77/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.4654 - auc: 0.9756\n",
      "Epoch 78/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.4336 - auc: 0.9808\n",
      "Epoch 79/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.4349 - auc: 0.9795\n",
      "Epoch 80/100\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.4087 - auc: 0.9842\n",
      "Epoch 81/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.4106 - auc: 0.9832\n",
      "Epoch 82/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.4645 - auc: 0.9748\n",
      "Epoch 83/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.4293 - auc: 0.9810\n",
      "Epoch 84/100\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.4578 - auc: 0.9763\n",
      "Epoch 85/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.4072 - auc: 0.9851\n",
      "Epoch 86/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.3956 - auc: 0.9847\n",
      "Epoch 87/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.4216 - auc: 0.9820\n",
      "Epoch 88/100\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.3921 - auc: 0.9859\n",
      "Epoch 89/100\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.4225 - auc: 0.9808\n",
      "Epoch 90/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.3905 - auc: 0.9872\n",
      "Epoch 91/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.3798 - auc: 0.9875\n",
      "Epoch 92/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.3713 - auc: 0.9889\n",
      "Epoch 93/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.3833 - auc: 0.9881\n",
      "Epoch 94/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.3943 - auc: 0.9861\n",
      "Epoch 95/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.3475 - auc: 0.9917\n",
      "Epoch 96/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.3620 - auc: 0.9896\n",
      "Epoch 97/100\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.3767 - auc: 0.9879\n",
      "Epoch 98/100\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 0.3384 - auc: 0.9916\n",
      "Epoch 99/100\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.3589 - auc: 0.9901\n",
      "Epoch 100/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.3717 - auc: 0.9867\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 3s 65ms/step - loss: 2.8787 - auc: 0.6067\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 1.9066 - auc: 0.6499\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 1.7757 - auc: 0.7072\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 1.7456 - auc: 0.7107\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 1.6936 - auc: 0.7364\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 1.6719 - auc: 0.7418\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 1.6272 - auc: 0.7623\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 1.6261 - auc: 0.7473\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 1.5958 - auc: 0.7618\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 1.5170 - auc: 0.7915\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 1.5050 - auc: 0.7899\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 64ms/step - loss: 1.4540 - auc: 0.8086\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 1.4185 - auc: 0.8207\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 1.4196 - auc: 0.8180\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 1.4602 - auc: 0.7972\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 1.3848 - auc: 0.8149\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 1s 69ms/step - loss: 1.3177 - auc: 0.8419\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 1.2742 - auc: 0.8552\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 1.2574 - auc: 0.8607\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 1.2598 - auc: 0.8524\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 1.2274 - auc: 0.8576\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 1.2509 - auc: 0.8449\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 1.2217 - auc: 0.8531\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 1.1957 - auc: 0.8601\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 1.1779 - auc: 0.8608\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 1.1643 - auc: 0.8662\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 1.0960 - auc: 0.8855\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 1.0958 - auc: 0.8831\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 1.0760 - auc: 0.8871\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 1.0728 - auc: 0.8839\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 1s 69ms/step - loss: 1.0361 - auc: 0.8949\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.9734 - auc: 0.9141\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 1.0181 - auc: 0.8949\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 1s 70ms/step - loss: 1.0004 - auc: 0.8980\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.9853 - auc: 0.9005\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.9742 - auc: 0.9025\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.9193 - auc: 0.9212\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.8939 - auc: 0.9249\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.8689 - auc: 0.9294\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.8492 - auc: 0.9327\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.8586 - auc: 0.9292\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.8492 - auc: 0.9291\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.8375 - auc: 0.9308\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 1s 80ms/step - loss: 0.8454 - auc: 0.9304\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 1s 75ms/step - loss: 0.8398 - auc: 0.9285\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.7865 - auc: 0.9440\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.8278 - auc: 0.9289\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.7805 - auc: 0.9407\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 0.8258 - auc: 0.9291\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.7926 - auc: 0.9357\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.7432 - auc: 0.9486\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 0.7353 - auc: 0.9480\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.6866 - auc: 0.9594\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.7079 - auc: 0.9533\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 1s 78ms/step - loss: 0.6831 - auc: 0.9570\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.6683 - auc: 0.9610\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.6150 - auc: 0.9719\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.6806 - auc: 0.9537\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.6835 - auc: 0.9530\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.6676 - auc: 0.9564\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 64ms/step - loss: 0.6651 - auc: 0.9569\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.6047 - auc: 0.9694\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 0.5716 - auc: 0.9761\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 0.5397 - auc: 0.9798\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.5211 - auc: 0.9829\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 0.4779 - auc: 0.9889\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 0.4942 - auc: 0.9836\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.6043 - auc: 0.9639\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 0.5636 - auc: 0.9708\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 0.5357 - auc: 0.9772\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.4996 - auc: 0.9825\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.4949 - auc: 0.9831\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 0.4878 - auc: 0.9829\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 0.4742 - auc: 0.9847\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 0.4657 - auc: 0.9849\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 0.5086 - auc: 0.9781\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 0.5362 - auc: 0.9732\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.5055 - auc: 0.9784\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.5034 - auc: 0.9796\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 0.4430 - auc: 0.9876\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 0.4490 - auc: 0.9864\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 0.4138 - auc: 0.9910\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 0.3816 - auc: 0.9937\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 0.3886 - auc: 0.9922\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.3931 - auc: 0.9896\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.3684 - auc: 0.9940\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 0.3589 - auc: 0.9943\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 0.4004 - auc: 0.9896\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 0.3922 - auc: 0.9907\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 0.3965 - auc: 0.9896\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.3753 - auc: 0.9919\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.4379 - auc: 0.9832\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 0.4410 - auc: 0.9835\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.4288 - auc: 0.9857\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 65ms/step - loss: 0.4071 - auc: 0.9882\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.3622 - auc: 0.9933\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.3690 - auc: 0.9923\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 0.3512 - auc: 0.9944\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.3796 - auc: 0.9910\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.3333 - auc: 0.9957\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 3s 46ms/step - loss: 2.6325 - auc: 0.6079\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 1.9653 - auc: 0.6624\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 1s 75ms/step - loss: 1.8236 - auc: 0.6968\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 1.7270 - auc: 0.7322\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 1.7078 - auc: 0.7364\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 1.7109 - auc: 0.7323\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 1.6818 - auc: 0.7246\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 1.6031 - auc: 0.7682\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 1.6166 - auc: 0.7670\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 1.5473 - auc: 0.7849\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 1.5500 - auc: 0.7802\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 1.5547 - auc: 0.7756\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 1.5168 - auc: 0.7790\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 1.4897 - auc: 0.7849\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 1.4556 - auc: 0.7927\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 1.3927 - auc: 0.8163\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 1s 69ms/step - loss: 1.3623 - auc: 0.8261\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 1.3439 - auc: 0.8303\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 1.3186 - auc: 0.8339\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 1s 71ms/step - loss: 1.3181 - auc: 0.8304\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 1.2936 - auc: 0.8358\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 1.2468 - auc: 0.8523\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 1.2865 - auc: 0.8334\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 1.1861 - auc: 0.8679\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 1.1898 - auc: 0.8638\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 1.1740 - auc: 0.8631\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 1.1532 - auc: 0.8676\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 1.1479 - auc: 0.8695\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 1.1017 - auc: 0.8798\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 1.1175 - auc: 0.8755\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 1.0965 - auc: 0.8754\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 1.0619 - auc: 0.8857\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 1.0689 - auc: 0.8832\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 1.0299 - auc: 0.8936\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 1.0423 - auc: 0.8878\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 0.9966 - auc: 0.9005\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 0.9755 - auc: 0.9051\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 0.9582 - auc: 0.9084\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.9338 - auc: 0.9135\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 0.9115 - auc: 0.9176\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.8902 - auc: 0.9231\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 63ms/step - loss: 0.8540 - auc: 0.9303\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.8770 - auc: 0.9240\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 0.9002 - auc: 0.9163\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 0.8356 - auc: 0.9317\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 0.7875 - auc: 0.9436\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 0.7971 - auc: 0.9415\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.8035 - auc: 0.9376\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.8246 - auc: 0.9308\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 0.7705 - auc: 0.9441\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 0.7742 - auc: 0.9417\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 0.7615 - auc: 0.9454\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.7332 - auc: 0.9498\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.7647 - auc: 0.9416\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 0.7012 - auc: 0.9562\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 0.6863 - auc: 0.9593\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 64ms/step - loss: 0.6821 - auc: 0.9576\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.6485 - auc: 0.9646\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 0.6239 - auc: 0.9701\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 0.6180 - auc: 0.9683\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 0.6425 - auc: 0.9629\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 0.6225 - auc: 0.9674\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 0.7071 - auc: 0.9491\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 0.6208 - auc: 0.9666\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 0.5735 - auc: 0.9770\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 0.5840 - auc: 0.9720\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 63ms/step - loss: 0.5321 - auc: 0.9816\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.5448 - auc: 0.9779\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.5463 - auc: 0.9770\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 0.5324 - auc: 0.9794\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 0.5413 - auc: 0.9768\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.5096 - auc: 0.9824\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.4679 - auc: 0.9872\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 0.5103 - auc: 0.9801\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 0.4665 - auc: 0.9875\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.4738 - auc: 0.9847\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.4679 - auc: 0.9854\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.4659 - auc: 0.9851\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.4545 - auc: 0.9859\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.4568 - auc: 0.9860\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.4928 - auc: 0.9813\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 1s 76ms/step - loss: 0.4632 - auc: 0.9856\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.4465 - auc: 0.9882\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.4381 - auc: 0.9880\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.3984 - auc: 0.9928\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.3991 - auc: 0.9916\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.4056 - auc: 0.9903\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.3847 - auc: 0.9924\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 1s 68ms/step - loss: 0.4105 - auc: 0.9888\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.3784 - auc: 0.9932\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 1s 80ms/step - loss: 0.3902 - auc: 0.9917\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 0.4295 - auc: 0.9855\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.4059 - auc: 0.9894\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.4041 - auc: 0.9901\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.3873 - auc: 0.9921\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.3813 - auc: 0.9925\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.3506 - auc: 0.9950\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 0.3487 - auc: 0.9953\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 1s 78ms/step - loss: 0.3073 - auc: 0.9985\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.3024 - auc: 0.9981\n",
      "Epoch 1/100\n",
      "30/30 [==============================] - 3s 10ms/step - loss: 1.8130 - auc: 0.6039\n",
      "Epoch 2/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 1.4647 - auc: 0.6753\n",
      "Epoch 3/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 1.4307 - auc: 0.6861\n",
      "Epoch 4/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 1.3718 - auc: 0.7091\n",
      "Epoch 5/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 1.3885 - auc: 0.7020\n",
      "Epoch 6/100\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 1.3461 - auc: 0.7219\n",
      "Epoch 7/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 1.3136 - auc: 0.7301\n",
      "Epoch 8/100\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.2792 - auc: 0.7372\n",
      "Epoch 9/100\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.2510 - auc: 0.7449\n",
      "Epoch 10/100\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.2190 - auc: 0.7630\n",
      "Epoch 11/100\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 1.2098 - auc: 0.7623\n",
      "Epoch 12/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 1.2099 - auc: 0.7541\n",
      "Epoch 13/100\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.1533 - auc: 0.7780\n",
      "Epoch 14/100\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.1511 - auc: 0.7782\n",
      "Epoch 15/100\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.1250 - auc: 0.7883\n",
      "Epoch 16/100\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.1130 - auc: 0.7942\n",
      "Epoch 17/100\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.1064 - auc: 0.7920\n",
      "Epoch 18/100\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.0766 - auc: 0.8059\n",
      "Epoch 19/100\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.0582 - auc: 0.8088\n",
      "Epoch 20/100\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.0477 - auc: 0.8113\n",
      "Epoch 21/100\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.0022 - auc: 0.8300\n",
      "Epoch 22/100\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 1.0143 - auc: 0.8216\n",
      "Epoch 23/100\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.9746 - auc: 0.8377\n",
      "Epoch 24/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.9832 - auc: 0.8301\n",
      "Epoch 25/100\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.9556 - auc: 0.8405\n",
      "Epoch 26/100\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.9635 - auc: 0.8364\n",
      "Epoch 27/100\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.9523 - auc: 0.8385\n",
      "Epoch 28/100\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.8997 - auc: 0.8607\n",
      "Epoch 29/100\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.9219 - auc: 0.8493\n",
      "Epoch 30/100\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.8847 - auc: 0.8635\n",
      "Epoch 31/100\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.8584 - auc: 0.8726\n",
      "Epoch 32/100\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.8583 - auc: 0.8724\n",
      "Epoch 33/100\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.8657 - auc: 0.8679\n",
      "Epoch 34/100\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.8173 - auc: 0.8877\n",
      "Epoch 35/100\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.8151 - auc: 0.8847\n",
      "Epoch 36/100\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.7899 - auc: 0.8943\n",
      "Epoch 37/100\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.7642 - auc: 0.9017\n",
      "Epoch 38/100\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.7946 - auc: 0.8892\n",
      "Epoch 39/100\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.7606 - auc: 0.9032\n",
      "Epoch 40/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.7606 - auc: 0.9008\n",
      "Epoch 41/100\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.7539 - auc: 0.9034\n",
      "Epoch 42/100\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.7375 - auc: 0.9082\n",
      "Epoch 43/100\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.7356 - auc: 0.9078\n",
      "Epoch 44/100\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.6982 - auc: 0.9204\n",
      "Epoch 45/100\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.7279 - auc: 0.9093\n",
      "Epoch 46/100\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.6519 - auc: 0.9372\n",
      "Epoch 47/100\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.6927 - auc: 0.9214\n",
      "Epoch 48/100\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.6479 - auc: 0.9342\n",
      "Epoch 49/100\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.6413 - auc: 0.9359\n",
      "Epoch 50/100\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.6992 - auc: 0.9203\n",
      "Epoch 51/100\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.6726 - auc: 0.9276\n",
      "Epoch 52/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.6113 - auc: 0.9444\n",
      "Epoch 53/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.6493 - auc: 0.9336\n",
      "Epoch 54/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.6195 - auc: 0.9420\n",
      "Epoch 55/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.6020 - auc: 0.9468\n",
      "Epoch 56/100\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.5730 - auc: 0.9539\n",
      "Epoch 57/100\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.5608 - auc: 0.9560\n",
      "Epoch 58/100\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.5622 - auc: 0.9568\n",
      "Epoch 59/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.6053 - auc: 0.9444\n",
      "Epoch 60/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.4995 - auc: 0.9707\n",
      "Epoch 61/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.5030 - auc: 0.9678\n",
      "Epoch 62/100\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.5475 - auc: 0.9581\n",
      "Epoch 63/100\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.5218 - auc: 0.9639\n",
      "Epoch 64/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.5230 - auc: 0.9635\n",
      "Epoch 65/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.5176 - auc: 0.9650\n",
      "Epoch 66/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.4889 - auc: 0.9704\n",
      "Epoch 67/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.5200 - auc: 0.9642\n",
      "Epoch 68/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.5013 - auc: 0.9685\n",
      "Epoch 69/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.4868 - auc: 0.9712\n",
      "Epoch 70/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.4890 - auc: 0.9707\n",
      "Epoch 71/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.4978 - auc: 0.9699\n",
      "Epoch 72/100\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.4908 - auc: 0.9703\n",
      "Epoch 73/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.4503 - auc: 0.9778\n",
      "Epoch 74/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.4598 - auc: 0.9757\n",
      "Epoch 75/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.4767 - auc: 0.9729\n",
      "Epoch 76/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.4913 - auc: 0.9699\n",
      "Epoch 77/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.4654 - auc: 0.9756\n",
      "Epoch 78/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.4336 - auc: 0.9808\n",
      "Epoch 79/100\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.4349 - auc: 0.9795\n",
      "Epoch 80/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.4087 - auc: 0.9842\n",
      "Epoch 81/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.4106 - auc: 0.9832\n",
      "Epoch 82/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.4645 - auc: 0.9748\n",
      "Epoch 83/100\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.4293 - auc: 0.9810\n",
      "Epoch 84/100\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.4578 - auc: 0.9763\n",
      "Epoch 85/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.4072 - auc: 0.9851\n",
      "Epoch 86/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.3956 - auc: 0.9847\n",
      "Epoch 87/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.4216 - auc: 0.9820\n",
      "Epoch 88/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.3921 - auc: 0.9859\n",
      "Epoch 89/100\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.4225 - auc: 0.9808\n",
      "Epoch 90/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.3905 - auc: 0.9872\n",
      "Epoch 91/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.3798 - auc: 0.9875\n",
      "Epoch 92/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.3713 - auc: 0.9889\n",
      "Epoch 93/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.3833 - auc: 0.9881\n",
      "Epoch 94/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.3943 - auc: 0.9861\n",
      "Epoch 95/100\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.3475 - auc: 0.9917\n",
      "Epoch 96/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.3620 - auc: 0.9896\n",
      "Epoch 97/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.3767 - auc: 0.9879\n",
      "Epoch 98/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.3384 - auc: 0.9916\n",
      "Epoch 99/100\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.3589 - auc: 0.9901\n",
      "Epoch 100/100\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.3717 - auc: 0.9867\n"
     ]
    }
   ],
   "source": [
    "with open('conv_classifier/best_seeds.json', 'r') as f:\n",
    "    best_seeds = json.load(f)\n",
    "\n",
    "# 21 Crear y entrenar los modelos con los hiperparámetros y semillas guardados\n",
    "models = []\n",
    "for params_str, seed in best_seeds.items():\n",
    "    params = json.loads(params_str.replace(\"'\", \"\\\"\"))\n",
    "    model = KerasClassifier(build_fn=create_model, random_state=seed, **params)\n",
    "    model.fit(X_scaled, y_one_hot)\n",
    "    # model.fit(X, y_one_hot)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 3s 56ms/step - loss: 2.8787 - auc: 0.6067\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 1s 84ms/step - loss: 1.9066 - auc: 0.6499\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 1.7757 - auc: 0.7072\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 1.7456 - auc: 0.7107\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 1.6936 - auc: 0.7364\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 1.6719 - auc: 0.7418\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 1.6272 - auc: 0.7623\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 1.6261 - auc: 0.7473\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 1s 55ms/step - loss: 1.5958 - auc: 0.7618\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 1.5170 - auc: 0.7915\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 1.5050 - auc: 0.7899\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 1.4540 - auc: 0.8086\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 1.4185 - auc: 0.8207\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 1.4196 - auc: 0.8180\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 1.4602 - auc: 0.7972\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 1.3848 - auc: 0.8149\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 1.3177 - auc: 0.8419\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 1.2742 - auc: 0.8552\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 1.2574 - auc: 0.8607\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 1.2598 - auc: 0.8524\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 1.2274 - auc: 0.8576\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 1.2509 - auc: 0.8449\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 1s 68ms/step - loss: 1.2217 - auc: 0.8531\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 1.1957 - auc: 0.8601\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 1.1779 - auc: 0.8608\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 1.1643 - auc: 0.8662\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 1.0960 - auc: 0.8855\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 1.0958 - auc: 0.8831\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 1.0760 - auc: 0.8871\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 1.0728 - auc: 0.8839\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 63ms/step - loss: 1.0361 - auc: 0.8949\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.9734 - auc: 0.9141\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 1.0181 - auc: 0.8949\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 1.0004 - auc: 0.8980\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 0.9853 - auc: 0.9005\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 0.9742 - auc: 0.9025\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 0.9193 - auc: 0.9212\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.8939 - auc: 0.9249\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.8689 - auc: 0.9294\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.8492 - auc: 0.9327\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 0.8586 - auc: 0.9292\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 0.8492 - auc: 0.9291\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 0.8375 - auc: 0.9308\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 0.8454 - auc: 0.9304\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.8398 - auc: 0.9285\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 0.7865 - auc: 0.9440\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 0.8278 - auc: 0.9289\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.7805 - auc: 0.9407\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.8258 - auc: 0.9291\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.7926 - auc: 0.9357\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 1s 78ms/step - loss: 0.7432 - auc: 0.9486\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.7353 - auc: 0.9480\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 0.6866 - auc: 0.9594\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.7079 - auc: 0.9533\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.6831 - auc: 0.9570\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 1s 70ms/step - loss: 0.6683 - auc: 0.9610\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 1s 72ms/step - loss: 0.6150 - auc: 0.9719\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 1s 97ms/step - loss: 0.6806 - auc: 0.9537\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 1s 113ms/step - loss: 0.6835 - auc: 0.9530\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 1s 98ms/step - loss: 0.6676 - auc: 0.9564\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 0.6651 - auc: 0.9569\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.6047 - auc: 0.9694\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.5716 - auc: 0.9761\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.5397 - auc: 0.9798\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.5211 - auc: 0.9829\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.4779 - auc: 0.9889\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 0.4942 - auc: 0.9836\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 0.6043 - auc: 0.9639\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.5636 - auc: 0.9708\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.5357 - auc: 0.9772\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.4996 - auc: 0.9825\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 63ms/step - loss: 0.4949 - auc: 0.9831\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.4878 - auc: 0.9829\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.4742 - auc: 0.9847\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 1s 72ms/step - loss: 0.4657 - auc: 0.9849\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.5086 - auc: 0.9781\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 0.5362 - auc: 0.9732\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.5055 - auc: 0.9784\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.5034 - auc: 0.9796\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.4430 - auc: 0.9876\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.4490 - auc: 0.9864\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.4138 - auc: 0.9910\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.3816 - auc: 0.9937\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.3886 - auc: 0.9922\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 1s 73ms/step - loss: 0.3931 - auc: 0.9896\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.3684 - auc: 0.9940\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 0.3589 - auc: 0.9943\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.4004 - auc: 0.9896\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.3922 - auc: 0.9907\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 1s 83ms/step - loss: 0.3965 - auc: 0.9896\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.3753 - auc: 0.9919\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.4379 - auc: 0.9832\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.4410 - auc: 0.9835\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.4288 - auc: 0.9857\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.4071 - auc: 0.9882\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 1s 59ms/step - loss: 0.3622 - auc: 0.9933\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.3690 - auc: 0.9923\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.3512 - auc: 0.9944\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 1s 61ms/step - loss: 0.3796 - auc: 0.9910\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 0.3333 - auc: 0.9957\n",
      "Epoch 1/100\n",
      "30/30 [==============================] - 3s 10ms/step - loss: 1.8130 - auc: 0.6039\n",
      "Epoch 2/100\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 1.4647 - auc: 0.6753\n",
      "Epoch 3/100\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.4307 - auc: 0.6861\n",
      "Epoch 4/100\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.3718 - auc: 0.7091\n",
      "Epoch 5/100\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.3885 - auc: 0.7020\n",
      "Epoch 6/100\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.3461 - auc: 0.7219\n",
      "Epoch 7/100\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 1.3136 - auc: 0.7301\n",
      "Epoch 8/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 1.2792 - auc: 0.7372\n",
      "Epoch 9/100\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.2510 - auc: 0.7449\n",
      "Epoch 10/100\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.2190 - auc: 0.7630\n",
      "Epoch 11/100\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 1.2098 - auc: 0.7623\n",
      "Epoch 12/100\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.2099 - auc: 0.7541\n",
      "Epoch 13/100\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.1533 - auc: 0.7780\n",
      "Epoch 14/100\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.1511 - auc: 0.7782\n",
      "Epoch 15/100\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.1250 - auc: 0.7883\n",
      "Epoch 16/100\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 1.1130 - auc: 0.7942\n",
      "Epoch 17/100\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.1064 - auc: 0.7920\n",
      "Epoch 18/100\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.0766 - auc: 0.8059\n",
      "Epoch 19/100\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.0582 - auc: 0.8088\n",
      "Epoch 20/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 1.0477 - auc: 0.8113\n",
      "Epoch 21/100\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 1.0022 - auc: 0.8300\n",
      "Epoch 22/100\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 1.0143 - auc: 0.8216\n",
      "Epoch 23/100\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.9746 - auc: 0.8377\n",
      "Epoch 24/100\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.9832 - auc: 0.8301\n",
      "Epoch 25/100\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.9556 - auc: 0.8405\n",
      "Epoch 26/100\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.9635 - auc: 0.8364\n",
      "Epoch 27/100\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.9523 - auc: 0.8385\n",
      "Epoch 28/100\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.8997 - auc: 0.8607\n",
      "Epoch 29/100\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.9219 - auc: 0.8493\n",
      "Epoch 30/100\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.8847 - auc: 0.8635\n",
      "Epoch 31/100\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.8584 - auc: 0.8726\n",
      "Epoch 32/100\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.8583 - auc: 0.8724\n",
      "Epoch 33/100\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.8657 - auc: 0.8679\n",
      "Epoch 34/100\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.8173 - auc: 0.8877\n",
      "Epoch 35/100\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.8151 - auc: 0.8847\n",
      "Epoch 36/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.7899 - auc: 0.8943\n",
      "Epoch 37/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.7642 - auc: 0.9017\n",
      "Epoch 38/100\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.7946 - auc: 0.8892\n",
      "Epoch 39/100\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.7606 - auc: 0.9032\n",
      "Epoch 40/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.7606 - auc: 0.9008\n",
      "Epoch 41/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.7539 - auc: 0.9034\n",
      "Epoch 42/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.7375 - auc: 0.9082\n",
      "Epoch 43/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.7356 - auc: 0.9078\n",
      "Epoch 44/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.6982 - auc: 0.9204\n",
      "Epoch 45/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.7279 - auc: 0.9093\n",
      "Epoch 46/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.6519 - auc: 0.9372\n",
      "Epoch 47/100\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.6927 - auc: 0.9214\n",
      "Epoch 48/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.6479 - auc: 0.9342\n",
      "Epoch 49/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.6413 - auc: 0.9359\n",
      "Epoch 50/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.6992 - auc: 0.9203\n",
      "Epoch 51/100\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.6726 - auc: 0.9276\n",
      "Epoch 52/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.6113 - auc: 0.9444\n",
      "Epoch 53/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.6493 - auc: 0.9336\n",
      "Epoch 54/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.6195 - auc: 0.9420\n",
      "Epoch 55/100\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.6020 - auc: 0.9468\n",
      "Epoch 56/100\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.5730 - auc: 0.9539\n",
      "Epoch 57/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.5608 - auc: 0.9560\n",
      "Epoch 58/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.5622 - auc: 0.9568\n",
      "Epoch 59/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.6053 - auc: 0.9444\n",
      "Epoch 60/100\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.4995 - auc: 0.9707\n",
      "Epoch 61/100\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.5030 - auc: 0.9678\n",
      "Epoch 62/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.5475 - auc: 0.9581\n",
      "Epoch 63/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.5218 - auc: 0.9639\n",
      "Epoch 64/100\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.5230 - auc: 0.9635\n",
      "Epoch 65/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.5176 - auc: 0.9650\n",
      "Epoch 66/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.4889 - auc: 0.9704\n",
      "Epoch 67/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.5200 - auc: 0.9642\n",
      "Epoch 68/100\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.5013 - auc: 0.9685\n",
      "Epoch 69/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.4868 - auc: 0.9712\n",
      "Epoch 70/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.4890 - auc: 0.9707\n",
      "Epoch 71/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.4978 - auc: 0.9699\n",
      "Epoch 72/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.4908 - auc: 0.9703\n",
      "Epoch 73/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.4503 - auc: 0.9778\n",
      "Epoch 74/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.4598 - auc: 0.9757\n",
      "Epoch 75/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.4767 - auc: 0.9729\n",
      "Epoch 76/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.4913 - auc: 0.9699\n",
      "Epoch 77/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.4654 - auc: 0.9756\n",
      "Epoch 78/100\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.4336 - auc: 0.9808\n",
      "Epoch 79/100\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.4349 - auc: 0.9795\n",
      "Epoch 80/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.4087 - auc: 0.9842\n",
      "Epoch 81/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.4106 - auc: 0.9832\n",
      "Epoch 82/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.4645 - auc: 0.9748\n",
      "Epoch 83/100\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.4293 - auc: 0.9810\n",
      "Epoch 84/100\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.4578 - auc: 0.9763\n",
      "Epoch 85/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.4072 - auc: 0.9851\n",
      "Epoch 86/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.3956 - auc: 0.9847\n",
      "Epoch 87/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.4216 - auc: 0.9820\n",
      "Epoch 88/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.3921 - auc: 0.9859\n",
      "Epoch 89/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.4225 - auc: 0.9808\n",
      "Epoch 90/100\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.3905 - auc: 0.9872\n",
      "Epoch 91/100\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.3798 - auc: 0.9875\n",
      "Epoch 92/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.3713 - auc: 0.9889\n",
      "Epoch 93/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.3833 - auc: 0.9881\n",
      "Epoch 94/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.3943 - auc: 0.9861\n",
      "Epoch 95/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.3475 - auc: 0.9917\n",
      "Epoch 96/100\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.3620 - auc: 0.9896\n",
      "Epoch 97/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.3767 - auc: 0.9879\n",
      "Epoch 98/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.3384 - auc: 0.9916\n",
      "Epoch 99/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.3589 - auc: 0.9901\n",
      "Epoch 100/100\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.3717 - auc: 0.9867\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 4s 55ms/step - loss: 2.8787 - auc: 0.6067\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 1.9066 - auc: 0.6499\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 1.7757 - auc: 0.7072\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 1.7456 - auc: 0.7107\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 1.6936 - auc: 0.7364\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 1.6719 - auc: 0.7418\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 1.6272 - auc: 0.7623\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 1.6261 - auc: 0.7473\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 1.5958 - auc: 0.7618\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 1s 55ms/step - loss: 1.5170 - auc: 0.7915\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 1.5050 - auc: 0.7899\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 1.4540 - auc: 0.8086\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 1.4185 - auc: 0.8207\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 1.4196 - auc: 0.8180\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 1.4602 - auc: 0.7972\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 1.3848 - auc: 0.8149\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 1s 73ms/step - loss: 1.3177 - auc: 0.8419\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 1.2742 - auc: 0.8552\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 1.2574 - auc: 0.8607\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 1.2598 - auc: 0.8524\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 1.2274 - auc: 0.8576\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 1.2509 - auc: 0.8449\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 1.2217 - auc: 0.8531\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 1.1957 - auc: 0.8601\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 1.1779 - auc: 0.8608\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 1.1643 - auc: 0.8662\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 1.0960 - auc: 0.8855\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 1.0958 - auc: 0.8831\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 64ms/step - loss: 1.0760 - auc: 0.8871\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 1.0728 - auc: 0.8839\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 1.0361 - auc: 0.8949\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.9734 - auc: 0.9141\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 1.0181 - auc: 0.8949\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 1s 57ms/step - loss: 1.0004 - auc: 0.8980\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.9853 - auc: 0.9005\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.9742 - auc: 0.9025\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 1s 55ms/step - loss: 0.9193 - auc: 0.9212\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.8939 - auc: 0.9249\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.8689 - auc: 0.9294\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.8492 - auc: 0.9327\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.8586 - auc: 0.9292\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.8492 - auc: 0.9291\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.8375 - auc: 0.9308\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.8454 - auc: 0.9304\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 0.8398 - auc: 0.9285\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.7865 - auc: 0.9440\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.8278 - auc: 0.9289\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 0.7805 - auc: 0.9407\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.8258 - auc: 0.9291\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.7926 - auc: 0.9357\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.7432 - auc: 0.9486\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 0.7353 - auc: 0.9480\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.6866 - auc: 0.9594\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 1s 82ms/step - loss: 0.7079 - auc: 0.9533\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.6831 - auc: 0.9570\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.6683 - auc: 0.9610\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.6150 - auc: 0.9719\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 0.6806 - auc: 0.9537\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 0.6835 - auc: 0.9530\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 1s 68ms/step - loss: 0.6676 - auc: 0.9564\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.6651 - auc: 0.9569\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 1s 73ms/step - loss: 0.6047 - auc: 0.9694\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 1s 71ms/step - loss: 0.5716 - auc: 0.9761\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 1s 61ms/step - loss: 0.5397 - auc: 0.9798\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.5211 - auc: 0.9829\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.4779 - auc: 0.9889\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 0.4942 - auc: 0.9836\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 1s 77ms/step - loss: 0.6043 - auc: 0.9639\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 1s 62ms/step - loss: 0.5636 - auc: 0.9708\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 0.5357 - auc: 0.9772\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.4996 - auc: 0.9825\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.4949 - auc: 0.9831\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 0.4878 - auc: 0.9829\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.4742 - auc: 0.9847\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.4657 - auc: 0.9849\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 1s 81ms/step - loss: 0.5086 - auc: 0.9781\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.5362 - auc: 0.9732\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.5055 - auc: 0.9784\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.5034 - auc: 0.9796\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.4430 - auc: 0.9876\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 0.4490 - auc: 0.9864\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.4138 - auc: 0.9910\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.3816 - auc: 0.9937\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 0.3886 - auc: 0.9922\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 0.3931 - auc: 0.9896\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 1s 75ms/step - loss: 0.3684 - auc: 0.9940\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 1s 77ms/step - loss: 0.3589 - auc: 0.9943\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.4004 - auc: 0.9896\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.3922 - auc: 0.9907\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 1s 74ms/step - loss: 0.3965 - auc: 0.9896\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.3753 - auc: 0.9919\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 0.4379 - auc: 0.9832\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 0.4410 - auc: 0.9835\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.4288 - auc: 0.9857\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.4071 - auc: 0.9882\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.3622 - auc: 0.9933\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 1s 72ms/step - loss: 0.3690 - auc: 0.9923\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 0.3512 - auc: 0.9944\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.3796 - auc: 0.9910\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.3333 - auc: 0.9957\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 4s 55ms/step - loss: 2.6325 - auc: 0.6079\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 1.9653 - auc: 0.6624\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 1s 78ms/step - loss: 1.8236 - auc: 0.6968\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 1.7270 - auc: 0.7322\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 1.7078 - auc: 0.7364\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 1.7109 - auc: 0.7323\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 1.6818 - auc: 0.7246\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 1.6031 - auc: 0.7682\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 1.6166 - auc: 0.7670\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 1.5473 - auc: 0.7849\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 1.5500 - auc: 0.7802\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 1s 73ms/step - loss: 1.5547 - auc: 0.7756\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 1.5168 - auc: 0.7790\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 1.4897 - auc: 0.7849\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 1.4556 - auc: 0.7927\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 1.3927 - auc: 0.8163\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 1.3623 - auc: 0.8261\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 1.3439 - auc: 0.8303\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 1.3186 - auc: 0.8339\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 1.3181 - auc: 0.8304\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 1.2936 - auc: 0.8358\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 1.2468 - auc: 0.8523\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 1.2865 - auc: 0.8334\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 1.1861 - auc: 0.8679\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 1.1898 - auc: 0.8638\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 1.1740 - auc: 0.8631\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 1.1532 - auc: 0.8676\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 1.1479 - auc: 0.8695\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 1s 53ms/step - loss: 1.1017 - auc: 0.8798\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 1.1175 - auc: 0.8755\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 1.0965 - auc: 0.8754\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 1.0619 - auc: 0.8857\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 1.0689 - auc: 0.8832\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 1.0299 - auc: 0.8936\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 1.0423 - auc: 0.8878\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 0.9966 - auc: 0.9005\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.9755 - auc: 0.9051\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.9582 - auc: 0.9084\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 0.9338 - auc: 0.9135\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 0.9115 - auc: 0.9176\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 0.8902 - auc: 0.9231\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 0.8540 - auc: 0.9303\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 0.8770 - auc: 0.9240\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.9002 - auc: 0.9163\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.8356 - auc: 0.9317\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 1s 79ms/step - loss: 0.7875 - auc: 0.9436\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 1s 68ms/step - loss: 0.7971 - auc: 0.9415\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.8035 - auc: 0.9376\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 1s 77ms/step - loss: 0.8246 - auc: 0.9308\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.7705 - auc: 0.9441\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.7742 - auc: 0.9417\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.7615 - auc: 0.9454\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.7332 - auc: 0.9498\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 0.7647 - auc: 0.9416\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 0.7012 - auc: 0.9562\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.6863 - auc: 0.9593\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 1s 77ms/step - loss: 0.6821 - auc: 0.9576\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.6485 - auc: 0.9646\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 0.6239 - auc: 0.9701\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.6180 - auc: 0.9683\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.6425 - auc: 0.9629\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.6225 - auc: 0.9674\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 0.7071 - auc: 0.9491\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.6208 - auc: 0.9666\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.5735 - auc: 0.9770\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 0.5840 - auc: 0.9720\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 1s 69ms/step - loss: 0.5321 - auc: 0.9816\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.5448 - auc: 0.9779\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 0.5463 - auc: 0.9770\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.5324 - auc: 0.9794\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 0.5413 - auc: 0.9768\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 1s 79ms/step - loss: 0.5096 - auc: 0.9824\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 1s 72ms/step - loss: 0.4679 - auc: 0.9872\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 1s 62ms/step - loss: 0.5103 - auc: 0.9801\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.4665 - auc: 0.9875\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.4738 - auc: 0.9847\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 0.4679 - auc: 0.9854\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.4659 - auc: 0.9851\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 0.4545 - auc: 0.9859\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 0.4568 - auc: 0.9860\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.4928 - auc: 0.9813\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.4632 - auc: 0.9856\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.4465 - auc: 0.9882\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.4381 - auc: 0.9880\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.3984 - auc: 0.9928\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.3991 - auc: 0.9916\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 0.4056 - auc: 0.9903\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 0.3847 - auc: 0.9924\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 0.4105 - auc: 0.9888\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 0.3784 - auc: 0.9932\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 1s 75ms/step - loss: 0.3902 - auc: 0.9917\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.4295 - auc: 0.9855\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.4059 - auc: 0.9894\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 1s 80ms/step - loss: 0.4041 - auc: 0.9901\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.3873 - auc: 0.9921\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.3813 - auc: 0.9925\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.3506 - auc: 0.9950\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.3487 - auc: 0.9953\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.3073 - auc: 0.9985\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.3024 - auc: 0.9981\n",
      "Epoch 1/100\n",
      "30/30 [==============================] - 3s 10ms/step - loss: 1.8130 - auc: 0.6039\n",
      "Epoch 2/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 1.4647 - auc: 0.6753\n",
      "Epoch 3/100\n",
      "30/30 [==============================] - 0s 17ms/step - loss: 1.4307 - auc: 0.6861\n",
      "Epoch 4/100\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 1.3718 - auc: 0.7091\n",
      "Epoch 5/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 1.3885 - auc: 0.7020\n",
      "Epoch 6/100\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 1.3461 - auc: 0.7219\n",
      "Epoch 7/100\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 1.3136 - auc: 0.7301\n",
      "Epoch 8/100\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 1.2792 - auc: 0.7372\n",
      "Epoch 9/100\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 1.2510 - auc: 0.7449\n",
      "Epoch 10/100\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 1.2190 - auc: 0.7630\n",
      "Epoch 11/100\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 1.2098 - auc: 0.7623\n",
      "Epoch 12/100\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 1.2099 - auc: 0.7541\n",
      "Epoch 13/100\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 1.1533 - auc: 0.7780\n",
      "Epoch 14/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 1.1511 - auc: 0.7782\n",
      "Epoch 15/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 1.1250 - auc: 0.7883\n",
      "Epoch 16/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 1.1130 - auc: 0.7942\n",
      "Epoch 17/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 1.1064 - auc: 0.7920\n",
      "Epoch 18/100\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 1.0766 - auc: 0.8059\n",
      "Epoch 19/100\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 1.0582 - auc: 0.8088\n",
      "Epoch 20/100\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 1.0477 - auc: 0.8113\n",
      "Epoch 21/100\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 1.0022 - auc: 0.8300\n",
      "Epoch 22/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 1.0143 - auc: 0.8216\n",
      "Epoch 23/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.9746 - auc: 0.8377\n",
      "Epoch 24/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.9832 - auc: 0.8301\n",
      "Epoch 25/100\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.9556 - auc: 0.8405\n",
      "Epoch 26/100\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.9635 - auc: 0.8364\n",
      "Epoch 27/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.9523 - auc: 0.8385\n",
      "Epoch 28/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.8997 - auc: 0.8607\n",
      "Epoch 29/100\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.9219 - auc: 0.8493\n",
      "Epoch 30/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.8847 - auc: 0.8635\n",
      "Epoch 31/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.8584 - auc: 0.8726\n",
      "Epoch 32/100\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.8583 - auc: 0.8724\n",
      "Epoch 33/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.8657 - auc: 0.8679\n",
      "Epoch 34/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.8173 - auc: 0.8877\n",
      "Epoch 35/100\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.8151 - auc: 0.8847\n",
      "Epoch 36/100\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.7899 - auc: 0.8943\n",
      "Epoch 37/100\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.7642 - auc: 0.9017\n",
      "Epoch 38/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.7946 - auc: 0.8892\n",
      "Epoch 39/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.7606 - auc: 0.9032\n",
      "Epoch 40/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.7606 - auc: 0.9008\n",
      "Epoch 41/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.7539 - auc: 0.9034\n",
      "Epoch 42/100\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.7375 - auc: 0.9082\n",
      "Epoch 43/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.7356 - auc: 0.9078\n",
      "Epoch 44/100\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.6982 - auc: 0.9204\n",
      "Epoch 45/100\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.7279 - auc: 0.9093\n",
      "Epoch 46/100\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.6519 - auc: 0.9372\n",
      "Epoch 47/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.6927 - auc: 0.9214\n",
      "Epoch 48/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.6479 - auc: 0.9342\n",
      "Epoch 49/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.6413 - auc: 0.9359\n",
      "Epoch 50/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.6992 - auc: 0.9203\n",
      "Epoch 51/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.6726 - auc: 0.9276\n",
      "Epoch 52/100\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.6113 - auc: 0.9444\n",
      "Epoch 53/100\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.6493 - auc: 0.9336\n",
      "Epoch 54/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.6195 - auc: 0.9420\n",
      "Epoch 55/100\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.6020 - auc: 0.9468\n",
      "Epoch 56/100\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.5730 - auc: 0.9539\n",
      "Epoch 57/100\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.5608 - auc: 0.9560\n",
      "Epoch 58/100\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.5622 - auc: 0.9568\n",
      "Epoch 59/100\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.6053 - auc: 0.9444\n",
      "Epoch 60/100\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.4995 - auc: 0.9707\n",
      "Epoch 61/100\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.5030 - auc: 0.9678\n",
      "Epoch 62/100\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.5475 - auc: 0.9581\n",
      "Epoch 63/100\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.5218 - auc: 0.9639\n",
      "Epoch 64/100\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.5230 - auc: 0.9635\n",
      "Epoch 65/100\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.5176 - auc: 0.9650\n",
      "Epoch 66/100\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 0.4889 - auc: 0.9704\n",
      "Epoch 67/100\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.5200 - auc: 0.9642\n",
      "Epoch 68/100\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.5013 - auc: 0.9685\n",
      "Epoch 69/100\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.4868 - auc: 0.9712\n",
      "Epoch 70/100\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.4890 - auc: 0.9707\n",
      "Epoch 71/100\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.4978 - auc: 0.9699\n",
      "Epoch 72/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.4908 - auc: 0.9703\n",
      "Epoch 73/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.4503 - auc: 0.9778\n",
      "Epoch 74/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.4598 - auc: 0.9757\n",
      "Epoch 75/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.4767 - auc: 0.9729\n",
      "Epoch 76/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.4913 - auc: 0.9699\n",
      "Epoch 77/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.4654 - auc: 0.9756\n",
      "Epoch 78/100\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.4336 - auc: 0.9808\n",
      "Epoch 79/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.4349 - auc: 0.9795\n",
      "Epoch 80/100\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.4087 - auc: 0.9842\n",
      "Epoch 81/100\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.4106 - auc: 0.9832\n",
      "Epoch 82/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.4645 - auc: 0.9748\n",
      "Epoch 83/100\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.4293 - auc: 0.9810\n",
      "Epoch 84/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.4578 - auc: 0.9763\n",
      "Epoch 85/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.4072 - auc: 0.9851\n",
      "Epoch 86/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.3956 - auc: 0.9847\n",
      "Epoch 87/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.4216 - auc: 0.9820\n",
      "Epoch 88/100\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.3921 - auc: 0.9859\n",
      "Epoch 89/100\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.4225 - auc: 0.9808\n",
      "Epoch 90/100\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.3905 - auc: 0.9872\n",
      "Epoch 91/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.3798 - auc: 0.9875\n",
      "Epoch 92/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.3713 - auc: 0.9889\n",
      "Epoch 93/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.3833 - auc: 0.9881\n",
      "Epoch 94/100\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.3943 - auc: 0.9861\n",
      "Epoch 95/100\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.3475 - auc: 0.9917\n",
      "Epoch 96/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.3620 - auc: 0.9896\n",
      "Epoch 97/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.3767 - auc: 0.9879\n",
      "Epoch 98/100\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.3384 - auc: 0.9916\n",
      "Epoch 99/100\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.3589 - auc: 0.9901\n",
      "Epoch 100/100\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.3717 - auc: 0.9867\n"
     ]
    }
   ],
   "source": [
    "ensemble = MultivariableVotingClassifier(models)\n",
    "ensemble.fit(X_scaled, y_one_hot)\n",
    "# ensemble.fit(X, y_one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Datos originales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open_time</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Number of trades</th>\n",
       "      <th>Volume_BTCUSDT</th>\n",
       "      <th>Number_of_trades_BTCUSDT</th>\n",
       "      <th>Volume_ETHUSDT</th>\n",
       "      <th>Volume_BNBUSDT</th>\n",
       "      <th>EMA_20</th>\n",
       "      <th>Upper_Band</th>\n",
       "      <th>Middle_Band</th>\n",
       "      <th>RSI</th>\n",
       "      <th>total_trades_coinbase</th>\n",
       "      <th>Tweets_Utilizados</th>\n",
       "      <th>Tweets_Utilizados_coin</th>\n",
       "      <th>Tweets_Utilizados_whale_alert</th>\n",
       "      <th>Buy_1000x_high</th>\n",
       "      <th>sell_1000x_high</th>\n",
       "      <th>Tendencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>946</th>\n",
       "      <td>2024-04-30</td>\n",
       "      <td>6.59</td>\n",
       "      <td>6.67</td>\n",
       "      <td>6.04</td>\n",
       "      <td>115512.00</td>\n",
       "      <td>54947.66</td>\n",
       "      <td>1985671.00</td>\n",
       "      <td>561717.49</td>\n",
       "      <td>766513.45</td>\n",
       "      <td>7.13</td>\n",
       "      <td>7.80</td>\n",
       "      <td>6.93</td>\n",
       "      <td>34.18</td>\n",
       "      <td>48709.00</td>\n",
       "      <td>142</td>\n",
       "      <td>187</td>\n",
       "      <td>23.00</td>\n",
       "      <td>379.00</td>\n",
       "      <td>377.00</td>\n",
       "      <td>Bajista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>947</th>\n",
       "      <td>2024-05-01</td>\n",
       "      <td>6.42</td>\n",
       "      <td>6.93</td>\n",
       "      <td>6.13</td>\n",
       "      <td>175570.00</td>\n",
       "      <td>81166.47</td>\n",
       "      <td>2401089.00</td>\n",
       "      <td>624963.78</td>\n",
       "      <td>669027.32</td>\n",
       "      <td>7.11</td>\n",
       "      <td>7.41</td>\n",
       "      <td>6.85</td>\n",
       "      <td>43.30</td>\n",
       "      <td>83718.00</td>\n",
       "      <td>130</td>\n",
       "      <td>177</td>\n",
       "      <td>36.00</td>\n",
       "      <td>327.00</td>\n",
       "      <td>340.00</td>\n",
       "      <td>Alcista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948</th>\n",
       "      <td>2024-05-02</td>\n",
       "      <td>6.90</td>\n",
       "      <td>7.41</td>\n",
       "      <td>6.69</td>\n",
       "      <td>109002.00</td>\n",
       "      <td>47583.82</td>\n",
       "      <td>1572898.00</td>\n",
       "      <td>365939.72</td>\n",
       "      <td>359794.32</td>\n",
       "      <td>7.12</td>\n",
       "      <td>7.42</td>\n",
       "      <td>6.85</td>\n",
       "      <td>49.27</td>\n",
       "      <td>61208.00</td>\n",
       "      <td>461</td>\n",
       "      <td>374</td>\n",
       "      <td>25.00</td>\n",
       "      <td>270.00</td>\n",
       "      <td>282.00</td>\n",
       "      <td>Alcista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949</th>\n",
       "      <td>2024-05-03</td>\n",
       "      <td>7.27</td>\n",
       "      <td>7.39</td>\n",
       "      <td>7.00</td>\n",
       "      <td>109634.00</td>\n",
       "      <td>43628.40</td>\n",
       "      <td>1558661.00</td>\n",
       "      <td>355825.84</td>\n",
       "      <td>342906.43</td>\n",
       "      <td>7.14</td>\n",
       "      <td>7.44</td>\n",
       "      <td>6.90</td>\n",
       "      <td>48.86</td>\n",
       "      <td>46255.00</td>\n",
       "      <td>573</td>\n",
       "      <td>474</td>\n",
       "      <td>22.00</td>\n",
       "      <td>386.00</td>\n",
       "      <td>635.00</td>\n",
       "      <td>Lateral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>2024-05-04</td>\n",
       "      <td>7.24</td>\n",
       "      <td>7.28</td>\n",
       "      <td>7.11</td>\n",
       "      <td>71120.00</td>\n",
       "      <td>24368.69</td>\n",
       "      <td>1113509.00</td>\n",
       "      <td>196263.95</td>\n",
       "      <td>197129.25</td>\n",
       "      <td>7.13</td>\n",
       "      <td>7.46</td>\n",
       "      <td>6.91</td>\n",
       "      <td>46.98</td>\n",
       "      <td>34251.00</td>\n",
       "      <td>407</td>\n",
       "      <td>472</td>\n",
       "      <td>14.00</td>\n",
       "      <td>203.00</td>\n",
       "      <td>232.00</td>\n",
       "      <td>Bajista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951</th>\n",
       "      <td>2024-05-05</td>\n",
       "      <td>7.12</td>\n",
       "      <td>7.40</td>\n",
       "      <td>7.01</td>\n",
       "      <td>72928.00</td>\n",
       "      <td>18526.75</td>\n",
       "      <td>992921.00</td>\n",
       "      <td>218760.27</td>\n",
       "      <td>180458.24</td>\n",
       "      <td>7.15</td>\n",
       "      <td>7.51</td>\n",
       "      <td>6.94</td>\n",
       "      <td>50.00</td>\n",
       "      <td>29197.00</td>\n",
       "      <td>417</td>\n",
       "      <td>499</td>\n",
       "      <td>6.00</td>\n",
       "      <td>320.00</td>\n",
       "      <td>284.00</td>\n",
       "      <td>Alcista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>2024-05-06</td>\n",
       "      <td>7.30</td>\n",
       "      <td>7.47</td>\n",
       "      <td>7.06</td>\n",
       "      <td>94264.00</td>\n",
       "      <td>34674.92</td>\n",
       "      <td>1392557.00</td>\n",
       "      <td>355135.30</td>\n",
       "      <td>278669.01</td>\n",
       "      <td>7.15</td>\n",
       "      <td>7.53</td>\n",
       "      <td>6.96</td>\n",
       "      <td>47.10</td>\n",
       "      <td>40027.00</td>\n",
       "      <td>482</td>\n",
       "      <td>531</td>\n",
       "      <td>25.00</td>\n",
       "      <td>339.00</td>\n",
       "      <td>249.00</td>\n",
       "      <td>Bajista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>2024-05-07</td>\n",
       "      <td>7.12</td>\n",
       "      <td>7.29</td>\n",
       "      <td>6.98</td>\n",
       "      <td>64947.00</td>\n",
       "      <td>25598.79</td>\n",
       "      <td>1272898.00</td>\n",
       "      <td>298796.68</td>\n",
       "      <td>289488.71</td>\n",
       "      <td>7.13</td>\n",
       "      <td>7.52</td>\n",
       "      <td>6.98</td>\n",
       "      <td>45.10</td>\n",
       "      <td>31028.00</td>\n",
       "      <td>495</td>\n",
       "      <td>494</td>\n",
       "      <td>28.00</td>\n",
       "      <td>296.00</td>\n",
       "      <td>205.00</td>\n",
       "      <td>Bajista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>2024-05-08</td>\n",
       "      <td>6.99</td>\n",
       "      <td>7.19</td>\n",
       "      <td>6.89</td>\n",
       "      <td>75550.00</td>\n",
       "      <td>26121.19</td>\n",
       "      <td>1415152.00</td>\n",
       "      <td>266934.81</td>\n",
       "      <td>297016.62</td>\n",
       "      <td>7.12</td>\n",
       "      <td>7.52</td>\n",
       "      <td>6.99</td>\n",
       "      <td>44.94</td>\n",
       "      <td>32040.00</td>\n",
       "      <td>426</td>\n",
       "      <td>494</td>\n",
       "      <td>24.00</td>\n",
       "      <td>230.00</td>\n",
       "      <td>177.00</td>\n",
       "      <td>Lateral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955</th>\n",
       "      <td>2024-05-09</td>\n",
       "      <td>6.98</td>\n",
       "      <td>7.09</td>\n",
       "      <td>6.78</td>\n",
       "      <td>75016.00</td>\n",
       "      <td>30660.81</td>\n",
       "      <td>1381957.00</td>\n",
       "      <td>238561.75</td>\n",
       "      <td>464857.60</td>\n",
       "      <td>7.11</td>\n",
       "      <td>7.52</td>\n",
       "      <td>7.01</td>\n",
       "      <td>46.32</td>\n",
       "      <td>29314.00</td>\n",
       "      <td>475</td>\n",
       "      <td>464</td>\n",
       "      <td>16.00</td>\n",
       "      <td>188.00</td>\n",
       "      <td>257.00</td>\n",
       "      <td>Lateral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Open_time  Open  High  Low  Number of trades  Volume_BTCUSDT  \\\n",
       "946 2024-04-30  6.59  6.67 6.04         115512.00        54947.66   \n",
       "947 2024-05-01  6.42  6.93 6.13         175570.00        81166.47   \n",
       "948 2024-05-02  6.90  7.41 6.69         109002.00        47583.82   \n",
       "949 2024-05-03  7.27  7.39 7.00         109634.00        43628.40   \n",
       "950 2024-05-04  7.24  7.28 7.11          71120.00        24368.69   \n",
       "951 2024-05-05  7.12  7.40 7.01          72928.00        18526.75   \n",
       "952 2024-05-06  7.30  7.47 7.06          94264.00        34674.92   \n",
       "953 2024-05-07  7.12  7.29 6.98          64947.00        25598.79   \n",
       "954 2024-05-08  6.99  7.19 6.89          75550.00        26121.19   \n",
       "955 2024-05-09  6.98  7.09 6.78          75016.00        30660.81   \n",
       "\n",
       "     Number_of_trades_BTCUSDT  Volume_ETHUSDT  Volume_BNBUSDT  EMA_20  \\\n",
       "946                1985671.00       561717.49       766513.45    7.13   \n",
       "947                2401089.00       624963.78       669027.32    7.11   \n",
       "948                1572898.00       365939.72       359794.32    7.12   \n",
       "949                1558661.00       355825.84       342906.43    7.14   \n",
       "950                1113509.00       196263.95       197129.25    7.13   \n",
       "951                 992921.00       218760.27       180458.24    7.15   \n",
       "952                1392557.00       355135.30       278669.01    7.15   \n",
       "953                1272898.00       298796.68       289488.71    7.13   \n",
       "954                1415152.00       266934.81       297016.62    7.12   \n",
       "955                1381957.00       238561.75       464857.60    7.11   \n",
       "\n",
       "     Upper_Band  Middle_Band   RSI  total_trades_coinbase  Tweets_Utilizados  \\\n",
       "946        7.80         6.93 34.18               48709.00                142   \n",
       "947        7.41         6.85 43.30               83718.00                130   \n",
       "948        7.42         6.85 49.27               61208.00                461   \n",
       "949        7.44         6.90 48.86               46255.00                573   \n",
       "950        7.46         6.91 46.98               34251.00                407   \n",
       "951        7.51         6.94 50.00               29197.00                417   \n",
       "952        7.53         6.96 47.10               40027.00                482   \n",
       "953        7.52         6.98 45.10               31028.00                495   \n",
       "954        7.52         6.99 44.94               32040.00                426   \n",
       "955        7.52         7.01 46.32               29314.00                475   \n",
       "\n",
       "     Tweets_Utilizados_coin  Tweets_Utilizados_whale_alert  Buy_1000x_high  \\\n",
       "946                     187                          23.00          379.00   \n",
       "947                     177                          36.00          327.00   \n",
       "948                     374                          25.00          270.00   \n",
       "949                     474                          22.00          386.00   \n",
       "950                     472                          14.00          203.00   \n",
       "951                     499                           6.00          320.00   \n",
       "952                     531                          25.00          339.00   \n",
       "953                     494                          28.00          296.00   \n",
       "954                     494                          24.00          230.00   \n",
       "955                     464                          16.00          188.00   \n",
       "\n",
       "     sell_1000x_high Tendencia  \n",
       "946           377.00   Bajista  \n",
       "947           340.00   Alcista  \n",
       "948           282.00   Alcista  \n",
       "949           635.00   Lateral  \n",
       "950           232.00   Bajista  \n",
       "951           284.00   Alcista  \n",
       "952           249.00   Bajista  \n",
       "953           205.00   Bajista  \n",
       "954           177.00   Lateral  \n",
       "955           257.00   Lateral  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(clasifier_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 189ms/step\n",
      "1/1 [==============================] - 0s 207ms/step\n",
      "1/1 [==============================] - 0s 495ms/step\n",
      "1/1 [==============================] - 0s 327ms/step\n",
      "1/1 [==============================] - 0s 252ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 0, 2, 2, 2, 1, 2, 2])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['Alcista', 'Bajista', 'Lateral'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Bajista',\n",
       " 'Bajista',\n",
       " 'Alcista',\n",
       " 'Alcista',\n",
       " 'Lateral',\n",
       " 'Lateral',\n",
       " 'Lateral',\n",
       " 'Bajista',\n",
       " 'Lateral',\n",
       " 'Lateral']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_clases = 3 \n",
    "\n",
    "# DESCOMENTAR PARA NORMALIZACION\n",
    "clasifier_validation_normalized = scaler.transform(clasifier_validation[columns].drop(columns=[\"Open_time\"]))\n",
    "validation_predictions = ensemble.predict(clasifier_validation_normalized)\n",
    "\n",
    "# validation_predictions = clasifier_validation[columns].drop(columns=[\"Open_time\"])\n",
    "# validation_predictions = ensemble.predict(validation_predictions)\n",
    "\n",
    "predicciones_one_hot = to_categorical(validation_predictions, num_classes=n_clases)\n",
    "etiquetas_numericas = np.argmax(predicciones_one_hot, axis=1)\n",
    "categorias_clases = onehot_encoder.categories_[0]\n",
    "nombres_clases = [categorias_clases[indice] for indice in etiquetas_numericas]\n",
    "\n",
    "display(validation_predictions)\n",
    "display(categorias_clases)\n",
    "display(nombres_clases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Datos generados por auto ml con prophet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['Alcista', 'Bajista', 'Lateral'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Lateral',\n",
       " 'Lateral',\n",
       " 'Lateral',\n",
       " 'Lateral',\n",
       " 'Lateral',\n",
       " 'Lateral',\n",
       " 'Lateral',\n",
       " 'Lateral',\n",
       " 'Lateral',\n",
       " 'Lateral']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "auto_ml_prophet_df = pd.read_csv('auto_timeseries_models_prophet/predicciones.csv')\n",
    "auto_ml_prophet_df = auto_ml_prophet_df[columns].drop(columns=[\"Open_time\"])\n",
    "\n",
    "# DESCOMENTAR PARA NORMALIZACION\n",
    "auto_ml_prophet_df_normalized = scaler.transform(auto_ml_prophet_df)\n",
    "auto_mp_prophet_predictions = ensemble.predict(auto_ml_prophet_df_normalized)\n",
    "\n",
    "# auto_mp_prophet_predictions = ensemble.predict(auto_ml_prophet_df)\n",
    "\n",
    "predicciones_one_hot = to_categorical(auto_mp_prophet_predictions, num_classes=n_clases)\n",
    "etiquetas_numericas = np.argmax(predicciones_one_hot, axis=1)\n",
    "categorias_clases = onehot_encoder.categories_[0]\n",
    "nombres_clases = [categorias_clases[indice] for indice in etiquetas_numericas]\n",
    "\n",
    "display(auto_mp_prophet_predictions)\n",
    "display(categorias_clases)\n",
    "display(nombres_clases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Datos generados por auto ml con stats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['Alcista', 'Bajista', 'Lateral'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Bajista',\n",
       " 'Alcista',\n",
       " 'Alcista',\n",
       " 'Alcista',\n",
       " 'Alcista',\n",
       " 'Alcista',\n",
       " 'Alcista',\n",
       " 'Alcista',\n",
       " 'Alcista',\n",
       " 'Alcista']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "auto_ml_stats_df = pd.read_csv('auto_timeseries_models/predicciones.csv')\n",
    "auto_ml_stats_df = auto_ml_stats_df[columns].drop(columns=[\"Open_time\"])\n",
    "\n",
    "# DESCOMENTAR PARA NORMALIZACION\n",
    "auto_ml_stats_df_normalized = scaler.transform(auto_ml_stats_df)\n",
    "auto_mp_stats_predictions = ensemble.predict(auto_ml_stats_df_normalized)\n",
    "\n",
    "# auto_mp_stats_predictions = ensemble.predict(auto_ml_stats_df)\n",
    "\n",
    "predicciones_one_hot = to_categorical(auto_mp_stats_predictions, num_classes=n_clases)\n",
    "etiquetas_numericas = np.argmax(predicciones_one_hot, axis=1)\n",
    "categorias_clases = onehot_encoder.categories_[0]\n",
    "nombres_clases = [categorias_clases[indice] for indice in etiquetas_numericas]\n",
    "\n",
    "display(auto_mp_stats_predictions)\n",
    "display(categorias_clases)\n",
    "display(nombres_clases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Datos generados por auto ml con modelos clasicos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['Alcista', 'Bajista', 'Lateral'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Bajista',\n",
       " 'Bajista',\n",
       " 'Bajista',\n",
       " 'Bajista',\n",
       " 'Bajista',\n",
       " 'Bajista',\n",
       " 'Bajista',\n",
       " 'Bajista',\n",
       " 'Bajista',\n",
       " 'Bajista']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "auto_ml_df = pd.read_csv('h2o_models/predicciones.csv')\n",
    "auto_ml_df = auto_ml_df[columns].drop(columns=[\"Open_time\"])\n",
    "\n",
    "# DESCOMENTAR PARA NORMALIZACION\n",
    "auto_ml_df_normalized = scaler.transform(auto_ml_df)\n",
    "auto_mp_predictions = ensemble.predict(auto_ml_df_normalized)\n",
    "\n",
    "# auto_mp_predictions = ensemble.predict(auto_ml_df)\n",
    "\n",
    "predicciones_one_hot = to_categorical(auto_mp_predictions, num_classes=n_clases)\n",
    "etiquetas_numericas = np.argmax(predicciones_one_hot, axis=1)\n",
    "categorias_clases = onehot_encoder.categories_[0]\n",
    "nombres_clases = [categorias_clases[indice] for indice in etiquetas_numericas]\n",
    "\n",
    "display(auto_mp_predictions)\n",
    "display(categorias_clases)\n",
    "display(nombres_clases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Datos generados por auto ml con skforecast:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 0, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['Alcista', 'Bajista', 'Lateral'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Bajista',\n",
       " 'Bajista',\n",
       " 'Alcista',\n",
       " 'Alcista',\n",
       " 'Lateral',\n",
       " 'Lateral',\n",
       " 'Lateral',\n",
       " 'Lateral',\n",
       " 'Lateral',\n",
       " 'Lateral']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "skforecast_df = pd.read_csv('skforecast/predicciones.csv')\n",
    "skforecast_df = skforecast_df[columns[1:]]\n",
    "\n",
    "# DESCOMENTAR PARA NORMALIZACION\n",
    "skforecast_df_normalized = scaler.transform(skforecast_df)\n",
    "skforecast_predictions = ensemble.predict(skforecast_df_normalized)\n",
    "\n",
    "# skforecast_predictions = ensemble.predict(skforecast_df)\n",
    "\n",
    "predicciones_one_hot = to_categorical(skforecast_predictions, num_classes=n_clases)\n",
    "etiquetas_numericas = np.argmax(predicciones_one_hot, axis=1)\n",
    "categorias_clases = onehot_encoder.categories_[0]\n",
    "nombres_clases = [categorias_clases[indice] for indice in etiquetas_numericas]\n",
    "\n",
    "display(skforecast_predictions)\n",
    "display(categorias_clases)\n",
    "display(nombres_clases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analizo los features mas importantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Permutation Feature Importance\n",
    "\n",
    "El método de Permutation Feature Importance implica permutar las características del dataset y medir el impacto en el rendimiento del modelo. Esto se hace una característica a la vez. Puedes utilizar el siguiente código para calcular la importancia de las características usando este método:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# def permutation_feature_importance(model, X, y, metric=accuracy_score, n_repeats=10):\n",
    "#     base_score = metric(y, model.predict(X))\n",
    "#     scores = np.zeros((X.shape[1], n_repeats))\n",
    "    \n",
    "#     for i in range(X.shape[1]):\n",
    "#         for n in range(n_repeats):\n",
    "#             X_permuted = X.copy()\n",
    "#             np.random.shuffle(X_permuted[:, i])\n",
    "#             permuted_score = metric(y, model.predict(X_permuted))\n",
    "#             scores[i, n] = base_score - permuted_score\n",
    "            \n",
    "#     importances = np.mean(scores, axis=1)\n",
    "#     return importances\n",
    "\n",
    "# # Usa el ensemble model para obtener las importancias\n",
    "# importances = permutation_feature_importance(ensemble, X.values, y)\n",
    "\n",
    "# # Ordena las características por importancia\n",
    "# feature_importances = sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# # Imprime las importancias\n",
    "# for feature, importance in feature_importances:\n",
    "#     print(f'Feature: {feature}, Importance: {importance}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SHAP (SHapley Additive exPlanations)\n",
    "\n",
    "SHAP es una técnica avanzada que explica las predicciones de cualquier modelo basado en el cálculo de los valores de Shapley de la teoría de juegos. Puedes usar la librería shap para calcular las importancias:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shap\n",
    "\n",
    "# # Crear un background dataset, usualmente se usa una muestra aleatoria de los datos de entrenamiento\n",
    "# background = X.sample(n=100)\n",
    "\n",
    "# # Crear el objeto explainer de SHAP\n",
    "# explainer = shap.KernelExplainer(ensemble.predict, background)\n",
    "\n",
    "# # Calcular los valores SHAP para el conjunto de datos completo\n",
    "# shap_values = explainer.shap_values(X)\n",
    "\n",
    "# # Visualizar las importancias de las características\n",
    "# shap.summary_plot(shap_values, X)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
