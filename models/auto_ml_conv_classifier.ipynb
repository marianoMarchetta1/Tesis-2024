{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam, RMSprop, SGD, AdamW, Nadam\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import log_loss\n",
    "from tensorflow.keras.metrics import AUC, Precision, Recall\n",
    "from keras.utils import to_categorical\n",
    "from adabound import AdaBound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evito que ciertas columnas se transformen a notacion cientifica en las predicciones\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    'Open_time',\n",
    "    'Open',\n",
    "    'High',\n",
    "    'Low',\n",
    "    # 'Close',\n",
    "    'Number of trades',\n",
    "    # 'Close_BTCUSDT',\n",
    "    'Volume_BTCUSDT',\n",
    "    'Number_of_trades_BTCUSDT',\n",
    "    # 'Close_ETHUSDT',\n",
    "    'Volume_ETHUSDT',\n",
    "    # 'Number_of_trades_ETHUSDT',\n",
    "    # 'Close_BNBUSDT',\n",
    "    'Volume_BNBUSDT',\n",
    "    # 'Number_of_trades_BNBUSDT',\n",
    "    # 'SMA_20',\n",
    "    'EMA_20',\n",
    "    'Upper_Band',\n",
    "    'Middle_Band',\n",
    "    # 'Lower_Band',\n",
    "    'RSI',\n",
    "    # 'buy_1000x_high_coinbase',\n",
    "    # 'sell_1000x_high_coinbase',\n",
    "    'total_trades_coinbase',\t\n",
    "    'Tweets_Utilizados',\n",
    "    'Tweets_Utilizados_coin',\n",
    "    # 'Tweets_Utilizados_referentes',\n",
    "    'Tweets_Utilizados_whale_alert',\n",
    "    'Buy_1000x_high',\n",
    "    'sell_1000x_high',\n",
    "    # 'total_trades_binance'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Armado y entrenamiento de un clasificador a partir de los datos originales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelo convolucional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_dataset = pd.read_csv('/Users/mmarchetta/Desktop/Tesis-2024/data-visualization/final_dataset.csv') \n",
    "classifier_dataset = complete_dataset[columns]\n",
    "classifier_dataset['Open_time'] = pd.to_datetime(classifier_dataset['Open_time'])\n",
    "classifier_dataset['Tendencia'] = complete_dataset['Tendencia']\n",
    "\n",
    "clasifier_validation = classifier_dataset[-10:]\n",
    "classifier_dataset = classifier_dataset[:-10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open_time</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Number of trades</th>\n",
       "      <th>Volume_BTCUSDT</th>\n",
       "      <th>Number_of_trades_BTCUSDT</th>\n",
       "      <th>Volume_ETHUSDT</th>\n",
       "      <th>Volume_BNBUSDT</th>\n",
       "      <th>EMA_20</th>\n",
       "      <th>Upper_Band</th>\n",
       "      <th>Middle_Band</th>\n",
       "      <th>RSI</th>\n",
       "      <th>total_trades_coinbase</th>\n",
       "      <th>Tweets_Utilizados</th>\n",
       "      <th>Tweets_Utilizados_coin</th>\n",
       "      <th>Tweets_Utilizados_whale_alert</th>\n",
       "      <th>Buy_1000x_high</th>\n",
       "      <th>sell_1000x_high</th>\n",
       "      <th>Tendencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>941</th>\n",
       "      <td>2024-04-25</td>\n",
       "      <td>6.93</td>\n",
       "      <td>7.00</td>\n",
       "      <td>6.70</td>\n",
       "      <td>71088.00</td>\n",
       "      <td>31341.46</td>\n",
       "      <td>1375324.00</td>\n",
       "      <td>352288.55</td>\n",
       "      <td>453745.52</td>\n",
       "      <td>7.45</td>\n",
       "      <td>9.08</td>\n",
       "      <td>7.43</td>\n",
       "      <td>38.83</td>\n",
       "      <td>33468.00</td>\n",
       "      <td>151</td>\n",
       "      <td>114</td>\n",
       "      <td>22.00</td>\n",
       "      <td>242.00</td>\n",
       "      <td>219.00</td>\n",
       "      <td>Lateral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942</th>\n",
       "      <td>2024-04-26</td>\n",
       "      <td>6.86</td>\n",
       "      <td>6.95</td>\n",
       "      <td>6.71</td>\n",
       "      <td>67383.00</td>\n",
       "      <td>27085.19</td>\n",
       "      <td>1025561.00</td>\n",
       "      <td>252522.65</td>\n",
       "      <td>302119.88</td>\n",
       "      <td>7.38</td>\n",
       "      <td>8.94</td>\n",
       "      <td>7.34</td>\n",
       "      <td>37.81</td>\n",
       "      <td>26619.00</td>\n",
       "      <td>117</td>\n",
       "      <td>106</td>\n",
       "      <td>14.00</td>\n",
       "      <td>292.00</td>\n",
       "      <td>324.00</td>\n",
       "      <td>Lateral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>943</th>\n",
       "      <td>2024-04-27</td>\n",
       "      <td>6.76</td>\n",
       "      <td>6.87</td>\n",
       "      <td>6.51</td>\n",
       "      <td>64779.00</td>\n",
       "      <td>20933.06</td>\n",
       "      <td>912422.00</td>\n",
       "      <td>323811.19</td>\n",
       "      <td>268783.91</td>\n",
       "      <td>7.33</td>\n",
       "      <td>8.73</td>\n",
       "      <td>7.24</td>\n",
       "      <td>38.57</td>\n",
       "      <td>25565.00</td>\n",
       "      <td>101</td>\n",
       "      <td>138</td>\n",
       "      <td>7.00</td>\n",
       "      <td>248.00</td>\n",
       "      <td>179.00</td>\n",
       "      <td>Lateral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>944</th>\n",
       "      <td>2024-04-28</td>\n",
       "      <td>6.81</td>\n",
       "      <td>6.95</td>\n",
       "      <td>6.69</td>\n",
       "      <td>43208.00</td>\n",
       "      <td>16949.20</td>\n",
       "      <td>790652.00</td>\n",
       "      <td>304766.01</td>\n",
       "      <td>258059.43</td>\n",
       "      <td>7.27</td>\n",
       "      <td>8.38</td>\n",
       "      <td>7.13</td>\n",
       "      <td>37.66</td>\n",
       "      <td>20954.00</td>\n",
       "      <td>82</td>\n",
       "      <td>106</td>\n",
       "      <td>13.00</td>\n",
       "      <td>173.00</td>\n",
       "      <td>165.00</td>\n",
       "      <td>Lateral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>945</th>\n",
       "      <td>2024-04-29</td>\n",
       "      <td>6.73</td>\n",
       "      <td>6.83</td>\n",
       "      <td>6.47</td>\n",
       "      <td>63006.00</td>\n",
       "      <td>28150.23</td>\n",
       "      <td>1152296.00</td>\n",
       "      <td>421831.29</td>\n",
       "      <td>330474.01</td>\n",
       "      <td>7.20</td>\n",
       "      <td>8.08</td>\n",
       "      <td>7.03</td>\n",
       "      <td>36.02</td>\n",
       "      <td>33959.00</td>\n",
       "      <td>115</td>\n",
       "      <td>125</td>\n",
       "      <td>24.00</td>\n",
       "      <td>260.00</td>\n",
       "      <td>188.00</td>\n",
       "      <td>Bajista</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Open_time  Open  High  Low  Number of trades  Volume_BTCUSDT  \\\n",
       "941 2024-04-25  6.93  7.00 6.70          71088.00        31341.46   \n",
       "942 2024-04-26  6.86  6.95 6.71          67383.00        27085.19   \n",
       "943 2024-04-27  6.76  6.87 6.51          64779.00        20933.06   \n",
       "944 2024-04-28  6.81  6.95 6.69          43208.00        16949.20   \n",
       "945 2024-04-29  6.73  6.83 6.47          63006.00        28150.23   \n",
       "\n",
       "     Number_of_trades_BTCUSDT  Volume_ETHUSDT  Volume_BNBUSDT  EMA_20  \\\n",
       "941                1375324.00       352288.55       453745.52    7.45   \n",
       "942                1025561.00       252522.65       302119.88    7.38   \n",
       "943                 912422.00       323811.19       268783.91    7.33   \n",
       "944                 790652.00       304766.01       258059.43    7.27   \n",
       "945                1152296.00       421831.29       330474.01    7.20   \n",
       "\n",
       "     Upper_Band  Middle_Band   RSI  total_trades_coinbase  Tweets_Utilizados  \\\n",
       "941        9.08         7.43 38.83               33468.00                151   \n",
       "942        8.94         7.34 37.81               26619.00                117   \n",
       "943        8.73         7.24 38.57               25565.00                101   \n",
       "944        8.38         7.13 37.66               20954.00                 82   \n",
       "945        8.08         7.03 36.02               33959.00                115   \n",
       "\n",
       "     Tweets_Utilizados_coin  Tweets_Utilizados_whale_alert  Buy_1000x_high  \\\n",
       "941                     114                          22.00          242.00   \n",
       "942                     106                          14.00          292.00   \n",
       "943                     138                           7.00          248.00   \n",
       "944                     106                          13.00          173.00   \n",
       "945                     125                          24.00          260.00   \n",
       "\n",
       "     sell_1000x_high Tendencia  \n",
       "941           219.00   Lateral  \n",
       "942           324.00   Lateral  \n",
       "943           179.00   Lateral  \n",
       "944           165.00   Lateral  \n",
       "945           188.00   Bajista  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(classifier_dataset.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(946, 20)"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = classifier_dataset.drop(columns=[\"Tendencia\", \"Open_time\"])\n",
    "y = classifier_dataset[\"Tendencia\"]\n",
    "\n",
    "y = y.to_numpy().reshape(-1, 1)\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "y_one_hot = onehot_encoder.fit_transform(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DESCOMENTAR PARA NORMALIZACION\n",
    "\n",
    "# scaler = MinMaxScaler()\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_validation = clasifier_validation.drop(columns=[\"Tendencia\", \"Open_time\"])\n",
    "y_validation = clasifier_validation[\"Tendencia\"]\n",
    "\n",
    "X_validation_scaled = scaler.transform(X_validation)\n",
    "y_validation = y_validation.to_numpy().reshape(-1, 1)\n",
    "y_validation_one_hot = onehot_encoder.transform(y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(y_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create the model\n",
    "def create_model(activation, units, dropout, learning_rate, l2_penalty, depth, optimizer='adam'):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(units, kernel_size=3, activation=activation, input_shape=(len(X.columns), 1)))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    for _ in range(depth - 1):\n",
    "        model.add(Conv1D(units, kernel_size=3, activation=activation))\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units, activation=activation, kernel_regularizer=l2(l2_penalty)))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(units=3, activation='softmax'))\n",
    "\n",
    "    if optimizer == 'adam':\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == 'rmsprop':\n",
    "        optimizer = RMSprop(learning_rate=learning_rate)\n",
    "    elif optimizer == 'sgd':\n",
    "        optimizer = SGD(learning_rate=learning_rate)\n",
    "    elif optimizer == 'adamw':\n",
    "        optimizer = AdamW(learning_rate=learning_rate)\n",
    "    # elif optimizer == 'nadam':\n",
    "    #     optimizer = Nadam(learning_rate=learning_rate)\n",
    "    # elif optimizer == 'adabound':\n",
    "    #     optimizer = AdaBound(learning_rate=learning_rate)\n",
    "\n",
    "    # model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=[AUC(name='auc')])\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=[AUC(name='auc'), Precision(name='precision'), Recall(name='recall')])\n",
    "    return model\n",
    "\n",
    "\n",
    "classifier = KerasClassifier(build_fn=create_model, verbose=0, activation='relu', units=50, dropout=0.2, learning_rate=0.1, l2_penalty=0.001, depth=2, optimizer='adam')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=0.00001)\n",
    "\n",
    "# Define cross-validation\n",
    "cv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Define parameter space\n",
    "param_space = {\n",
    "    'depth': [2],\n",
    "    'activation': ['relu', 'tanh', 'swish', 'selu'],\n",
    "    'units': [64, 128, 256, 512],\n",
    "    'dropout': [0.1, 0.2, 0.3, 0.4],\n",
    "    'learning_rate': [0.01, 0.001, 0.0001],\n",
    "    'epochs': [10, 20, 30, 50, 100],\n",
    "    'batch_size': [32, 64, 128],\n",
    "    'optimizer': ['adam', 'adamw', 'rmsprop', 'sgd'],\n",
    "    # 'optimizer': ['adam', , 'nadam', 'adabound'],\n",
    "    'l2_penalty': [0.001, 0.01, 0.1]\n",
    "}\n",
    "\n",
    "def categorical_crossentropy_loss(estimator, X_test, y_test):\n",
    "    y_pred = estimator.predict_proba(X_test)\n",
    "    loss = log_loss(y_test, y_pred)\n",
    "    return loss\n",
    "\n",
    "# Perform Bayesian optimization\n",
    "bayes_search = BayesSearchCV(classifier, param_space, scoring=categorical_crossentropy_loss, verbose=0, cv=cv)\n",
    "bayes_result = bayes_search.fit(X_scaled, y_one_hot, callbacks=[early_stopping, reduce_lr])\n",
    "# bayes_result = bayes_search.fit(X, y_one_hot, callbacks=[early_stopping, reduce_lr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 1.6261478044474036\n",
      "Best parameters: OrderedDict([('activation', 'tanh'), ('batch_size', 16), ('depth', 2), ('dropout', 0.3019717632848964), ('epochs', 50), ('l2_penalty', 0.1), ('learning_rate', 0.0018813338707884845), ('optimizer', 'sgd'), ('units', 128)])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KerasClassifier(\n",
       "\tmodel=None\n",
       "\tbuild_fn=&lt;function create_model at 0x7f83cea25e50&gt;\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=sgd\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=16\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=0\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=50\n",
       "\tactivation=tanh\n",
       "\tunits=128\n",
       "\tdropout=0.3019717632848964\n",
       "\tlearning_rate=0.0018813338707884845\n",
       "\tl2_penalty=0.1\n",
       "\tdepth=2\n",
       "\tclass_weight=None\n",
       ")</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KerasClassifier</label><div class=\"sk-toggleable__content\"><pre>KerasClassifier(\n",
       "\tmodel=None\n",
       "\tbuild_fn=&lt;function create_model at 0x7f83cea25e50&gt;\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=sgd\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=16\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=0\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=50\n",
       "\tactivation=tanh\n",
       "\tunits=128\n",
       "\tdropout=0.3019717632848964\n",
       "\tlearning_rate=0.0018813338707884845\n",
       "\tl2_penalty=0.1\n",
       "\tdepth=2\n",
       "\tclass_weight=None\n",
       ")</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KerasClassifier(\n",
       "\tmodel=None\n",
       "\tbuild_fn=<function create_model at 0x7f83cea25e50>\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=sgd\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=16\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=0\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=50\n",
       "\tactivation=tanh\n",
       "\tunits=128\n",
       "\tdropout=0.3019717632848964\n",
       "\tlearning_rate=0.0018813338707884845\n",
       "\tl2_penalty=0.1\n",
       "\tdepth=2\n",
       "\tclass_weight=None\n",
       ")"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show best results\n",
    "print(\"Best score:\", bayes_result.best_score_)\n",
    "print(\"Best parameters:\", bayes_result.best_params_)\n",
    "\n",
    "# Entrenar el modelo con los mejores hiperparámetros\n",
    "best_model = bayes_result.best_estimator_\n",
    "best_model.fit(X_scaled, y_one_hot)\n",
    "# best_model.fit(X, y_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history = bayes_result.best_estimator_.model_.history\n",
    "\n",
    "plt.plot(history.history['auc'])\n",
    "# plt.plot(history.history['val_auc'])\n",
    "plt.title('Model AUC')\n",
    "plt.ylabel('AUC')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 mejores modelos:\n",
      "Modelo 1\n",
      "Hiperparámetros: OrderedDict([('activation', 'tanh'), ('batch_size', 16), ('depth', 2), ('dropout', 0.3019717632848964), ('epochs', 50), ('l2_penalty', 0.1), ('learning_rate', 0.0018813338707884845), ('optimizer', 'sgd'), ('units', 128)])\n",
      "Puntaje: 1.6261478044474036\n",
      "Modelo 2\n",
      "Hiperparámetros: OrderedDict([('activation', 'tanh'), ('batch_size', 16), ('depth', 2), ('dropout', 0.3), ('epochs', 50), ('l2_penalty', 0.1), ('learning_rate', 0.001), ('optimizer', 'sgd'), ('units', 64)])\n",
      "Puntaje: 1.6255124043938385\n",
      "Modelo 3\n",
      "Hiperparámetros: OrderedDict([('activation', 'tanh'), ('batch_size', 16), ('depth', 2), ('dropout', 0.34377204430256), ('epochs', 50), ('l2_penalty', 0.1), ('learning_rate', 0.0010035637077416323), ('optimizer', 'sgd'), ('units', 128)])\n",
      "Puntaje: 1.5148183824676134\n",
      "Modelo 4\n",
      "Hiperparámetros: OrderedDict([('activation', 'tanh'), ('batch_size', 16), ('depth', 2), ('dropout', 0.3), ('epochs', 50), ('l2_penalty', 0.1), ('learning_rate', 0.001), ('optimizer', 'sgd'), ('units', 128)])\n",
      "Puntaje: 1.4935397311551237\n",
      "Modelo 5\n",
      "Hiperparámetros: OrderedDict([('activation', 'tanh'), ('batch_size', 16), ('depth', 2), ('dropout', 0.3), ('epochs', 50), ('l2_penalty', 0.3), ('learning_rate', 0.001), ('optimizer', 'sgd'), ('units', 64)])\n",
      "Puntaje: 1.4816676823507509\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Obtener los hiperparámetros y puntajes de los 5 mejores modelos\n",
    "top_n_models = 5\n",
    "best_params_list = []\n",
    "best_scores_list = []\n",
    "\n",
    "if 'rank_test_score' in bayes_search.cv_results_:\n",
    "    sorted_indices = np.argsort(bayes_search.cv_results_['rank_test_score'])\n",
    "\n",
    "    for i in range(min(top_n_models, len(sorted_indices))):\n",
    "        best_params_list.append(bayes_search.cv_results_['params'][sorted_indices[i]])\n",
    "        best_scores_list.append(bayes_search.cv_results_['mean_test_score'][sorted_indices[i]])\n",
    "\n",
    "    # Guardar los hiperparámetros de los 5 mejores modelos en un archivo JSON\n",
    "    with open('conv_classifier/top_5_hyperparameters.json', 'w') as f:\n",
    "        json.dump({'best_params': best_params_list, 'best_scores': best_scores_list}, f)\n",
    "\n",
    "    # O imprimir los hiperparámetros\n",
    "    print(\"Top 5 mejores modelos:\")\n",
    "    for i in range(len(best_params_list)):\n",
    "        print(\"Modelo\", i + 1)\n",
    "        print(\"Hiperparámetros:\", best_params_list[i])\n",
    "        print(\"Puntaje:\", best_scores_list[i])\n",
    "\n",
    "else:\n",
    "    print(\"Error: 'rank_test_score' no encontrado en cv_results_\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Armado del ensamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prime_seeds(n):\n",
    "    seeds = []\n",
    "    num = 70001  # Comenzamos desde el primer número primo mayor que 70000\n",
    "    while len(seeds) < n:\n",
    "        is_prime = True\n",
    "        for i in range(2, int(num**0.5) + 1):\n",
    "            if num % i == 0:\n",
    "                is_prime = False\n",
    "                break\n",
    "        if is_prime:\n",
    "            seeds.append(num)\n",
    "        num += 1\n",
    "    return seeds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clase personalizada para hacer el ensamble, dado que sklearn no provee ninguna clase que permita hacer ensmble\n",
    "## de modelos re regresion multivariados\n",
    "class MultivariableVotingClassifier:\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for model in self.models:\n",
    "            model.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Hacer predicciones con cada modelo\n",
    "        predictions = np.array([model.predict(X) for model in self.models])\n",
    "        \n",
    "        # Calcular la moda de las predicciones\n",
    "        mode_predictions = np.argmax(np.sum(predictions, axis=0), axis=1)\n",
    "        \n",
    "        return mode_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model number 4, seed number 5\n",
      "model number 4, seed number 6\n",
      "model number 4, seed number 7\n",
      "model number 4, seed number 8\n",
      "model number 4, seed number 9\n",
      "model number 4, seed number 10\n",
      "model number 4, seed number 11\n",
      "model number 4, seed number 12\n",
      "model number 4, seed number 13\n",
      "model number 4, seed number 14\n",
      "model number 4, seed number 15\n",
      "model number 4, seed number 16\n",
      "model number 4, seed number 17\n",
      "model number 4, seed number 18\n",
      "model number 4, seed number 19\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "# Leer los hiperparámetros desde el archivo JSON\n",
    "with open('conv_classifier/top_5_hyperparameters.json', 'r') as f:\n",
    "    top_hyperparameters = json.load(f)\n",
    "\n",
    "prime_seeds = generate_prime_seeds(20)\n",
    "models = []\n",
    "best_seeds= {}\n",
    "\n",
    "# Train models with different seeds for each set of hyperparameters\n",
    "for mode_number, params in enumerate(top_hyperparameters['best_params']):\n",
    "    best_validation_errors = {}\n",
    "    \n",
    "    for seed_number, seed in enumerate(prime_seeds):\n",
    "        model = KerasClassifier(build_fn=create_model, random_state=seed, verbose=0, **params)\n",
    "        model.fit(X_scaled, y_one_hot)\n",
    "        # model.fit(X, y_one_hot)\n",
    "        \n",
    "        train_error = categorical_crossentropy_loss(model, X_scaled, y_one_hot)\n",
    "        # train_error = categorical_crossentropy_loss(model, X, y_one_hot)\n",
    "        \n",
    "        mean_train_error = np.mean(train_error)\n",
    "        \n",
    "        # Update best validation error for this seed\n",
    "        best_validation_errors[seed] = mean_train_error\n",
    "        \n",
    "        print(f\"model number {mode_number}, seed number {seed_number}\")\n",
    "    \n",
    "    # print(\"Best validation errors:\", best_validation_errors)\n",
    "\n",
    "    # Find the best seed for this set of hyperparameters\n",
    "    best_seed_for_params = min(best_validation_errors, key=lambda k: best_validation_errors[k])\n",
    "    best_seeds[str(params)] = best_seed_for_params\n",
    "    \n",
    "    # Create and train the model with the best seed\n",
    "    model = KerasClassifier(build_fn=create_model, random_state=best_seed_for_params, verbose=0, **params)\n",
    "    model.fit(X_scaled, y_one_hot)\n",
    "    # model.fit(X, y_one_hot)\n",
    "    \n",
    "    models.append(model)\n",
    "\n",
    "# Define and train the ensemble model\n",
    "ensemble = MultivariableVotingClassifier(models)\n",
    "ensemble.fit(X_scaled, y_one_hot)\n",
    "# ensemble.fit(X, y_one_hot)\n",
    "\n",
    "# Save the best seeds to a JSON file\n",
    "with open('conv_classifier/best_seeds.json', 'w') as f:\n",
    "    json.dump(best_seeds, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clasificacion con el ensamble sobre las redicciones de los modelos generativos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "60/60 [==============================] - 10s 11ms/step - loss: 10.2087 - auc: 0.6018 - precision: 0.4278 - recall: 0.3541\n",
      "Epoch 2/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 2.4385 - auc: 0.6078 - precision: 0.4518 - recall: 0.3372\n",
      "Epoch 3/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.5686 - auc: 0.6058 - precision: 0.4433 - recall: 0.3140\n",
      "Epoch 4/50\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 1.3701 - auc: 0.6443 - precision: 0.5150 - recall: 0.3457\n",
      "Epoch 5/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.4612 - auc: 0.6200 - precision: 0.4855 - recall: 0.3351\n",
      "Epoch 6/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.3119 - auc: 0.6491 - precision: 0.5278 - recall: 0.3414\n",
      "Epoch 7/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.2865 - auc: 0.6403 - precision: 0.5133 - recall: 0.3266\n",
      "Epoch 8/50\n",
      "60/60 [==============================] - 1s 15ms/step - loss: 1.2253 - auc: 0.6731 - precision: 0.5583 - recall: 0.3340\n",
      "Epoch 9/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.2267 - auc: 0.6594 - precision: 0.5342 - recall: 0.2970\n",
      "Epoch 10/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.1953 - auc: 0.6756 - precision: 0.5622 - recall: 0.3393\n",
      "Epoch 11/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.1401 - auc: 0.6857 - precision: 0.5782 - recall: 0.3203\n",
      "Epoch 12/50\n",
      "60/60 [==============================] - 1s 8ms/step - loss: 1.1784 - auc: 0.6743 - precision: 0.5734 - recall: 0.3013\n",
      "Epoch 13/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.1586 - auc: 0.6804 - precision: 0.5654 - recall: 0.3245\n",
      "Epoch 14/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.1213 - auc: 0.7002 - precision: 0.5819 - recall: 0.3531\n",
      "Epoch 15/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.0895 - auc: 0.7119 - precision: 0.5983 - recall: 0.3636\n",
      "Epoch 16/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.1358 - auc: 0.7048 - precision: 0.6062 - recall: 0.3288\n",
      "Epoch 17/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.1765 - auc: 0.7176 - precision: 0.6279 - recall: 0.3763\n",
      "Epoch 18/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.1317 - auc: 0.7214 - precision: 0.6167 - recall: 0.3911\n",
      "Epoch 19/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.1427 - auc: 0.7080 - precision: 0.5830 - recall: 0.3562\n",
      "Epoch 20/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.1529 - auc: 0.7122 - precision: 0.6182 - recall: 0.3732\n",
      "Epoch 21/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.0760 - auc: 0.7253 - precision: 0.6227 - recall: 0.3647\n",
      "Epoch 22/50\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 1.0636 - auc: 0.7317 - precision: 0.6206 - recall: 0.4080\n",
      "Epoch 23/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.2021 - auc: 0.7161 - precision: 0.6138 - recall: 0.3562\n",
      "Epoch 24/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.1771 - auc: 0.7134 - precision: 0.5852 - recall: 0.3340\n",
      "Epoch 25/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.2006 - auc: 0.7172 - precision: 0.6084 - recall: 0.3827\n",
      "Epoch 26/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.3151 - auc: 0.7060 - precision: 0.6007 - recall: 0.3594\n",
      "Epoch 27/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.1764 - auc: 0.7173 - precision: 0.6091 - recall: 0.3541\n",
      "Epoch 28/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.1229 - auc: 0.7323 - precision: 0.6471 - recall: 0.3721\n",
      "Epoch 29/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.0962 - auc: 0.7282 - precision: 0.6222 - recall: 0.3848\n",
      "Epoch 30/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.1072 - auc: 0.7289 - precision: 0.6029 - recall: 0.3932\n",
      "Epoch 31/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.0607 - auc: 0.7375 - precision: 0.6362 - recall: 0.4197\n",
      "Epoch 32/50\n",
      "60/60 [==============================] - 1s 12ms/step - loss: 1.0810 - auc: 0.7414 - precision: 0.6387 - recall: 0.3943\n",
      "Epoch 33/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.0912 - auc: 0.7357 - precision: 0.6360 - recall: 0.3805\n",
      "Epoch 34/50\n",
      "60/60 [==============================] - 1s 14ms/step - loss: 1.0832 - auc: 0.7427 - precision: 0.6649 - recall: 0.4006\n",
      "Epoch 35/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.0888 - auc: 0.7459 - precision: 0.6524 - recall: 0.4186\n",
      "Epoch 36/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.0892 - auc: 0.7549 - precision: 0.6439 - recall: 0.4376\n",
      "Epoch 37/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.0980 - auc: 0.7345 - precision: 0.6189 - recall: 0.4017\n",
      "Epoch 38/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.0973 - auc: 0.7396 - precision: 0.6286 - recall: 0.4186\n",
      "Epoch 39/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.0608 - auc: 0.7472 - precision: 0.6231 - recall: 0.4228\n",
      "Epoch 40/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.0461 - auc: 0.7595 - precision: 0.6765 - recall: 0.4133\n",
      "Epoch 41/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.0599 - auc: 0.7537 - precision: 0.6483 - recall: 0.4345\n",
      "Epoch 42/50\n",
      "60/60 [==============================] - 1s 8ms/step - loss: 1.0375 - auc: 0.7619 - precision: 0.6477 - recall: 0.4218\n",
      "Epoch 43/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.0522 - auc: 0.7537 - precision: 0.6418 - recall: 0.4545\n",
      "Epoch 44/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.0585 - auc: 0.7553 - precision: 0.6540 - recall: 0.4376\n",
      "Epoch 45/50\n",
      "60/60 [==============================] - 1s 8ms/step - loss: 1.0713 - auc: 0.7592 - precision: 0.6457 - recall: 0.4450\n",
      "Epoch 46/50\n",
      "60/60 [==============================] - 1s 12ms/step - loss: 1.0709 - auc: 0.7523 - precision: 0.6540 - recall: 0.4017\n",
      "Epoch 47/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.1181 - auc: 0.7505 - precision: 0.6250 - recall: 0.4281\n",
      "Epoch 48/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.0131 - auc: 0.7830 - precision: 0.6784 - recall: 0.4704\n",
      "Epoch 49/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.0519 - auc: 0.7664 - precision: 0.6484 - recall: 0.4366\n",
      "Epoch 50/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.0279 - auc: 0.7782 - precision: 0.6470 - recall: 0.4514\n",
      "Epoch 1/50\n",
      "60/60 [==============================] - 9s 11ms/step - loss: 8.3563 - auc: 0.5757 - precision: 0.4168 - recall: 0.3203\n",
      "Epoch 2/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 4.6230 - auc: 0.6509 - precision: 0.5078 - recall: 0.3763\n",
      "Epoch 3/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 2.9073 - auc: 0.6457 - precision: 0.4888 - recall: 0.3689\n",
      "Epoch 4/50\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 2.0935 - auc: 0.6658 - precision: 0.5090 - recall: 0.3901\n",
      "Epoch 5/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.6647 - auc: 0.6655 - precision: 0.5347 - recall: 0.3742\n",
      "Epoch 6/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.4050 - auc: 0.6905 - precision: 0.5460 - recall: 0.4080\n",
      "Epoch 7/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.3017 - auc: 0.6803 - precision: 0.5415 - recall: 0.3795\n",
      "Epoch 8/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.2209 - auc: 0.6880 - precision: 0.5465 - recall: 0.4101\n",
      "Epoch 9/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.2052 - auc: 0.6751 - precision: 0.5361 - recall: 0.3763\n",
      "Epoch 10/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.1247 - auc: 0.7021 - precision: 0.5694 - recall: 0.3901\n",
      "Epoch 11/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.1407 - auc: 0.6925 - precision: 0.5648 - recall: 0.4006\n",
      "Epoch 12/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.1170 - auc: 0.6873 - precision: 0.5430 - recall: 0.3668\n",
      "Epoch 13/50\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 1.0741 - auc: 0.7155 - precision: 0.5710 - recall: 0.4080\n",
      "Epoch 14/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.0969 - auc: 0.7088 - precision: 0.5811 - recall: 0.3975\n",
      "Epoch 15/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.0375 - auc: 0.7391 - precision: 0.6025 - recall: 0.4133\n",
      "Epoch 16/50\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 1.0315 - auc: 0.7420 - precision: 0.6160 - recall: 0.4239\n",
      "Epoch 17/50\n",
      "60/60 [==============================] - 1s 8ms/step - loss: 1.0612 - auc: 0.7208 - precision: 0.5818 - recall: 0.4133\n",
      "Epoch 18/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.0379 - auc: 0.7366 - precision: 0.6149 - recall: 0.4271\n",
      "Epoch 19/50\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 1.0116 - auc: 0.7461 - precision: 0.6367 - recall: 0.4186\n",
      "Epoch 20/50\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 1.0555 - auc: 0.7283 - precision: 0.6023 - recall: 0.3858\n",
      "Epoch 21/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.0195 - auc: 0.7540 - precision: 0.6265 - recall: 0.4345\n",
      "Epoch 22/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.9949 - auc: 0.7618 - precision: 0.6449 - recall: 0.4281\n",
      "Epoch 23/50\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 1.0073 - auc: 0.7590 - precision: 0.6218 - recall: 0.4397\n",
      "Epoch 24/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.0063 - auc: 0.7586 - precision: 0.6340 - recall: 0.4376\n",
      "Epoch 25/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.9826 - auc: 0.7755 - precision: 0.6696 - recall: 0.4820\n",
      "Epoch 26/50\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.9871 - auc: 0.7699 - precision: 0.6452 - recall: 0.4651\n",
      "Epoch 27/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.0041 - auc: 0.7670 - precision: 0.6708 - recall: 0.4567\n",
      "Epoch 28/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.0050 - auc: 0.7699 - precision: 0.6581 - recall: 0.4577\n",
      "Epoch 29/50\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.9838 - auc: 0.7750 - precision: 0.6631 - recall: 0.4535\n",
      "Epoch 30/50\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 0.9800 - auc: 0.7797 - precision: 0.6662 - recall: 0.4810\n",
      "Epoch 31/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.9212 - auc: 0.8079 - precision: 0.6824 - recall: 0.5201\n",
      "Epoch 32/50\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 0.9332 - auc: 0.7999 - precision: 0.6721 - recall: 0.4831\n",
      "Epoch 33/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.9672 - auc: 0.7862 - precision: 0.6736 - recall: 0.4820\n",
      "Epoch 34/50\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.9507 - auc: 0.7896 - precision: 0.6830 - recall: 0.4715\n",
      "Epoch 35/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.9350 - auc: 0.8032 - precision: 0.6846 - recall: 0.5233\n",
      "Epoch 36/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.9491 - auc: 0.7961 - precision: 0.6836 - recall: 0.4979\n",
      "Epoch 37/50\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.9213 - auc: 0.8072 - precision: 0.6812 - recall: 0.5106\n",
      "Epoch 38/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.9521 - auc: 0.8024 - precision: 0.6827 - recall: 0.5095\n",
      "Epoch 39/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.9126 - auc: 0.8230 - precision: 0.7199 - recall: 0.5381\n",
      "Epoch 40/50\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.8867 - auc: 0.8293 - precision: 0.7076 - recall: 0.5423\n",
      "Epoch 41/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.8979 - auc: 0.8211 - precision: 0.7027 - recall: 0.5296\n",
      "Epoch 42/50\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 0.9266 - auc: 0.8094 - precision: 0.6883 - recall: 0.5370\n",
      "Epoch 43/50\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 0.9846 - auc: 0.7868 - precision: 0.6682 - recall: 0.4725\n",
      "Epoch 44/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.9011 - auc: 0.8299 - precision: 0.7088 - recall: 0.5507\n",
      "Epoch 45/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.9462 - auc: 0.8063 - precision: 0.6849 - recall: 0.5032\n",
      "Epoch 46/50\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.8920 - auc: 0.8321 - precision: 0.7269 - recall: 0.5402\n",
      "Epoch 47/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.8859 - auc: 0.8309 - precision: 0.7125 - recall: 0.5423\n",
      "Epoch 48/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.8731 - auc: 0.8362 - precision: 0.6963 - recall: 0.5624\n",
      "Epoch 49/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.9100 - auc: 0.8217 - precision: 0.7089 - recall: 0.5740\n",
      "Epoch 50/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.8822 - auc: 0.8360 - precision: 0.7194 - recall: 0.5772\n",
      "Epoch 1/50\n",
      "60/60 [==============================] - 12s 12ms/step - loss: 13.6467 - auc: 0.6088 - precision: 0.4431 - recall: 0.3584\n",
      "Epoch 2/50\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 5.4389 - auc: 0.6302 - precision: 0.4766 - recall: 0.3658\n",
      "Epoch 3/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 2.7375 - auc: 0.6416 - precision: 0.5000 - recall: 0.3658\n",
      "Epoch 4/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.8189 - auc: 0.6472 - precision: 0.4934 - recall: 0.3573\n",
      "Epoch 5/50\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 1.5124 - auc: 0.6560 - precision: 0.5091 - recall: 0.3837\n",
      "Epoch 6/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.3135 - auc: 0.6787 - precision: 0.5370 - recall: 0.3837\n",
      "Epoch 7/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.3026 - auc: 0.6731 - precision: 0.5349 - recall: 0.3890\n",
      "Epoch 8/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.2659 - auc: 0.6810 - precision: 0.5431 - recall: 0.3932\n",
      "Epoch 9/50\n",
      "60/60 [==============================] - 1s 13ms/step - loss: 1.2767 - auc: 0.6531 - precision: 0.4993 - recall: 0.3573\n",
      "Epoch 10/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.1711 - auc: 0.7049 - precision: 0.5789 - recall: 0.3953\n",
      "Epoch 11/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.1761 - auc: 0.6916 - precision: 0.5769 - recall: 0.3964\n",
      "Epoch 12/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.1578 - auc: 0.6957 - precision: 0.5726 - recall: 0.3753\n",
      "Epoch 13/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.1771 - auc: 0.6968 - precision: 0.5737 - recall: 0.3742\n",
      "Epoch 14/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.1419 - auc: 0.7015 - precision: 0.5829 - recall: 0.3901\n",
      "Epoch 15/50\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 1.1197 - auc: 0.7086 - precision: 0.5791 - recall: 0.3753\n",
      "Epoch 16/50\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 1.1043 - auc: 0.7114 - precision: 0.5964 - recall: 0.3827\n",
      "Epoch 17/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.1082 - auc: 0.7251 - precision: 0.5878 - recall: 0.3858\n",
      "Epoch 18/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.0999 - auc: 0.7324 - precision: 0.6104 - recall: 0.3975\n",
      "Epoch 19/50\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 1.1118 - auc: 0.7310 - precision: 0.6006 - recall: 0.4006\n",
      "Epoch 20/50\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 1.1341 - auc: 0.7263 - precision: 0.6202 - recall: 0.3953\n",
      "Epoch 21/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.1230 - auc: 0.7215 - precision: 0.5905 - recall: 0.3932\n",
      "Epoch 22/50\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 1.1057 - auc: 0.7331 - precision: 0.6314 - recall: 0.3911\n",
      "Epoch 23/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.0840 - auc: 0.7494 - precision: 0.6196 - recall: 0.4271\n",
      "Epoch 24/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.0604 - auc: 0.7513 - precision: 0.6399 - recall: 0.4207\n",
      "Epoch 25/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.0613 - auc: 0.7455 - precision: 0.6324 - recall: 0.4165\n",
      "Epoch 26/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.0410 - auc: 0.7615 - precision: 0.6592 - recall: 0.4376\n",
      "Epoch 27/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.1360 - auc: 0.7422 - precision: 0.6134 - recall: 0.4175\n",
      "Epoch 28/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.1025 - auc: 0.7435 - precision: 0.6246 - recall: 0.4186\n",
      "Epoch 29/50\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 1.0598 - auc: 0.7554 - precision: 0.6462 - recall: 0.4228\n",
      "Epoch 30/50\n",
      "60/60 [==============================] - 1s 12ms/step - loss: 1.0480 - auc: 0.7646 - precision: 0.6471 - recall: 0.4440\n",
      "Epoch 31/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.0431 - auc: 0.7674 - precision: 0.6495 - recall: 0.4387\n",
      "Epoch 32/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.0476 - auc: 0.7739 - precision: 0.6617 - recall: 0.4651\n",
      "Epoch 33/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.0341 - auc: 0.7755 - precision: 0.6687 - recall: 0.4651\n",
      "Epoch 34/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.0117 - auc: 0.7803 - precision: 0.6698 - recall: 0.4588\n",
      "Epoch 35/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.0334 - auc: 0.7683 - precision: 0.6510 - recall: 0.4535\n",
      "Epoch 36/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.0259 - auc: 0.7698 - precision: 0.6485 - recall: 0.4524\n",
      "Epoch 37/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.9828 - auc: 0.7955 - precision: 0.6828 - recall: 0.4778\n",
      "Epoch 38/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.0153 - auc: 0.7982 - precision: 0.6844 - recall: 0.5042\n",
      "Epoch 39/50\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 1.0133 - auc: 0.7903 - precision: 0.6581 - recall: 0.4884\n",
      "Epoch 40/50\n",
      "60/60 [==============================] - 1s 12ms/step - loss: 1.0648 - auc: 0.7800 - precision: 0.6602 - recall: 0.4662\n",
      "Epoch 41/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.1141 - auc: 0.7759 - precision: 0.6558 - recall: 0.4672\n",
      "Epoch 42/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.1049 - auc: 0.7878 - precision: 0.6780 - recall: 0.4831\n",
      "Epoch 43/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.0477 - auc: 0.7986 - precision: 0.6823 - recall: 0.4926\n",
      "Epoch 44/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.0139 - auc: 0.7985 - precision: 0.6839 - recall: 0.5169\n",
      "Epoch 45/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.9802 - auc: 0.8134 - precision: 0.7033 - recall: 0.5338\n",
      "Epoch 46/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.0162 - auc: 0.8105 - precision: 0.6900 - recall: 0.5317\n",
      "Epoch 47/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.0516 - auc: 0.7966 - precision: 0.6713 - recall: 0.5137\n",
      "Epoch 48/50\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 0.9665 - auc: 0.8252 - precision: 0.7082 - recall: 0.5412\n",
      "Epoch 49/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.9483 - auc: 0.8261 - precision: 0.6946 - recall: 0.5433\n",
      "Epoch 50/50\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 1.0169 - auc: 0.8157 - precision: 0.7007 - recall: 0.5222\n",
      "Epoch 1/50\n",
      "60/60 [==============================] - 9s 11ms/step - loss: 13.6669 - auc: 0.6113 - precision: 0.4558 - recall: 0.3541\n",
      "Epoch 2/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 5.3670 - auc: 0.6332 - precision: 0.4697 - recall: 0.3605\n",
      "Epoch 3/50\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 2.6668 - auc: 0.6262 - precision: 0.4632 - recall: 0.3531\n",
      "Epoch 4/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.7635 - auc: 0.6413 - precision: 0.4909 - recall: 0.3436\n",
      "Epoch 5/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.4308 - auc: 0.6732 - precision: 0.5340 - recall: 0.3985\n",
      "Epoch 6/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.3309 - auc: 0.6570 - precision: 0.5215 - recall: 0.3584\n",
      "Epoch 7/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.2735 - auc: 0.6630 - precision: 0.5255 - recall: 0.3700\n",
      "Epoch 8/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.2856 - auc: 0.6491 - precision: 0.5195 - recall: 0.3520\n",
      "Epoch 9/50\n",
      "60/60 [==============================] - 1s 12ms/step - loss: 1.2802 - auc: 0.6673 - precision: 0.5563 - recall: 0.3605\n",
      "Epoch 10/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.2812 - auc: 0.6809 - precision: 0.5545 - recall: 0.3710\n",
      "Epoch 11/50\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 1.2411 - auc: 0.6820 - precision: 0.5535 - recall: 0.3827\n",
      "Epoch 12/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.1858 - auc: 0.6920 - precision: 0.5645 - recall: 0.3837\n",
      "Epoch 13/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.1646 - auc: 0.6941 - precision: 0.5627 - recall: 0.3700\n",
      "Epoch 14/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.1402 - auc: 0.7087 - precision: 0.5714 - recall: 0.3932\n",
      "Epoch 15/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.2043 - auc: 0.6983 - precision: 0.5747 - recall: 0.3742\n",
      "Epoch 16/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.2841 - auc: 0.6799 - precision: 0.5394 - recall: 0.3763\n",
      "Epoch 17/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.2046 - auc: 0.7113 - precision: 0.6065 - recall: 0.3732\n",
      "Epoch 18/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.1541 - auc: 0.7218 - precision: 0.6111 - recall: 0.4070\n",
      "Epoch 19/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.1261 - auc: 0.7137 - precision: 0.5852 - recall: 0.3848\n",
      "Epoch 20/50\n",
      "60/60 [==============================] - 1s 14ms/step - loss: 1.0935 - auc: 0.7313 - precision: 0.6202 - recall: 0.4091\n",
      "Epoch 21/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.0914 - auc: 0.7420 - precision: 0.6284 - recall: 0.4345\n",
      "Epoch 22/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.0779 - auc: 0.7412 - precision: 0.6402 - recall: 0.4175\n",
      "Epoch 23/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.0754 - auc: 0.7412 - precision: 0.6099 - recall: 0.4165\n",
      "Epoch 24/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.0677 - auc: 0.7349 - precision: 0.6112 - recall: 0.4154\n",
      "Epoch 25/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.0590 - auc: 0.7532 - precision: 0.6482 - recall: 0.4323\n",
      "Epoch 26/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.0752 - auc: 0.7507 - precision: 0.6278 - recall: 0.4207\n",
      "Epoch 27/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.0437 - auc: 0.7542 - precision: 0.6341 - recall: 0.4323\n",
      "Epoch 28/50\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 1.0119 - auc: 0.7782 - precision: 0.6651 - recall: 0.4514\n",
      "Epoch 29/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.0215 - auc: 0.7697 - precision: 0.6400 - recall: 0.4567\n",
      "Epoch 30/50\n",
      "60/60 [==============================] - 1s 12ms/step - loss: 1.0193 - auc: 0.7662 - precision: 0.6562 - recall: 0.4440\n",
      "Epoch 31/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.0048 - auc: 0.7697 - precision: 0.6407 - recall: 0.4429\n",
      "Epoch 32/50\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 1.0018 - auc: 0.7811 - precision: 0.6521 - recall: 0.4736\n",
      "Epoch 33/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.0380 - auc: 0.7772 - precision: 0.6617 - recall: 0.4693\n",
      "Epoch 34/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.9746 - auc: 0.8084 - precision: 0.6937 - recall: 0.4979\n",
      "Epoch 35/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.0168 - auc: 0.7807 - precision: 0.6647 - recall: 0.4715\n",
      "Epoch 36/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.0362 - auc: 0.7801 - precision: 0.6591 - recall: 0.4577\n",
      "Epoch 37/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.0231 - auc: 0.7893 - precision: 0.6681 - recall: 0.4958\n",
      "Epoch 38/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.0868 - auc: 0.7647 - precision: 0.6542 - recall: 0.4641\n",
      "Epoch 39/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.0977 - auc: 0.7723 - precision: 0.6543 - recall: 0.4461\n",
      "Epoch 40/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.9901 - auc: 0.8108 - precision: 0.7119 - recall: 0.4989\n",
      "Epoch 41/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.9762 - auc: 0.8118 - precision: 0.6852 - recall: 0.5201\n",
      "Epoch 42/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.9906 - auc: 0.8025 - precision: 0.6919 - recall: 0.5032\n",
      "Epoch 43/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.9487 - auc: 0.8218 - precision: 0.7174 - recall: 0.5285\n",
      "Epoch 44/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.9558 - auc: 0.8191 - precision: 0.7153 - recall: 0.5338\n",
      "Epoch 45/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.9798 - auc: 0.8145 - precision: 0.6891 - recall: 0.5412\n",
      "Epoch 46/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.0015 - auc: 0.8017 - precision: 0.6718 - recall: 0.5106\n",
      "Epoch 47/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.9690 - auc: 0.8112 - precision: 0.6914 - recall: 0.5518\n",
      "Epoch 48/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.9781 - auc: 0.8126 - precision: 0.6917 - recall: 0.5264\n",
      "Epoch 49/50\n",
      "60/60 [==============================] - 1s 13ms/step - loss: 0.9340 - auc: 0.8323 - precision: 0.7113 - recall: 0.5782\n",
      "Epoch 50/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.9605 - auc: 0.8146 - precision: 0.6768 - recall: 0.5402\n",
      "Epoch 1/50\n",
      "60/60 [==============================] - 11s 11ms/step - loss: 19.3020 - auc: 0.6136 - precision: 0.4381 - recall: 0.3478\n",
      "Epoch 2/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 6.9626 - auc: 0.6416 - precision: 0.4822 - recall: 0.3721\n",
      "Epoch 3/50\n",
      "60/60 [==============================] - 1s 8ms/step - loss: 3.0402 - auc: 0.6501 - precision: 0.5145 - recall: 0.3742\n",
      "Epoch 4/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.8664 - auc: 0.6619 - precision: 0.5197 - recall: 0.3763\n",
      "Epoch 5/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.5152 - auc: 0.6538 - precision: 0.5140 - recall: 0.3679\n",
      "Epoch 6/50\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 1.3573 - auc: 0.6623 - precision: 0.5167 - recall: 0.3594\n",
      "Epoch 7/50\n",
      "60/60 [==============================] - 1s 13ms/step - loss: 1.2718 - auc: 0.6641 - precision: 0.5251 - recall: 0.3763\n",
      "Epoch 8/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.1945 - auc: 0.6875 - precision: 0.5491 - recall: 0.3901\n",
      "Epoch 9/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.1837 - auc: 0.6772 - precision: 0.5310 - recall: 0.3710\n",
      "Epoch 10/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.1504 - auc: 0.6946 - precision: 0.5595 - recall: 0.3827\n",
      "Epoch 11/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.1606 - auc: 0.6842 - precision: 0.5730 - recall: 0.3774\n",
      "Epoch 12/50\n",
      "60/60 [==============================] - 1s 8ms/step - loss: 1.1417 - auc: 0.6867 - precision: 0.5627 - recall: 0.3510\n",
      "Epoch 13/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.1556 - auc: 0.6760 - precision: 0.5468 - recall: 0.3584\n",
      "Epoch 14/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.1092 - auc: 0.7024 - precision: 0.5836 - recall: 0.3837\n",
      "Epoch 15/50\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 1.1140 - auc: 0.7039 - precision: 0.5795 - recall: 0.3774\n",
      "Epoch 16/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.1031 - auc: 0.7050 - precision: 0.5986 - recall: 0.3689\n",
      "Epoch 17/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.0821 - auc: 0.7184 - precision: 0.5984 - recall: 0.4049\n",
      "Epoch 18/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.1239 - auc: 0.7173 - precision: 0.6071 - recall: 0.3985\n",
      "Epoch 19/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.1231 - auc: 0.7075 - precision: 0.5908 - recall: 0.3784\n",
      "Epoch 20/50\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 1.0705 - auc: 0.7293 - precision: 0.6072 - recall: 0.4101\n",
      "Epoch 21/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.0466 - auc: 0.7318 - precision: 0.6028 - recall: 0.4027\n",
      "Epoch 22/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.0456 - auc: 0.7324 - precision: 0.6026 - recall: 0.3975\n",
      "Epoch 23/50\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 1.0838 - auc: 0.7282 - precision: 0.6330 - recall: 0.3975\n",
      "Epoch 24/50\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 1.0906 - auc: 0.7204 - precision: 0.6122 - recall: 0.3721\n",
      "Epoch 25/50\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 1.0743 - auc: 0.7323 - precision: 0.6284 - recall: 0.3932\n",
      "Epoch 26/50\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 1.0851 - auc: 0.7257 - precision: 0.6211 - recall: 0.3985\n",
      "Epoch 27/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.0351 - auc: 0.7370 - precision: 0.6478 - recall: 0.3869\n",
      "Epoch 28/50\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 1.0537 - auc: 0.7341 - precision: 0.6397 - recall: 0.4017\n",
      "Epoch 29/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.0233 - auc: 0.7501 - precision: 0.6408 - recall: 0.4017\n",
      "Epoch 30/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.0566 - auc: 0.7328 - precision: 0.6083 - recall: 0.3858\n",
      "Epoch 31/50\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 1.0031 - auc: 0.7510 - precision: 0.6380 - recall: 0.4080\n",
      "Epoch 32/50\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 1.0036 - auc: 0.7525 - precision: 0.6377 - recall: 0.4112\n",
      "Epoch 33/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.9982 - auc: 0.7570 - precision: 0.6555 - recall: 0.4123\n",
      "Epoch 34/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.9913 - auc: 0.7706 - precision: 0.6656 - recall: 0.4334\n",
      "Epoch 35/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.9871 - auc: 0.7695 - precision: 0.6700 - recall: 0.4313\n",
      "Epoch 36/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.9727 - auc: 0.7786 - precision: 0.6778 - recall: 0.4493\n",
      "Epoch 37/50\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.9916 - auc: 0.7682 - precision: 0.6508 - recall: 0.4471\n",
      "Epoch 38/50\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 1.0045 - auc: 0.7603 - precision: 0.6423 - recall: 0.4271\n",
      "Epoch 39/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.9988 - auc: 0.7643 - precision: 0.6651 - recall: 0.4471\n",
      "Epoch 40/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.0018 - auc: 0.7631 - precision: 0.6755 - recall: 0.4334\n",
      "Epoch 41/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.0013 - auc: 0.7656 - precision: 0.6667 - recall: 0.4376\n",
      "Epoch 42/50\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 0.9741 - auc: 0.7763 - precision: 0.6651 - recall: 0.4493\n",
      "Epoch 43/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.9767 - auc: 0.7774 - precision: 0.6718 - recall: 0.4609\n",
      "Epoch 44/50\n",
      "60/60 [==============================] - 1s 8ms/step - loss: 0.9547 - auc: 0.7904 - precision: 0.6888 - recall: 0.4820\n",
      "Epoch 45/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.0086 - auc: 0.7700 - precision: 0.6651 - recall: 0.4619\n",
      "Epoch 46/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.9970 - auc: 0.7766 - precision: 0.6758 - recall: 0.4450\n",
      "Epoch 47/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.9939 - auc: 0.7837 - precision: 0.6749 - recall: 0.4609\n",
      "Epoch 48/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.9617 - auc: 0.7940 - precision: 0.6810 - recall: 0.4672\n",
      "Epoch 49/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.9687 - auc: 0.7903 - precision: 0.6824 - recall: 0.4725\n",
      "Epoch 50/50\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 0.9401 - auc: 0.8048 - precision: 0.6988 - recall: 0.4757\n"
     ]
    }
   ],
   "source": [
    "with open('conv_classifier/best_seeds.json', 'r') as f:\n",
    "    best_seeds = json.load(f)\n",
    "\n",
    "# 21 Crear y entrenar los modelos con los hiperparámetros y semillas guardados\n",
    "models = []\n",
    "for params_str, seed in best_seeds.items():\n",
    "    params = json.loads(params_str.replace(\"'\", \"\\\"\"))\n",
    "    model = KerasClassifier(build_fn=create_model, random_state=seed, **params)\n",
    "    model.fit(X_scaled, y_one_hot)\n",
    "    # model.fit(X, y_one_hot)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "60/60 [==============================] - 11s 10ms/step - loss: 10.2087 - auc: 0.6018 - precision: 0.4278 - recall: 0.3541\n",
      "Epoch 2/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 2.4385 - auc: 0.6078 - precision: 0.4518 - recall: 0.3372\n",
      "Epoch 3/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.5686 - auc: 0.6058 - precision: 0.4433 - recall: 0.3140\n",
      "Epoch 4/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.3701 - auc: 0.6443 - precision: 0.5150 - recall: 0.3457\n",
      "Epoch 5/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.4612 - auc: 0.6200 - precision: 0.4855 - recall: 0.3351\n",
      "Epoch 6/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.3119 - auc: 0.6491 - precision: 0.5278 - recall: 0.3414\n",
      "Epoch 7/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.2865 - auc: 0.6403 - precision: 0.5133 - recall: 0.3266\n",
      "Epoch 8/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.2253 - auc: 0.6731 - precision: 0.5583 - recall: 0.3340\n",
      "Epoch 9/50\n",
      "60/60 [==============================] - 1s 12ms/step - loss: 1.2267 - auc: 0.6594 - precision: 0.5342 - recall: 0.2970\n",
      "Epoch 10/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.1953 - auc: 0.6756 - precision: 0.5622 - recall: 0.3393\n",
      "Epoch 11/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.1401 - auc: 0.6857 - precision: 0.5782 - recall: 0.3203\n",
      "Epoch 12/50\n",
      "60/60 [==============================] - 1s 8ms/step - loss: 1.1784 - auc: 0.6743 - precision: 0.5734 - recall: 0.3013\n",
      "Epoch 13/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.1586 - auc: 0.6804 - precision: 0.5654 - recall: 0.3245\n",
      "Epoch 14/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.1213 - auc: 0.7002 - precision: 0.5819 - recall: 0.3531\n",
      "Epoch 15/50\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 1.0895 - auc: 0.7119 - precision: 0.5983 - recall: 0.3636\n",
      "Epoch 16/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.1358 - auc: 0.7048 - precision: 0.6062 - recall: 0.3288\n",
      "Epoch 17/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.1765 - auc: 0.7176 - precision: 0.6279 - recall: 0.3763\n",
      "Epoch 18/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.1317 - auc: 0.7214 - precision: 0.6167 - recall: 0.3911\n",
      "Epoch 19/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.1427 - auc: 0.7080 - precision: 0.5830 - recall: 0.3562\n",
      "Epoch 20/50\n",
      "60/60 [==============================] - 1s 12ms/step - loss: 1.1529 - auc: 0.7122 - precision: 0.6182 - recall: 0.3732\n",
      "Epoch 21/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.0760 - auc: 0.7253 - precision: 0.6227 - recall: 0.3647\n",
      "Epoch 22/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.0636 - auc: 0.7317 - precision: 0.6206 - recall: 0.4080\n",
      "Epoch 23/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.2021 - auc: 0.7161 - precision: 0.6138 - recall: 0.3562\n",
      "Epoch 24/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.1771 - auc: 0.7134 - precision: 0.5852 - recall: 0.3340\n",
      "Epoch 25/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.2006 - auc: 0.7172 - precision: 0.6084 - recall: 0.3827\n",
      "Epoch 26/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.3151 - auc: 0.7060 - precision: 0.6007 - recall: 0.3594\n",
      "Epoch 27/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.1764 - auc: 0.7173 - precision: 0.6091 - recall: 0.3541\n",
      "Epoch 28/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.1229 - auc: 0.7323 - precision: 0.6471 - recall: 0.3721\n",
      "Epoch 29/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.0962 - auc: 0.7282 - precision: 0.6222 - recall: 0.3848\n",
      "Epoch 30/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.1072 - auc: 0.7289 - precision: 0.6029 - recall: 0.3932\n",
      "Epoch 31/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.0607 - auc: 0.7375 - precision: 0.6362 - recall: 0.4197\n",
      "Epoch 32/50\n",
      "60/60 [==============================] - 1s 12ms/step - loss: 1.0810 - auc: 0.7414 - precision: 0.6387 - recall: 0.3943\n",
      "Epoch 33/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.0912 - auc: 0.7357 - precision: 0.6360 - recall: 0.3805\n",
      "Epoch 34/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.0832 - auc: 0.7427 - precision: 0.6649 - recall: 0.4006\n",
      "Epoch 35/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.0888 - auc: 0.7459 - precision: 0.6524 - recall: 0.4186\n",
      "Epoch 36/50\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 1.0892 - auc: 0.7549 - precision: 0.6439 - recall: 0.4376\n",
      "Epoch 37/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.0980 - auc: 0.7345 - precision: 0.6189 - recall: 0.4017\n",
      "Epoch 38/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.0973 - auc: 0.7396 - precision: 0.6286 - recall: 0.4186\n",
      "Epoch 39/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.0608 - auc: 0.7472 - precision: 0.6231 - recall: 0.4228\n",
      "Epoch 40/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.0461 - auc: 0.7595 - precision: 0.6765 - recall: 0.4133\n",
      "Epoch 41/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.0599 - auc: 0.7537 - precision: 0.6483 - recall: 0.4345\n",
      "Epoch 42/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.0375 - auc: 0.7619 - precision: 0.6477 - recall: 0.4218\n",
      "Epoch 43/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.0522 - auc: 0.7537 - precision: 0.6418 - recall: 0.4545\n",
      "Epoch 44/50\n",
      "60/60 [==============================] - 1s 12ms/step - loss: 1.0585 - auc: 0.7553 - precision: 0.6540 - recall: 0.4376\n",
      "Epoch 45/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.0713 - auc: 0.7592 - precision: 0.6457 - recall: 0.4450\n",
      "Epoch 46/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.0709 - auc: 0.7523 - precision: 0.6540 - recall: 0.4017\n",
      "Epoch 47/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.1181 - auc: 0.7505 - precision: 0.6250 - recall: 0.4281\n",
      "Epoch 48/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.0131 - auc: 0.7830 - precision: 0.6784 - recall: 0.4704\n",
      "Epoch 49/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.0519 - auc: 0.7664 - precision: 0.6484 - recall: 0.4366\n",
      "Epoch 50/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.0279 - auc: 0.7782 - precision: 0.6470 - recall: 0.4514\n",
      "Epoch 1/50\n",
      "60/60 [==============================] - 9s 8ms/step - loss: 8.3563 - auc: 0.5757 - precision: 0.4168 - recall: 0.3203\n",
      "Epoch 2/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 4.6230 - auc: 0.6509 - precision: 0.5078 - recall: 0.3763\n",
      "Epoch 3/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 2.9073 - auc: 0.6457 - precision: 0.4888 - recall: 0.3689\n",
      "Epoch 4/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 2.0935 - auc: 0.6658 - precision: 0.5090 - recall: 0.3901\n",
      "Epoch 5/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.6647 - auc: 0.6655 - precision: 0.5347 - recall: 0.3742\n",
      "Epoch 6/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.4050 - auc: 0.6905 - precision: 0.5460 - recall: 0.4080\n",
      "Epoch 7/50\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 1.3017 - auc: 0.6803 - precision: 0.5415 - recall: 0.3795\n",
      "Epoch 8/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.2209 - auc: 0.6880 - precision: 0.5465 - recall: 0.4101\n",
      "Epoch 9/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.2052 - auc: 0.6751 - precision: 0.5361 - recall: 0.3763\n",
      "Epoch 10/50\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 1.1247 - auc: 0.7021 - precision: 0.5694 - recall: 0.3901\n",
      "Epoch 11/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.1407 - auc: 0.6925 - precision: 0.5648 - recall: 0.4006\n",
      "Epoch 12/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.1170 - auc: 0.6873 - precision: 0.5430 - recall: 0.3668\n",
      "Epoch 13/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.0741 - auc: 0.7155 - precision: 0.5710 - recall: 0.4080\n",
      "Epoch 14/50\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 1.0969 - auc: 0.7088 - precision: 0.5811 - recall: 0.3975\n",
      "Epoch 15/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.0375 - auc: 0.7391 - precision: 0.6025 - recall: 0.4133\n",
      "Epoch 16/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.0315 - auc: 0.7420 - precision: 0.6160 - recall: 0.4239\n",
      "Epoch 17/50\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 1.0612 - auc: 0.7208 - precision: 0.5818 - recall: 0.4133\n",
      "Epoch 18/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.0379 - auc: 0.7366 - precision: 0.6149 - recall: 0.4271\n",
      "Epoch 19/50\n",
      "60/60 [==============================] - 1s 8ms/step - loss: 1.0116 - auc: 0.7461 - precision: 0.6367 - recall: 0.4186\n",
      "Epoch 20/50\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 1.0555 - auc: 0.7283 - precision: 0.6023 - recall: 0.3858\n",
      "Epoch 21/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.0195 - auc: 0.7540 - precision: 0.6265 - recall: 0.4345\n",
      "Epoch 22/50\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.9949 - auc: 0.7618 - precision: 0.6449 - recall: 0.4281\n",
      "Epoch 23/50\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 1.0073 - auc: 0.7590 - precision: 0.6218 - recall: 0.4397\n",
      "Epoch 24/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.0063 - auc: 0.7586 - precision: 0.6340 - recall: 0.4376\n",
      "Epoch 25/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.9826 - auc: 0.7755 - precision: 0.6696 - recall: 0.4820\n",
      "Epoch 26/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.9871 - auc: 0.7699 - precision: 0.6452 - recall: 0.4651\n",
      "Epoch 27/50\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 1.0041 - auc: 0.7670 - precision: 0.6708 - recall: 0.4567\n",
      "Epoch 28/50\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 1.0050 - auc: 0.7699 - precision: 0.6581 - recall: 0.4577\n",
      "Epoch 29/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.9838 - auc: 0.7750 - precision: 0.6631 - recall: 0.4535\n",
      "Epoch 30/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.9800 - auc: 0.7797 - precision: 0.6662 - recall: 0.4810\n",
      "Epoch 31/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.9212 - auc: 0.8079 - precision: 0.6824 - recall: 0.5201\n",
      "Epoch 32/50\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 0.9332 - auc: 0.7999 - precision: 0.6721 - recall: 0.4831\n",
      "Epoch 33/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.9672 - auc: 0.7862 - precision: 0.6736 - recall: 0.4820\n",
      "Epoch 34/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.9507 - auc: 0.7896 - precision: 0.6830 - recall: 0.4715\n",
      "Epoch 35/50\n",
      "60/60 [==============================] - 1s 8ms/step - loss: 0.9350 - auc: 0.8032 - precision: 0.6846 - recall: 0.5233\n",
      "Epoch 36/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.9491 - auc: 0.7961 - precision: 0.6836 - recall: 0.4979\n",
      "Epoch 37/50\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 0.9213 - auc: 0.8072 - precision: 0.6812 - recall: 0.5106\n",
      "Epoch 38/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.9521 - auc: 0.8024 - precision: 0.6827 - recall: 0.5095\n",
      "Epoch 39/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.9126 - auc: 0.8230 - precision: 0.7199 - recall: 0.5381\n",
      "Epoch 40/50\n",
      "60/60 [==============================] - 1s 13ms/step - loss: 0.8867 - auc: 0.8293 - precision: 0.7076 - recall: 0.5423\n",
      "Epoch 41/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.8979 - auc: 0.8211 - precision: 0.7027 - recall: 0.5296\n",
      "Epoch 42/50\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 0.9266 - auc: 0.8094 - precision: 0.6883 - recall: 0.5370\n",
      "Epoch 43/50\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 0.9846 - auc: 0.7868 - precision: 0.6682 - recall: 0.4725\n",
      "Epoch 44/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.9011 - auc: 0.8299 - precision: 0.7088 - recall: 0.5507\n",
      "Epoch 45/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.9462 - auc: 0.8063 - precision: 0.6849 - recall: 0.5032\n",
      "Epoch 46/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.8920 - auc: 0.8321 - precision: 0.7269 - recall: 0.5402\n",
      "Epoch 47/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.8859 - auc: 0.8309 - precision: 0.7125 - recall: 0.5423\n",
      "Epoch 48/50\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 0.8731 - auc: 0.8362 - precision: 0.6963 - recall: 0.5624\n",
      "Epoch 49/50\n",
      "60/60 [==============================] - 1s 8ms/step - loss: 0.9100 - auc: 0.8217 - precision: 0.7089 - recall: 0.5740\n",
      "Epoch 50/50\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 0.8822 - auc: 0.8360 - precision: 0.7194 - recall: 0.5772\n",
      "Epoch 1/50\n",
      "60/60 [==============================] - 10s 11ms/step - loss: 13.6467 - auc: 0.6088 - precision: 0.4431 - recall: 0.3584\n",
      "Epoch 2/50\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 5.4389 - auc: 0.6302 - precision: 0.4766 - recall: 0.3658\n",
      "Epoch 3/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 2.7375 - auc: 0.6416 - precision: 0.5000 - recall: 0.3658\n",
      "Epoch 4/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.8189 - auc: 0.6472 - precision: 0.4934 - recall: 0.3573\n",
      "Epoch 5/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.5124 - auc: 0.6560 - precision: 0.5091 - recall: 0.3837\n",
      "Epoch 6/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.3135 - auc: 0.6787 - precision: 0.5370 - recall: 0.3837\n",
      "Epoch 7/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.3026 - auc: 0.6731 - precision: 0.5349 - recall: 0.3890\n",
      "Epoch 8/50\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 1.2659 - auc: 0.6810 - precision: 0.5431 - recall: 0.3932\n",
      "Epoch 9/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.2767 - auc: 0.6531 - precision: 0.4993 - recall: 0.3573\n",
      "Epoch 10/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.1711 - auc: 0.7049 - precision: 0.5789 - recall: 0.3953\n",
      "Epoch 11/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.1761 - auc: 0.6916 - precision: 0.5769 - recall: 0.3964\n",
      "Epoch 12/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.1578 - auc: 0.6957 - precision: 0.5726 - recall: 0.3753\n",
      "Epoch 13/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.1771 - auc: 0.6968 - precision: 0.5737 - recall: 0.3742\n",
      "Epoch 14/50\n",
      "60/60 [==============================] - 1s 12ms/step - loss: 1.1419 - auc: 0.7015 - precision: 0.5829 - recall: 0.3901\n",
      "Epoch 15/50\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 1.1197 - auc: 0.7086 - precision: 0.5791 - recall: 0.3753\n",
      "Epoch 16/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.1043 - auc: 0.7114 - precision: 0.5964 - recall: 0.3827\n",
      "Epoch 17/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.1082 - auc: 0.7251 - precision: 0.5878 - recall: 0.3858\n",
      "Epoch 18/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.0999 - auc: 0.7324 - precision: 0.6104 - recall: 0.3975\n",
      "Epoch 19/50\n",
      "60/60 [==============================] - 1s 13ms/step - loss: 1.1118 - auc: 0.7310 - precision: 0.6006 - recall: 0.4006\n",
      "Epoch 20/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.1341 - auc: 0.7263 - precision: 0.6202 - recall: 0.3953\n",
      "Epoch 21/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.1230 - auc: 0.7215 - precision: 0.5905 - recall: 0.3932\n",
      "Epoch 22/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.1057 - auc: 0.7331 - precision: 0.6314 - recall: 0.3911\n",
      "Epoch 23/50\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 1.0840 - auc: 0.7494 - precision: 0.6196 - recall: 0.4271\n",
      "Epoch 24/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.0604 - auc: 0.7513 - precision: 0.6399 - recall: 0.4207\n",
      "Epoch 25/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.0613 - auc: 0.7455 - precision: 0.6324 - recall: 0.4165\n",
      "Epoch 26/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.0410 - auc: 0.7615 - precision: 0.6592 - recall: 0.4376\n",
      "Epoch 27/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.1360 - auc: 0.7422 - precision: 0.6134 - recall: 0.4175\n",
      "Epoch 28/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.1025 - auc: 0.7435 - precision: 0.6246 - recall: 0.4186\n",
      "Epoch 29/50\n",
      "60/60 [==============================] - 1s 12ms/step - loss: 1.0598 - auc: 0.7554 - precision: 0.6462 - recall: 0.4228\n",
      "Epoch 30/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.0480 - auc: 0.7646 - precision: 0.6471 - recall: 0.4440\n",
      "Epoch 31/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.0431 - auc: 0.7674 - precision: 0.6495 - recall: 0.4387\n",
      "Epoch 32/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.0476 - auc: 0.7739 - precision: 0.6617 - recall: 0.4651\n",
      "Epoch 33/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.0341 - auc: 0.7755 - precision: 0.6687 - recall: 0.4651\n",
      "Epoch 34/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.0117 - auc: 0.7803 - precision: 0.6698 - recall: 0.4588\n",
      "Epoch 35/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.0334 - auc: 0.7683 - precision: 0.6510 - recall: 0.4535\n",
      "Epoch 36/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.0259 - auc: 0.7698 - precision: 0.6485 - recall: 0.4524\n",
      "Epoch 37/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.9828 - auc: 0.7955 - precision: 0.6828 - recall: 0.4778\n",
      "Epoch 38/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.0153 - auc: 0.7982 - precision: 0.6844 - recall: 0.5042\n",
      "Epoch 39/50\n",
      "60/60 [==============================] - 1s 12ms/step - loss: 1.0133 - auc: 0.7903 - precision: 0.6581 - recall: 0.4884\n",
      "Epoch 40/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.0648 - auc: 0.7800 - precision: 0.6602 - recall: 0.4662\n",
      "Epoch 41/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.1141 - auc: 0.7759 - precision: 0.6558 - recall: 0.4672\n",
      "Epoch 42/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.1049 - auc: 0.7878 - precision: 0.6780 - recall: 0.4831\n",
      "Epoch 43/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.0477 - auc: 0.7986 - precision: 0.6823 - recall: 0.4926\n",
      "Epoch 44/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.0139 - auc: 0.7985 - precision: 0.6839 - recall: 0.5169\n",
      "Epoch 45/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.9802 - auc: 0.8134 - precision: 0.7033 - recall: 0.5338\n",
      "Epoch 46/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.0162 - auc: 0.8105 - precision: 0.6900 - recall: 0.5317\n",
      "Epoch 47/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.0516 - auc: 0.7966 - precision: 0.6713 - recall: 0.5137\n",
      "Epoch 48/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.9665 - auc: 0.8252 - precision: 0.7082 - recall: 0.5412\n",
      "Epoch 49/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.9483 - auc: 0.8261 - precision: 0.6946 - recall: 0.5433\n",
      "Epoch 50/50\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 1.0169 - auc: 0.8157 - precision: 0.7007 - recall: 0.5222\n",
      "Epoch 1/50\n",
      "60/60 [==============================] - 10s 9ms/step - loss: 13.6669 - auc: 0.6113 - precision: 0.4558 - recall: 0.3541\n",
      "Epoch 2/50\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 5.3670 - auc: 0.6332 - precision: 0.4697 - recall: 0.3605\n",
      "Epoch 3/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 2.6668 - auc: 0.6262 - precision: 0.4632 - recall: 0.3531\n",
      "Epoch 4/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.7635 - auc: 0.6413 - precision: 0.4909 - recall: 0.3436\n",
      "Epoch 5/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.4308 - auc: 0.6732 - precision: 0.5340 - recall: 0.3985\n",
      "Epoch 6/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.3309 - auc: 0.6570 - precision: 0.5215 - recall: 0.3584\n",
      "Epoch 7/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.2735 - auc: 0.6630 - precision: 0.5255 - recall: 0.3700\n",
      "Epoch 8/50\n",
      "60/60 [==============================] - 1s 12ms/step - loss: 1.2856 - auc: 0.6491 - precision: 0.5195 - recall: 0.3520\n",
      "Epoch 9/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.2802 - auc: 0.6673 - precision: 0.5563 - recall: 0.3605\n",
      "Epoch 10/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.2812 - auc: 0.6809 - precision: 0.5545 - recall: 0.3710\n",
      "Epoch 11/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.2411 - auc: 0.6820 - precision: 0.5535 - recall: 0.3827\n",
      "Epoch 12/50\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 1.1858 - auc: 0.6920 - precision: 0.5645 - recall: 0.3837\n",
      "Epoch 13/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.1646 - auc: 0.6941 - precision: 0.5627 - recall: 0.3700\n",
      "Epoch 14/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.1402 - auc: 0.7087 - precision: 0.5714 - recall: 0.3932\n",
      "Epoch 15/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.2043 - auc: 0.6983 - precision: 0.5747 - recall: 0.3742\n",
      "Epoch 16/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.2841 - auc: 0.6799 - precision: 0.5394 - recall: 0.3763\n",
      "Epoch 17/50\n",
      "60/60 [==============================] - 1s 12ms/step - loss: 1.2046 - auc: 0.7113 - precision: 0.6065 - recall: 0.3732\n",
      "Epoch 18/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.1541 - auc: 0.7218 - precision: 0.6111 - recall: 0.4070\n",
      "Epoch 19/50\n",
      "60/60 [==============================] - 1s 8ms/step - loss: 1.1261 - auc: 0.7137 - precision: 0.5852 - recall: 0.3848\n",
      "Epoch 20/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.0935 - auc: 0.7313 - precision: 0.6202 - recall: 0.4091\n",
      "Epoch 21/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.0914 - auc: 0.7420 - precision: 0.6284 - recall: 0.4345\n",
      "Epoch 22/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.0779 - auc: 0.7412 - precision: 0.6402 - recall: 0.4175\n",
      "Epoch 23/50\n",
      "60/60 [==============================] - 1s 8ms/step - loss: 1.0754 - auc: 0.7412 - precision: 0.6099 - recall: 0.4165\n",
      "Epoch 24/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.0677 - auc: 0.7349 - precision: 0.6112 - recall: 0.4154\n",
      "Epoch 25/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.0590 - auc: 0.7532 - precision: 0.6482 - recall: 0.4323\n",
      "Epoch 26/50\n",
      "60/60 [==============================] - 1s 12ms/step - loss: 1.0752 - auc: 0.7507 - precision: 0.6278 - recall: 0.4207\n",
      "Epoch 27/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.0437 - auc: 0.7542 - precision: 0.6341 - recall: 0.4323\n",
      "Epoch 28/50\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 1.0119 - auc: 0.7782 - precision: 0.6651 - recall: 0.4514\n",
      "Epoch 29/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.0215 - auc: 0.7697 - precision: 0.6400 - recall: 0.4567\n",
      "Epoch 30/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.0193 - auc: 0.7662 - precision: 0.6562 - recall: 0.4440\n",
      "Epoch 31/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.0048 - auc: 0.7697 - precision: 0.6407 - recall: 0.4429\n",
      "Epoch 32/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.0018 - auc: 0.7811 - precision: 0.6521 - recall: 0.4736\n",
      "Epoch 33/50\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 1.0380 - auc: 0.7772 - precision: 0.6617 - recall: 0.4693\n",
      "Epoch 34/50\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 0.9746 - auc: 0.8084 - precision: 0.6937 - recall: 0.4979\n",
      "Epoch 35/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.0168 - auc: 0.7807 - precision: 0.6647 - recall: 0.4715\n",
      "Epoch 36/50\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 1.0362 - auc: 0.7801 - precision: 0.6591 - recall: 0.4577\n",
      "Epoch 37/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.0231 - auc: 0.7893 - precision: 0.6681 - recall: 0.4958\n",
      "Epoch 38/50\n",
      "60/60 [==============================] - 1s 8ms/step - loss: 1.0868 - auc: 0.7647 - precision: 0.6542 - recall: 0.4641\n",
      "Epoch 39/50\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 1.0977 - auc: 0.7723 - precision: 0.6543 - recall: 0.4461\n",
      "Epoch 40/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.9901 - auc: 0.8108 - precision: 0.7119 - recall: 0.4989\n",
      "Epoch 41/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.9762 - auc: 0.8118 - precision: 0.6852 - recall: 0.5201\n",
      "Epoch 42/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.9906 - auc: 0.8025 - precision: 0.6919 - recall: 0.5032\n",
      "Epoch 43/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.9487 - auc: 0.8218 - precision: 0.7174 - recall: 0.5285\n",
      "Epoch 44/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.9558 - auc: 0.8191 - precision: 0.7153 - recall: 0.5338\n",
      "Epoch 45/50\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 0.9798 - auc: 0.8145 - precision: 0.6891 - recall: 0.5412\n",
      "Epoch 46/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.0015 - auc: 0.8017 - precision: 0.6718 - recall: 0.5106\n",
      "Epoch 47/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.9690 - auc: 0.8112 - precision: 0.6914 - recall: 0.5518\n",
      "Epoch 48/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.9781 - auc: 0.8126 - precision: 0.6917 - recall: 0.5264\n",
      "Epoch 49/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.9340 - auc: 0.8323 - precision: 0.7113 - recall: 0.5782\n",
      "Epoch 50/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.9605 - auc: 0.8146 - precision: 0.6768 - recall: 0.5402\n",
      "Epoch 1/50\n",
      "60/60 [==============================] - 10s 14ms/step - loss: 19.3020 - auc: 0.6136 - precision: 0.4381 - recall: 0.3478\n",
      "Epoch 2/50\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 6.9626 - auc: 0.6416 - precision: 0.4822 - recall: 0.3721\n",
      "Epoch 3/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 3.0402 - auc: 0.6501 - precision: 0.5145 - recall: 0.3742\n",
      "Epoch 4/50\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 1.8664 - auc: 0.6619 - precision: 0.5197 - recall: 0.3763\n",
      "Epoch 5/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.5152 - auc: 0.6538 - precision: 0.5140 - recall: 0.3679\n",
      "Epoch 6/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.3573 - auc: 0.6623 - precision: 0.5167 - recall: 0.3594\n",
      "Epoch 7/50\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 1.2718 - auc: 0.6641 - precision: 0.5251 - recall: 0.3763\n",
      "Epoch 8/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.1945 - auc: 0.6875 - precision: 0.5491 - recall: 0.3901\n",
      "Epoch 9/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.1837 - auc: 0.6772 - precision: 0.5310 - recall: 0.3710\n",
      "Epoch 10/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.1504 - auc: 0.6946 - precision: 0.5595 - recall: 0.3827\n",
      "Epoch 11/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.1606 - auc: 0.6842 - precision: 0.5730 - recall: 0.3774\n",
      "Epoch 12/50\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 1.1417 - auc: 0.6867 - precision: 0.5627 - recall: 0.3510\n",
      "Epoch 13/50\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 1.1556 - auc: 0.6760 - precision: 0.5468 - recall: 0.3584\n",
      "Epoch 14/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.1092 - auc: 0.7024 - precision: 0.5836 - recall: 0.3837\n",
      "Epoch 15/50\n",
      "60/60 [==============================] - 1s 8ms/step - loss: 1.1140 - auc: 0.7039 - precision: 0.5795 - recall: 0.3774\n",
      "Epoch 16/50\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 1.1031 - auc: 0.7050 - precision: 0.5986 - recall: 0.3689\n",
      "Epoch 17/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.0821 - auc: 0.7184 - precision: 0.5984 - recall: 0.4049\n",
      "Epoch 18/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.1239 - auc: 0.7173 - precision: 0.6071 - recall: 0.3985\n",
      "Epoch 19/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.1231 - auc: 0.7075 - precision: 0.5908 - recall: 0.3784\n",
      "Epoch 20/50\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 1.0705 - auc: 0.7293 - precision: 0.6072 - recall: 0.4101\n",
      "Epoch 21/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.0466 - auc: 0.7318 - precision: 0.6028 - recall: 0.4027\n",
      "Epoch 22/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.0456 - auc: 0.7324 - precision: 0.6026 - recall: 0.3975\n",
      "Epoch 23/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.0838 - auc: 0.7282 - precision: 0.6330 - recall: 0.3975\n",
      "Epoch 24/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.0906 - auc: 0.7204 - precision: 0.6122 - recall: 0.3721\n",
      "Epoch 25/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.0743 - auc: 0.7323 - precision: 0.6284 - recall: 0.3932\n",
      "Epoch 26/50\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 1.0851 - auc: 0.7257 - precision: 0.6211 - recall: 0.3985\n",
      "Epoch 27/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.0351 - auc: 0.7370 - precision: 0.6478 - recall: 0.3869\n",
      "Epoch 28/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.0537 - auc: 0.7341 - precision: 0.6397 - recall: 0.4017\n",
      "Epoch 29/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.0233 - auc: 0.7501 - precision: 0.6408 - recall: 0.4017\n",
      "Epoch 30/50\n",
      "60/60 [==============================] - 1s 14ms/step - loss: 1.0566 - auc: 0.7328 - precision: 0.6083 - recall: 0.3858\n",
      "Epoch 31/50\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 1.0031 - auc: 0.7510 - precision: 0.6380 - recall: 0.4080\n",
      "Epoch 32/50\n",
      "60/60 [==============================] - 1s 13ms/step - loss: 1.0036 - auc: 0.7525 - precision: 0.6377 - recall: 0.4112\n",
      "Epoch 33/50\n",
      "60/60 [==============================] - 1s 15ms/step - loss: 0.9982 - auc: 0.7570 - precision: 0.6555 - recall: 0.4123\n",
      "Epoch 34/50\n",
      "60/60 [==============================] - 1s 15ms/step - loss: 0.9913 - auc: 0.7706 - precision: 0.6656 - recall: 0.4334\n",
      "Epoch 35/50\n",
      "60/60 [==============================] - 1s 13ms/step - loss: 0.9871 - auc: 0.7695 - precision: 0.6700 - recall: 0.4313\n",
      "Epoch 36/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.9727 - auc: 0.7786 - precision: 0.6778 - recall: 0.4493\n",
      "Epoch 37/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.9916 - auc: 0.7682 - precision: 0.6508 - recall: 0.4471\n",
      "Epoch 38/50\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 1.0045 - auc: 0.7603 - precision: 0.6423 - recall: 0.4271\n",
      "Epoch 39/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.9988 - auc: 0.7643 - precision: 0.6651 - recall: 0.4471\n",
      "Epoch 40/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.0018 - auc: 0.7631 - precision: 0.6755 - recall: 0.4334\n",
      "Epoch 41/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.0013 - auc: 0.7656 - precision: 0.6667 - recall: 0.4376\n",
      "Epoch 42/50\n",
      "60/60 [==============================] - 1s 8ms/step - loss: 0.9741 - auc: 0.7763 - precision: 0.6651 - recall: 0.4493\n",
      "Epoch 43/50\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.9767 - auc: 0.7774 - precision: 0.6718 - recall: 0.4609\n",
      "Epoch 44/50\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.9547 - auc: 0.7904 - precision: 0.6888 - recall: 0.4820\n",
      "Epoch 45/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.0086 - auc: 0.7700 - precision: 0.6651 - recall: 0.4619\n",
      "Epoch 46/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.9970 - auc: 0.7766 - precision: 0.6758 - recall: 0.4450\n",
      "Epoch 47/50\n",
      "60/60 [==============================] - 1s 8ms/step - loss: 0.9939 - auc: 0.7837 - precision: 0.6749 - recall: 0.4609\n",
      "Epoch 48/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.9617 - auc: 0.7940 - precision: 0.6810 - recall: 0.4672\n",
      "Epoch 49/50\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.9687 - auc: 0.7903 - precision: 0.6824 - recall: 0.4725\n",
      "Epoch 50/50\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 0.9401 - auc: 0.8048 - precision: 0.6988 - recall: 0.4757\n"
     ]
    }
   ],
   "source": [
    "ensemble = MultivariableVotingClassifier(models)\n",
    "ensemble.fit(X_scaled, y_one_hot)\n",
    "# ensemble.fit(X, y_one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Datos originales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open_time</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Number of trades</th>\n",
       "      <th>Volume_BTCUSDT</th>\n",
       "      <th>Number_of_trades_BTCUSDT</th>\n",
       "      <th>Volume_ETHUSDT</th>\n",
       "      <th>Volume_BNBUSDT</th>\n",
       "      <th>EMA_20</th>\n",
       "      <th>Upper_Band</th>\n",
       "      <th>Middle_Band</th>\n",
       "      <th>RSI</th>\n",
       "      <th>total_trades_coinbase</th>\n",
       "      <th>Tweets_Utilizados</th>\n",
       "      <th>Tweets_Utilizados_coin</th>\n",
       "      <th>Tweets_Utilizados_whale_alert</th>\n",
       "      <th>Buy_1000x_high</th>\n",
       "      <th>sell_1000x_high</th>\n",
       "      <th>Tendencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>946</th>\n",
       "      <td>2024-04-30</td>\n",
       "      <td>6.59</td>\n",
       "      <td>6.67</td>\n",
       "      <td>6.04</td>\n",
       "      <td>115512.00</td>\n",
       "      <td>54947.66</td>\n",
       "      <td>1985671.00</td>\n",
       "      <td>561717.49</td>\n",
       "      <td>766513.45</td>\n",
       "      <td>7.13</td>\n",
       "      <td>7.80</td>\n",
       "      <td>6.93</td>\n",
       "      <td>34.18</td>\n",
       "      <td>48709.00</td>\n",
       "      <td>142</td>\n",
       "      <td>187</td>\n",
       "      <td>23.00</td>\n",
       "      <td>379.00</td>\n",
       "      <td>377.00</td>\n",
       "      <td>Bajista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>947</th>\n",
       "      <td>2024-05-01</td>\n",
       "      <td>6.42</td>\n",
       "      <td>6.93</td>\n",
       "      <td>6.13</td>\n",
       "      <td>175570.00</td>\n",
       "      <td>81166.47</td>\n",
       "      <td>2401089.00</td>\n",
       "      <td>624963.78</td>\n",
       "      <td>669027.32</td>\n",
       "      <td>7.11</td>\n",
       "      <td>7.41</td>\n",
       "      <td>6.85</td>\n",
       "      <td>43.30</td>\n",
       "      <td>83718.00</td>\n",
       "      <td>130</td>\n",
       "      <td>177</td>\n",
       "      <td>36.00</td>\n",
       "      <td>327.00</td>\n",
       "      <td>340.00</td>\n",
       "      <td>Alcista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948</th>\n",
       "      <td>2024-05-02</td>\n",
       "      <td>6.90</td>\n",
       "      <td>7.41</td>\n",
       "      <td>6.69</td>\n",
       "      <td>109002.00</td>\n",
       "      <td>47583.82</td>\n",
       "      <td>1572898.00</td>\n",
       "      <td>365939.72</td>\n",
       "      <td>359794.32</td>\n",
       "      <td>7.12</td>\n",
       "      <td>7.42</td>\n",
       "      <td>6.85</td>\n",
       "      <td>49.27</td>\n",
       "      <td>61208.00</td>\n",
       "      <td>461</td>\n",
       "      <td>374</td>\n",
       "      <td>25.00</td>\n",
       "      <td>270.00</td>\n",
       "      <td>282.00</td>\n",
       "      <td>Alcista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949</th>\n",
       "      <td>2024-05-03</td>\n",
       "      <td>7.27</td>\n",
       "      <td>7.39</td>\n",
       "      <td>7.00</td>\n",
       "      <td>109634.00</td>\n",
       "      <td>43628.40</td>\n",
       "      <td>1558661.00</td>\n",
       "      <td>355825.84</td>\n",
       "      <td>342906.43</td>\n",
       "      <td>7.14</td>\n",
       "      <td>7.44</td>\n",
       "      <td>6.90</td>\n",
       "      <td>48.86</td>\n",
       "      <td>46255.00</td>\n",
       "      <td>573</td>\n",
       "      <td>474</td>\n",
       "      <td>22.00</td>\n",
       "      <td>386.00</td>\n",
       "      <td>635.00</td>\n",
       "      <td>Lateral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>2024-05-04</td>\n",
       "      <td>7.24</td>\n",
       "      <td>7.28</td>\n",
       "      <td>7.11</td>\n",
       "      <td>71120.00</td>\n",
       "      <td>24368.69</td>\n",
       "      <td>1113509.00</td>\n",
       "      <td>196263.95</td>\n",
       "      <td>197129.25</td>\n",
       "      <td>7.13</td>\n",
       "      <td>7.46</td>\n",
       "      <td>6.91</td>\n",
       "      <td>46.98</td>\n",
       "      <td>34251.00</td>\n",
       "      <td>407</td>\n",
       "      <td>472</td>\n",
       "      <td>14.00</td>\n",
       "      <td>203.00</td>\n",
       "      <td>232.00</td>\n",
       "      <td>Bajista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951</th>\n",
       "      <td>2024-05-05</td>\n",
       "      <td>7.12</td>\n",
       "      <td>7.40</td>\n",
       "      <td>7.01</td>\n",
       "      <td>72928.00</td>\n",
       "      <td>18526.75</td>\n",
       "      <td>992921.00</td>\n",
       "      <td>218760.27</td>\n",
       "      <td>180458.24</td>\n",
       "      <td>7.15</td>\n",
       "      <td>7.51</td>\n",
       "      <td>6.94</td>\n",
       "      <td>50.00</td>\n",
       "      <td>29197.00</td>\n",
       "      <td>417</td>\n",
       "      <td>499</td>\n",
       "      <td>6.00</td>\n",
       "      <td>320.00</td>\n",
       "      <td>284.00</td>\n",
       "      <td>Alcista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>2024-05-06</td>\n",
       "      <td>7.30</td>\n",
       "      <td>7.47</td>\n",
       "      <td>7.06</td>\n",
       "      <td>94264.00</td>\n",
       "      <td>34674.92</td>\n",
       "      <td>1392557.00</td>\n",
       "      <td>355135.30</td>\n",
       "      <td>278669.01</td>\n",
       "      <td>7.15</td>\n",
       "      <td>7.53</td>\n",
       "      <td>6.96</td>\n",
       "      <td>47.10</td>\n",
       "      <td>40027.00</td>\n",
       "      <td>482</td>\n",
       "      <td>531</td>\n",
       "      <td>25.00</td>\n",
       "      <td>339.00</td>\n",
       "      <td>249.00</td>\n",
       "      <td>Bajista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>2024-05-07</td>\n",
       "      <td>7.12</td>\n",
       "      <td>7.29</td>\n",
       "      <td>6.98</td>\n",
       "      <td>64947.00</td>\n",
       "      <td>25598.79</td>\n",
       "      <td>1272898.00</td>\n",
       "      <td>298796.68</td>\n",
       "      <td>289488.71</td>\n",
       "      <td>7.13</td>\n",
       "      <td>7.52</td>\n",
       "      <td>6.98</td>\n",
       "      <td>45.10</td>\n",
       "      <td>31028.00</td>\n",
       "      <td>495</td>\n",
       "      <td>494</td>\n",
       "      <td>28.00</td>\n",
       "      <td>296.00</td>\n",
       "      <td>205.00</td>\n",
       "      <td>Bajista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>2024-05-08</td>\n",
       "      <td>6.99</td>\n",
       "      <td>7.19</td>\n",
       "      <td>6.89</td>\n",
       "      <td>75550.00</td>\n",
       "      <td>26121.19</td>\n",
       "      <td>1415152.00</td>\n",
       "      <td>266934.81</td>\n",
       "      <td>297016.62</td>\n",
       "      <td>7.12</td>\n",
       "      <td>7.52</td>\n",
       "      <td>6.99</td>\n",
       "      <td>44.94</td>\n",
       "      <td>32040.00</td>\n",
       "      <td>426</td>\n",
       "      <td>494</td>\n",
       "      <td>24.00</td>\n",
       "      <td>230.00</td>\n",
       "      <td>177.00</td>\n",
       "      <td>Lateral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955</th>\n",
       "      <td>2024-05-09</td>\n",
       "      <td>6.98</td>\n",
       "      <td>7.09</td>\n",
       "      <td>6.78</td>\n",
       "      <td>75016.00</td>\n",
       "      <td>30660.81</td>\n",
       "      <td>1381957.00</td>\n",
       "      <td>238561.75</td>\n",
       "      <td>464857.60</td>\n",
       "      <td>7.11</td>\n",
       "      <td>7.52</td>\n",
       "      <td>7.01</td>\n",
       "      <td>46.32</td>\n",
       "      <td>29314.00</td>\n",
       "      <td>475</td>\n",
       "      <td>464</td>\n",
       "      <td>16.00</td>\n",
       "      <td>188.00</td>\n",
       "      <td>257.00</td>\n",
       "      <td>Lateral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Open_time  Open  High  Low  Number of trades  Volume_BTCUSDT  \\\n",
       "946 2024-04-30  6.59  6.67 6.04         115512.00        54947.66   \n",
       "947 2024-05-01  6.42  6.93 6.13         175570.00        81166.47   \n",
       "948 2024-05-02  6.90  7.41 6.69         109002.00        47583.82   \n",
       "949 2024-05-03  7.27  7.39 7.00         109634.00        43628.40   \n",
       "950 2024-05-04  7.24  7.28 7.11          71120.00        24368.69   \n",
       "951 2024-05-05  7.12  7.40 7.01          72928.00        18526.75   \n",
       "952 2024-05-06  7.30  7.47 7.06          94264.00        34674.92   \n",
       "953 2024-05-07  7.12  7.29 6.98          64947.00        25598.79   \n",
       "954 2024-05-08  6.99  7.19 6.89          75550.00        26121.19   \n",
       "955 2024-05-09  6.98  7.09 6.78          75016.00        30660.81   \n",
       "\n",
       "     Number_of_trades_BTCUSDT  Volume_ETHUSDT  Volume_BNBUSDT  EMA_20  \\\n",
       "946                1985671.00       561717.49       766513.45    7.13   \n",
       "947                2401089.00       624963.78       669027.32    7.11   \n",
       "948                1572898.00       365939.72       359794.32    7.12   \n",
       "949                1558661.00       355825.84       342906.43    7.14   \n",
       "950                1113509.00       196263.95       197129.25    7.13   \n",
       "951                 992921.00       218760.27       180458.24    7.15   \n",
       "952                1392557.00       355135.30       278669.01    7.15   \n",
       "953                1272898.00       298796.68       289488.71    7.13   \n",
       "954                1415152.00       266934.81       297016.62    7.12   \n",
       "955                1381957.00       238561.75       464857.60    7.11   \n",
       "\n",
       "     Upper_Band  Middle_Band   RSI  total_trades_coinbase  Tweets_Utilizados  \\\n",
       "946        7.80         6.93 34.18               48709.00                142   \n",
       "947        7.41         6.85 43.30               83718.00                130   \n",
       "948        7.42         6.85 49.27               61208.00                461   \n",
       "949        7.44         6.90 48.86               46255.00                573   \n",
       "950        7.46         6.91 46.98               34251.00                407   \n",
       "951        7.51         6.94 50.00               29197.00                417   \n",
       "952        7.53         6.96 47.10               40027.00                482   \n",
       "953        7.52         6.98 45.10               31028.00                495   \n",
       "954        7.52         6.99 44.94               32040.00                426   \n",
       "955        7.52         7.01 46.32               29314.00                475   \n",
       "\n",
       "     Tweets_Utilizados_coin  Tweets_Utilizados_whale_alert  Buy_1000x_high  \\\n",
       "946                     187                          23.00          379.00   \n",
       "947                     177                          36.00          327.00   \n",
       "948                     374                          25.00          270.00   \n",
       "949                     474                          22.00          386.00   \n",
       "950                     472                          14.00          203.00   \n",
       "951                     499                           6.00          320.00   \n",
       "952                     531                          25.00          339.00   \n",
       "953                     494                          28.00          296.00   \n",
       "954                     494                          24.00          230.00   \n",
       "955                     464                          16.00          188.00   \n",
       "\n",
       "     sell_1000x_high Tendencia  \n",
       "946           377.00   Bajista  \n",
       "947           340.00   Alcista  \n",
       "948           282.00   Alcista  \n",
       "949           635.00   Lateral  \n",
       "950           232.00   Bajista  \n",
       "951           284.00   Alcista  \n",
       "952           249.00   Bajista  \n",
       "953           205.00   Bajista  \n",
       "954           177.00   Lateral  \n",
       "955           257.00   Lateral  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(clasifier_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 715ms/step\n",
      "1/1 [==============================] - 1s 738ms/step\n",
      "1/1 [==============================] - 1s 710ms/step\n",
      "1/1 [==============================] - 1s 690ms/step\n",
      "1/1 [==============================] - 1s 640ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 2, 2, 0, 0, 0, 2])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['Alcista', 'Bajista', 'Lateral'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Bajista',\n",
       " 'Bajista',\n",
       " 'Bajista',\n",
       " 'Alcista',\n",
       " 'Lateral',\n",
       " 'Lateral',\n",
       " 'Alcista',\n",
       " 'Alcista',\n",
       " 'Alcista',\n",
       " 'Lateral']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_clases = 3 \n",
    "\n",
    "# DESCOMENTAR PARA NORMALIZACION\n",
    "clasifier_validation_normalized = scaler.transform(clasifier_validation[columns].drop(columns=[\"Open_time\"]))\n",
    "validation_predictions = ensemble.predict(clasifier_validation_normalized)\n",
    "\n",
    "# validation_predictions = clasifier_validation[columns].drop(columns=[\"Open_time\"])\n",
    "# validation_predictions = ensemble.predict(validation_predictions)\n",
    "\n",
    "predicciones_one_hot = to_categorical(validation_predictions, num_classes=n_clases)\n",
    "etiquetas_numericas = np.argmax(predicciones_one_hot, axis=1)\n",
    "categorias_clases = onehot_encoder.categories_[0]\n",
    "nombres_clases = [categorias_clases[indice] for indice in etiquetas_numericas]\n",
    "\n",
    "display(validation_predictions)\n",
    "display(categorias_clases)\n",
    "display(nombres_clases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Datos generados por auto ml con prophet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 223ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 204ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2, 0, 0, 0, 2, 2, 0, 0, 0, 0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['Alcista', 'Bajista', 'Lateral'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Lateral',\n",
       " 'Alcista',\n",
       " 'Alcista',\n",
       " 'Alcista',\n",
       " 'Lateral',\n",
       " 'Lateral',\n",
       " 'Alcista',\n",
       " 'Alcista',\n",
       " 'Alcista',\n",
       " 'Alcista']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "auto_ml_prophet_df = pd.read_csv('auto_timeseries_models_prophet/predicciones.csv')\n",
    "auto_ml_prophet_df = auto_ml_prophet_df[columns].drop(columns=[\"Open_time\"])\n",
    "\n",
    "# DESCOMENTAR PARA NORMALIZACION\n",
    "auto_ml_prophet_df_normalized = scaler.transform(auto_ml_prophet_df)\n",
    "auto_mp_prophet_predictions = ensemble.predict(auto_ml_prophet_df_normalized)\n",
    "\n",
    "# auto_mp_prophet_predictions = ensemble.predict(auto_ml_prophet_df)\n",
    "\n",
    "predicciones_one_hot = to_categorical(auto_mp_prophet_predictions, num_classes=n_clases)\n",
    "etiquetas_numericas = np.argmax(predicciones_one_hot, axis=1)\n",
    "categorias_clases = onehot_encoder.categories_[0]\n",
    "nombres_clases = [categorias_clases[indice] for indice in etiquetas_numericas]\n",
    "\n",
    "display(auto_mp_prophet_predictions)\n",
    "display(categorias_clases)\n",
    "display(nombres_clases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Datos generados por auto ml con stats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 121ms/step\n",
      "1/1 [==============================] - 0s 209ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 152ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['Alcista', 'Bajista', 'Lateral'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Lateral',\n",
       " 'Lateral',\n",
       " 'Lateral',\n",
       " 'Lateral',\n",
       " 'Lateral',\n",
       " 'Lateral',\n",
       " 'Lateral',\n",
       " 'Lateral',\n",
       " 'Lateral',\n",
       " 'Lateral']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "auto_ml_stats_df = pd.read_csv('auto_timeseries_models/predicciones.csv')\n",
    "auto_ml_stats_df = auto_ml_stats_df[columns].drop(columns=[\"Open_time\"])\n",
    "\n",
    "# DESCOMENTAR PARA NORMALIZACION\n",
    "auto_ml_stats_df_normalized = scaler.transform(auto_ml_stats_df)\n",
    "auto_mp_stats_predictions = ensemble.predict(auto_ml_stats_df_normalized)\n",
    "\n",
    "# auto_mp_stats_predictions = ensemble.predict(auto_ml_stats_df)\n",
    "\n",
    "predicciones_one_hot = to_categorical(auto_mp_stats_predictions, num_classes=n_clases)\n",
    "etiquetas_numericas = np.argmax(predicciones_one_hot, axis=1)\n",
    "categorias_clases = onehot_encoder.categories_[0]\n",
    "nombres_clases = [categorias_clases[indice] for indice in etiquetas_numericas]\n",
    "\n",
    "display(auto_mp_stats_predictions)\n",
    "display(categorias_clases)\n",
    "display(nombres_clases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Datos generados por auto ml con modelos clasicos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 204ms/step\n",
      "1/1 [==============================] - 0s 199ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 144ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['Alcista', 'Bajista', 'Lateral'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Bajista',\n",
       " 'Bajista',\n",
       " 'Bajista',\n",
       " 'Bajista',\n",
       " 'Bajista',\n",
       " 'Bajista',\n",
       " 'Bajista',\n",
       " 'Bajista',\n",
       " 'Bajista',\n",
       " 'Bajista']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "auto_ml_df = pd.read_csv('h2o_models/predicciones.csv')\n",
    "auto_ml_df = auto_ml_df[columns].drop(columns=[\"Open_time\"])\n",
    "\n",
    "# DESCOMENTAR PARA NORMALIZACION\n",
    "auto_ml_df_normalized = scaler.transform(auto_ml_df)\n",
    "auto_mp_predictions = ensemble.predict(auto_ml_df_normalized)\n",
    "\n",
    "# auto_mp_predictions = ensemble.predict(auto_ml_df)\n",
    "\n",
    "predicciones_one_hot = to_categorical(auto_mp_predictions, num_classes=n_clases)\n",
    "etiquetas_numericas = np.argmax(predicciones_one_hot, axis=1)\n",
    "categorias_clases = onehot_encoder.categories_[0]\n",
    "nombres_clases = [categorias_clases[indice] for indice in etiquetas_numericas]\n",
    "\n",
    "display(auto_mp_predictions)\n",
    "display(categorias_clases)\n",
    "display(nombres_clases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Datos generados por auto ml con skforecast:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 176ms/step\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 200ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 2, 2, 2, 2, 0, 1, 1, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['Alcista', 'Bajista', 'Lateral'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Bajista',\n",
       " 'Bajista',\n",
       " 'Lateral',\n",
       " 'Lateral',\n",
       " 'Lateral',\n",
       " 'Lateral',\n",
       " 'Alcista',\n",
       " 'Bajista',\n",
       " 'Bajista',\n",
       " 'Bajista']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "skforecast_df = pd.read_csv('skforecast/predicciones.csv')\n",
    "skforecast_df = skforecast_df[columns[1:]]\n",
    "\n",
    "# DESCOMENTAR PARA NORMALIZACION\n",
    "skforecast_df_normalized = scaler.transform(skforecast_df)\n",
    "skforecast_predictions = ensemble.predict(skforecast_df_normalized)\n",
    "\n",
    "# skforecast_predictions = ensemble.predict(skforecast_df)\n",
    "\n",
    "predicciones_one_hot = to_categorical(skforecast_predictions, num_classes=n_clases)\n",
    "etiquetas_numericas = np.argmax(predicciones_one_hot, axis=1)\n",
    "categorias_clases = onehot_encoder.categories_[0]\n",
    "nombres_clases = [categorias_clases[indice] for indice in etiquetas_numericas]\n",
    "\n",
    "display(skforecast_predictions)\n",
    "display(categorias_clases)\n",
    "display(nombres_clases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analizo los features mas importantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Permutation Feature Importance\n",
    "\n",
    "El método de Permutation Feature Importance implica permutar las características del dataset y medir el impacto en el rendimiento del modelo. Esto se hace una característica a la vez. Puedes utilizar el siguiente código para calcular la importancia de las características usando este método:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# def permutation_feature_importance(model, X, y, metric=accuracy_score, n_repeats=10):\n",
    "#     base_score = metric(y, model.predict(X))\n",
    "#     scores = np.zeros((X.shape[1], n_repeats))\n",
    "    \n",
    "#     for i in range(X.shape[1]):\n",
    "#         for n in range(n_repeats):\n",
    "#             X_permuted = X.copy()\n",
    "#             np.random.shuffle(X_permuted[:, i])\n",
    "#             permuted_score = metric(y, model.predict(X_permuted))\n",
    "#             scores[i, n] = base_score - permuted_score\n",
    "            \n",
    "#     importances = np.mean(scores, axis=1)\n",
    "#     return importances\n",
    "\n",
    "# # Usa el ensemble model para obtener las importancias\n",
    "# importances = permutation_feature_importance(ensemble, X.values, y)\n",
    "\n",
    "# # Ordena las características por importancia\n",
    "# feature_importances = sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# # Imprime las importancias\n",
    "# for feature, importance in feature_importances:\n",
    "#     print(f'Feature: {feature}, Importance: {importance}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SHAP (SHapley Additive exPlanations)\n",
    "\n",
    "SHAP es una técnica avanzada que explica las predicciones de cualquier modelo basado en el cálculo de los valores de Shapley de la teoría de juegos. Puedes usar la librería shap para calcular las importancias:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shap\n",
    "\n",
    "# # Crear un background dataset, usualmente se usa una muestra aleatoria de los datos de entrenamiento\n",
    "# background = X.sample(n=100)\n",
    "\n",
    "# # Crear el objeto explainer de SHAP\n",
    "# explainer = shap.KernelExplainer(ensemble.predict, background)\n",
    "\n",
    "# # Calcular los valores SHAP para el conjunto de datos completo\n",
    "# shap_values = explainer.shap_values(X)\n",
    "\n",
    "# # Visualizar las importancias de las características\n",
    "# shap.summary_plot(shap_values, X)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
