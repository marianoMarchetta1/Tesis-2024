{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported auto_timeseries version:0.0.90. Call by using:\n",
      "model = auto_timeseries(score_type='rmse',\n",
      "        time_interval='M', non_seasonal_pdq=None, seasonality=False,\n",
      "        seasonal_period=12, model_type=['best'], verbose=2, dask_xgboost_flag=0)\n",
      "model.fit(traindata, ts_column,target)\n",
      "model.predict(testdata, model='best')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-13 09:01:18.822521: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from auto_ts import auto_timeseries\n",
    "import dill\n",
    "import talib\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from keras.callbacks import EarlyStopping\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.losses import categorical_crossentropy\n",
    "from sklearn.metrics import log_loss\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evito que ciertas columnas se transformen a notacion cientifica en las predicciones\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    'Open_time',\n",
    "    'Open',\n",
    "    'High',\n",
    "    'Low',\n",
    "    # 'Close',\n",
    "    'Number of trades',\n",
    "    'Close_BTCUSDT',\n",
    "    'Volume_BTCUSDT',\n",
    "    'Number_of_trades_BTCUSDT',\n",
    "    'Close_ETHUSDT',\n",
    "    'Volume_ETHUSDT',\n",
    "    'Number_of_trades_ETHUSDT',\n",
    "    'Close_BNBUSDT',\n",
    "    'Volume_BNBUSDT',\n",
    "    'Number_of_trades_BNBUSDT',\n",
    "    'SMA_20',\n",
    "    'EMA_20',\n",
    "    'Upper_Band',\n",
    "    'Middle_Band',\n",
    "    'Lower_Band',\n",
    "    'RSI',\n",
    "    'buy_1000x_high_coinbase',\n",
    "    'sell_1000x_high_coinbase',\n",
    "    'total_trades_coinbase',\t\n",
    "    'Tweets_Utilizados',\n",
    "    'Tweets_Utilizados_coin',\n",
    "    'Tweets_Utilizados_referentes',\n",
    "    'Tweets_Utilizados_whale_alert',\n",
    "    'Buy_1000x_high',\n",
    "    'sell_1000x_high',\n",
    "    'total_trades_binance'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Armado y entrenamiento de un clasificador a partir de los datos originales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelo convolucional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_dataset = pd.read_csv('/Users/mmarchetta/Desktop/Tesis-2024/data-visualization/final_dataset.csv') \n",
    "classifier_dataset = complete_dataset[columns]\n",
    "classifier_dataset['Open_time'] = pd.to_datetime(classifier_dataset['Open_time'])\n",
    "classifier_dataset['Tendencia'] = complete_dataset['Tendencia']\n",
    "\n",
    "clasifier_validation = classifier_dataset[-10:]\n",
    "classifier_dataset = classifier_dataset[:-10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open_time</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Number of trades</th>\n",
       "      <th>Close_BTCUSDT</th>\n",
       "      <th>Volume_BTCUSDT</th>\n",
       "      <th>Number_of_trades_BTCUSDT</th>\n",
       "      <th>Close_ETHUSDT</th>\n",
       "      <th>Volume_ETHUSDT</th>\n",
       "      <th>Number_of_trades_ETHUSDT</th>\n",
       "      <th>Close_BNBUSDT</th>\n",
       "      <th>Volume_BNBUSDT</th>\n",
       "      <th>Number_of_trades_BNBUSDT</th>\n",
       "      <th>SMA_20</th>\n",
       "      <th>EMA_20</th>\n",
       "      <th>Upper_Band</th>\n",
       "      <th>Middle_Band</th>\n",
       "      <th>Lower_Band</th>\n",
       "      <th>RSI</th>\n",
       "      <th>buy_1000x_high_coinbase</th>\n",
       "      <th>sell_1000x_high_coinbase</th>\n",
       "      <th>total_trades_coinbase</th>\n",
       "      <th>Tweets_Utilizados</th>\n",
       "      <th>Tweets_Utilizados_coin</th>\n",
       "      <th>Tweets_Utilizados_referentes</th>\n",
       "      <th>Tweets_Utilizados_whale_alert</th>\n",
       "      <th>Buy_1000x_high</th>\n",
       "      <th>sell_1000x_high</th>\n",
       "      <th>total_trades_binance</th>\n",
       "      <th>Tendencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>941</th>\n",
       "      <td>2024-04-25</td>\n",
       "      <td>6.93</td>\n",
       "      <td>7.00</td>\n",
       "      <td>6.70</td>\n",
       "      <td>71088.00</td>\n",
       "      <td>64498.34</td>\n",
       "      <td>31341.46</td>\n",
       "      <td>1375324.00</td>\n",
       "      <td>3155.80</td>\n",
       "      <td>352288.55</td>\n",
       "      <td>861077.00</td>\n",
       "      <td>613.20</td>\n",
       "      <td>453745.52</td>\n",
       "      <td>353114.00</td>\n",
       "      <td>7.43</td>\n",
       "      <td>7.45</td>\n",
       "      <td>9.08</td>\n",
       "      <td>7.43</td>\n",
       "      <td>5.77</td>\n",
       "      <td>38.83</td>\n",
       "      <td>21.00</td>\n",
       "      <td>26.00</td>\n",
       "      <td>33468.00</td>\n",
       "      <td>151</td>\n",
       "      <td>114</td>\n",
       "      <td>0.00</td>\n",
       "      <td>22.00</td>\n",
       "      <td>242.00</td>\n",
       "      <td>219.00</td>\n",
       "      <td>48000.00</td>\n",
       "      <td>Lateral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942</th>\n",
       "      <td>2024-04-26</td>\n",
       "      <td>6.86</td>\n",
       "      <td>6.95</td>\n",
       "      <td>6.71</td>\n",
       "      <td>67383.00</td>\n",
       "      <td>63770.01</td>\n",
       "      <td>27085.19</td>\n",
       "      <td>1025561.00</td>\n",
       "      <td>3131.30</td>\n",
       "      <td>252522.65</td>\n",
       "      <td>628635.00</td>\n",
       "      <td>598.00</td>\n",
       "      <td>302119.88</td>\n",
       "      <td>269508.00</td>\n",
       "      <td>7.34</td>\n",
       "      <td>7.38</td>\n",
       "      <td>8.94</td>\n",
       "      <td>7.34</td>\n",
       "      <td>5.74</td>\n",
       "      <td>37.81</td>\n",
       "      <td>29.00</td>\n",
       "      <td>24.00</td>\n",
       "      <td>26619.00</td>\n",
       "      <td>117</td>\n",
       "      <td>106</td>\n",
       "      <td>0.00</td>\n",
       "      <td>14.00</td>\n",
       "      <td>292.00</td>\n",
       "      <td>324.00</td>\n",
       "      <td>42000.00</td>\n",
       "      <td>Lateral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>943</th>\n",
       "      <td>2024-04-27</td>\n",
       "      <td>6.76</td>\n",
       "      <td>6.87</td>\n",
       "      <td>6.51</td>\n",
       "      <td>64779.00</td>\n",
       "      <td>63461.98</td>\n",
       "      <td>20933.06</td>\n",
       "      <td>912422.00</td>\n",
       "      <td>3255.56</td>\n",
       "      <td>323811.19</td>\n",
       "      <td>734026.00</td>\n",
       "      <td>596.20</td>\n",
       "      <td>268783.91</td>\n",
       "      <td>233820.00</td>\n",
       "      <td>7.24</td>\n",
       "      <td>7.33</td>\n",
       "      <td>8.73</td>\n",
       "      <td>7.24</td>\n",
       "      <td>5.76</td>\n",
       "      <td>38.57</td>\n",
       "      <td>17.00</td>\n",
       "      <td>17.00</td>\n",
       "      <td>25565.00</td>\n",
       "      <td>101</td>\n",
       "      <td>138</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>248.00</td>\n",
       "      <td>179.00</td>\n",
       "      <td>41000.00</td>\n",
       "      <td>Lateral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>944</th>\n",
       "      <td>2024-04-28</td>\n",
       "      <td>6.81</td>\n",
       "      <td>6.95</td>\n",
       "      <td>6.69</td>\n",
       "      <td>43208.00</td>\n",
       "      <td>63118.62</td>\n",
       "      <td>16949.20</td>\n",
       "      <td>790652.00</td>\n",
       "      <td>3263.45</td>\n",
       "      <td>304766.01</td>\n",
       "      <td>753239.00</td>\n",
       "      <td>600.20</td>\n",
       "      <td>258059.43</td>\n",
       "      <td>206703.00</td>\n",
       "      <td>7.13</td>\n",
       "      <td>7.27</td>\n",
       "      <td>8.38</td>\n",
       "      <td>7.13</td>\n",
       "      <td>5.88</td>\n",
       "      <td>37.66</td>\n",
       "      <td>16.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>20954.00</td>\n",
       "      <td>82</td>\n",
       "      <td>106</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13.00</td>\n",
       "      <td>173.00</td>\n",
       "      <td>165.00</td>\n",
       "      <td>26000.00</td>\n",
       "      <td>Lateral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>945</th>\n",
       "      <td>2024-04-29</td>\n",
       "      <td>6.73</td>\n",
       "      <td>6.83</td>\n",
       "      <td>6.47</td>\n",
       "      <td>63006.00</td>\n",
       "      <td>63866.00</td>\n",
       "      <td>28150.23</td>\n",
       "      <td>1152296.00</td>\n",
       "      <td>3216.73</td>\n",
       "      <td>421831.29</td>\n",
       "      <td>943719.00</td>\n",
       "      <td>592.80</td>\n",
       "      <td>330474.01</td>\n",
       "      <td>271926.00</td>\n",
       "      <td>7.03</td>\n",
       "      <td>7.20</td>\n",
       "      <td>8.08</td>\n",
       "      <td>7.03</td>\n",
       "      <td>5.97</td>\n",
       "      <td>36.02</td>\n",
       "      <td>69.00</td>\n",
       "      <td>37.00</td>\n",
       "      <td>33959.00</td>\n",
       "      <td>115</td>\n",
       "      <td>125</td>\n",
       "      <td>0.00</td>\n",
       "      <td>24.00</td>\n",
       "      <td>260.00</td>\n",
       "      <td>188.00</td>\n",
       "      <td>41000.00</td>\n",
       "      <td>Bajista</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Open_time  Open  High  Low  Number of trades  Close_BTCUSDT  \\\n",
       "941 2024-04-25  6.93  7.00 6.70          71088.00       64498.34   \n",
       "942 2024-04-26  6.86  6.95 6.71          67383.00       63770.01   \n",
       "943 2024-04-27  6.76  6.87 6.51          64779.00       63461.98   \n",
       "944 2024-04-28  6.81  6.95 6.69          43208.00       63118.62   \n",
       "945 2024-04-29  6.73  6.83 6.47          63006.00       63866.00   \n",
       "\n",
       "     Volume_BTCUSDT  Number_of_trades_BTCUSDT  Close_ETHUSDT  Volume_ETHUSDT  \\\n",
       "941        31341.46                1375324.00        3155.80       352288.55   \n",
       "942        27085.19                1025561.00        3131.30       252522.65   \n",
       "943        20933.06                 912422.00        3255.56       323811.19   \n",
       "944        16949.20                 790652.00        3263.45       304766.01   \n",
       "945        28150.23                1152296.00        3216.73       421831.29   \n",
       "\n",
       "     Number_of_trades_ETHUSDT  Close_BNBUSDT  Volume_BNBUSDT  \\\n",
       "941                 861077.00         613.20       453745.52   \n",
       "942                 628635.00         598.00       302119.88   \n",
       "943                 734026.00         596.20       268783.91   \n",
       "944                 753239.00         600.20       258059.43   \n",
       "945                 943719.00         592.80       330474.01   \n",
       "\n",
       "     Number_of_trades_BNBUSDT  SMA_20  EMA_20  Upper_Band  Middle_Band  \\\n",
       "941                 353114.00    7.43    7.45        9.08         7.43   \n",
       "942                 269508.00    7.34    7.38        8.94         7.34   \n",
       "943                 233820.00    7.24    7.33        8.73         7.24   \n",
       "944                 206703.00    7.13    7.27        8.38         7.13   \n",
       "945                 271926.00    7.03    7.20        8.08         7.03   \n",
       "\n",
       "     Lower_Band   RSI  buy_1000x_high_coinbase  sell_1000x_high_coinbase  \\\n",
       "941        5.77 38.83                    21.00                     26.00   \n",
       "942        5.74 37.81                    29.00                     24.00   \n",
       "943        5.76 38.57                    17.00                     17.00   \n",
       "944        5.88 37.66                    16.00                     20.00   \n",
       "945        5.97 36.02                    69.00                     37.00   \n",
       "\n",
       "     total_trades_coinbase  Tweets_Utilizados  Tweets_Utilizados_coin  \\\n",
       "941               33468.00                151                     114   \n",
       "942               26619.00                117                     106   \n",
       "943               25565.00                101                     138   \n",
       "944               20954.00                 82                     106   \n",
       "945               33959.00                115                     125   \n",
       "\n",
       "     Tweets_Utilizados_referentes  Tweets_Utilizados_whale_alert  \\\n",
       "941                          0.00                          22.00   \n",
       "942                          0.00                          14.00   \n",
       "943                          0.00                           7.00   \n",
       "944                          0.00                          13.00   \n",
       "945                          0.00                          24.00   \n",
       "\n",
       "     Buy_1000x_high  sell_1000x_high  total_trades_binance Tendencia  \n",
       "941          242.00           219.00              48000.00   Lateral  \n",
       "942          292.00           324.00              42000.00   Lateral  \n",
       "943          248.00           179.00              41000.00   Lateral  \n",
       "944          173.00           165.00              26000.00   Lateral  \n",
       "945          260.00           188.00              41000.00   Bajista  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(classifier_dataset.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(946, 31)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = classifier_dataset.drop(columns=[\"Tendencia\", \"Open_time\"])\n",
    "y = classifier_dataset[\"Tendencia\"]\n",
    "\n",
    "# label_encoder = LabelEncoder()\n",
    "# y_encoded = label_encoder.fit_transform(classifier_dataset[\"Tendencia\"])\n",
    "\n",
    "y = y.to_numpy().reshape(-1, 1)\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "y_one_hot = onehot_encoder.fit_transform(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(y_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create the model\n",
    "def create_model(activation, units, dropout, learning_rate, l2_penalty, depth, optimizer='adam'):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(units, kernel_size=3, activation=activation, input_shape=(len(X.columns), 1)))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    for _ in range(depth - 1):\n",
    "        model.add(Conv1D(units, kernel_size=3, activation=activation))\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units, activation=activation, kernel_regularizer=l2(l2_penalty)))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(units=3, activation='softmax'))\n",
    "\n",
    "    if optimizer == 'adam':\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == 'rmsprop':\n",
    "        optimizer = RMSprop(learning_rate=learning_rate)\n",
    "    elif optimizer == 'sgd':\n",
    "        optimizer = SGD(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "regressor = KerasRegressor(build_fn=create_model, verbose=0, activation='relu', units=50, dropout=0.2, learning_rate=0.1, l2_penalty=0.001, depth=2, optimizer='adam')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# Define cross-validation\n",
    "cv = TimeSeriesSplit(n_splits=10).split(X)\n",
    "\n",
    "# Define parameter space\n",
    "param_space = {\n",
    "    'depth': [2, 3],\n",
    "    'activation': ['relu', 'tanh', 'swish', 'selu'],\n",
    "    'units': [64, 128, 256, 512],\n",
    "    'dropout': [0.1, 0.2, 0.3, 0.4],\n",
    "    'learning_rate': [0.01, 0.001, 0.0001],\n",
    "    'epochs': [10, 20, 30, 50, 100],\n",
    "    'batch_size': [32, 64, 128],\n",
    "    'optimizer': ['adam', 'rmsprop', 'sgd'],\n",
    "    'l2_penalty': [0.001, 0.01, 0.1]\n",
    "}\n",
    "\n",
    "def categorical_crossentropy_loss(estimator, X_test, y_test):\n",
    "    y_pred = estimator.predict(X_test)\n",
    "    loss = log_loss(y_test, y_pred)\n",
    "    return loss\n",
    "\n",
    "# Perform Bayesian optimization\n",
    "bayes_search = BayesSearchCV(regressor, param_space, scoring=categorical_crossentropy_loss, cv=cv,verbose=1)\n",
    "bayes_result = bayes_search.fit(X, y_one_hot, callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 6.935171386702793\n",
      "Best parameters: OrderedDict([('activation', 'selu'), ('batch_size', 128), ('depth', 3), ('dropout', 0.3), ('epochs', 10), ('l2_penalty', 0.001), ('learning_rate', 0.01), ('optimizer', 'rmsprop'), ('units', 512)])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KerasRegressor(\n",
       "\tmodel=None\n",
       "\tbuild_fn=&lt;function create_model at 0x7fe48cdbad30&gt;\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=rmsprop\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=128\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=0\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=10\n",
       "\tactivation=selu\n",
       "\tunits=512\n",
       "\tdropout=0.3\n",
       "\tlearning_rate=0.01\n",
       "\tl2_penalty=0.001\n",
       "\tdepth=3\n",
       ")</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KerasRegressor</label><div class=\"sk-toggleable__content\"><pre>KerasRegressor(\n",
       "\tmodel=None\n",
       "\tbuild_fn=&lt;function create_model at 0x7fe48cdbad30&gt;\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=rmsprop\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=128\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=0\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=10\n",
       "\tactivation=selu\n",
       "\tunits=512\n",
       "\tdropout=0.3\n",
       "\tlearning_rate=0.01\n",
       "\tl2_penalty=0.001\n",
       "\tdepth=3\n",
       ")</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KerasRegressor(\n",
       "\tmodel=None\n",
       "\tbuild_fn=<function create_model at 0x7fe48cdbad30>\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=rmsprop\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=128\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=0\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=10\n",
       "\tactivation=selu\n",
       "\tunits=512\n",
       "\tdropout=0.3\n",
       "\tlearning_rate=0.01\n",
       "\tl2_penalty=0.001\n",
       "\tdepth=3\n",
       ")"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show best results\n",
    "print(\"Best score:\", bayes_result.best_score_)\n",
    "print(\"Best parameters:\", bayes_result.best_params_)\n",
    "\n",
    "# Entrenar el modelo con los mejores hiperparámetros\n",
    "best_model = bayes_result.best_estimator_\n",
    "best_model.fit(X, y_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 mejores modelos:\n",
      "Modelo 1\n",
      "Hiperparámetros: OrderedDict([('activation', 'selu'), ('batch_size', 128), ('depth', 2), ('dropout', 0.2), ('epochs', 20), ('l2_penalty', 0.1), ('learning_rate', 0.001), ('optimizer', 'adam'), ('units', 64)])\n",
      "Puntaje: 1.5275572624457179\n",
      "Modelo 2\n",
      "Hiperparámetros: OrderedDict([('activation', 'swish'), ('batch_size', 128), ('depth', 3), ('dropout', 0.2), ('epochs', 100), ('l2_penalty', 0.01), ('learning_rate', 0.01), ('optimizer', 'adam'), ('units', 64)])\n",
      "Puntaje: 1.5749713206672962\n",
      "Modelo 3\n",
      "Hiperparámetros: OrderedDict([('activation', 'swish'), ('batch_size', 64), ('depth', 2), ('dropout', 0.1), ('epochs', 10), ('l2_penalty', 0.1), ('learning_rate', 0.0001), ('optimizer', 'rmsprop'), ('units', 256)])\n",
      "Puntaje: 1.5119730230325128\n",
      "Modelo 4\n",
      "Hiperparámetros: OrderedDict([('activation', 'relu'), ('batch_size', 32), ('depth', 2), ('dropout', 0.3), ('epochs', 100), ('l2_penalty', 0.001), ('learning_rate', 0.001), ('optimizer', 'rmsprop'), ('units', 64)])\n",
      "Puntaje: 1.1974772418115385\n",
      "Modelo 5\n",
      "Hiperparámetros: OrderedDict([('activation', 'swish'), ('batch_size', 32), ('depth', 3), ('dropout', 0.4), ('epochs', 100), ('l2_penalty', 0.001), ('learning_rate', 0.01), ('optimizer', 'adam'), ('units', 256)])\n",
      "Puntaje: 1.547862837107821\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Obtener los hiperparámetros y puntajes de los 5 mejores modelos\n",
    "top_n_models = 5\n",
    "best_params_list = []\n",
    "best_scores_list = []\n",
    "\n",
    "for i in range(min(top_n_models, len(bayes_search.cv_results_['params']))):\n",
    "    best_params_list.append(bayes_search.cv_results_['params'][i])\n",
    "    best_scores_list.append(bayes_search.cv_results_['mean_test_score'][i])\n",
    "\n",
    "# Guardar los hiperparámetros de los 5 mejores modelos en un archivo JSON\n",
    "with open('conv_classifier/top_5_hyperparameters.json', 'w') as f:\n",
    "    json.dump({'best_params': best_params_list, 'best_scores': best_scores_list}, f)\n",
    "\n",
    "# O imprimir los hiperparámetros\n",
    "print(\"Top 5 mejores modelos:\")\n",
    "for i in range(len(best_params_list)):\n",
    "    print(\"Modelo\", i+1)\n",
    "    print(\"Hiperparámetros:\", best_params_list[i])\n",
    "    print(\"Puntaje:\", best_scores_list[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Armado del ensamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prime_seeds(n):\n",
    "    seeds = []\n",
    "    num = 70001  # Comenzamos desde el primer número primo mayor que 70000\n",
    "    while len(seeds) < n:\n",
    "        is_prime = True\n",
    "        for i in range(2, int(num**0.5) + 1):\n",
    "            if num % i == 0:\n",
    "                is_prime = False\n",
    "                break\n",
    "        if is_prime:\n",
    "            seeds.append(num)\n",
    "        num += 1\n",
    "    return seeds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clase personalizada para hacer el ensamble, dado que sklearn no provee ninguna clase que permita hacer ensmble\n",
    "## de modelos re regresion multivariados\n",
    "class MultivariableVotingRegressor:\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for model in self.models:\n",
    "            model.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Hacer predicciones con cada modelo\n",
    "        predictions = np.array([model.predict(X) for model in self.models])\n",
    "        \n",
    "        # Calcular la moda de las predicciones\n",
    "        mode_predictions = np.argmax(np.sum(predictions, axis=0), axis=1)\n",
    "        \n",
    "        return mode_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "# Leer los hiperparámetros desde el archivo JSON\n",
    "with open('conv_classifier/top_5_hyperparameters.json', 'r') as f:\n",
    "    top_hyperparameters = json.load(f)\n",
    "\n",
    "prime_seeds = generate_prime_seeds(300)\n",
    "models = []\n",
    "best_seeds= {}\n",
    "\n",
    "# Train models with different seeds for each set of hyperparameters\n",
    "for mode_number, params in enumerate(top_hyperparameters['best_params']):\n",
    "    best_validation_errors = {}\n",
    "    \n",
    "    for seed_number, seed in enumerate(prime_seeds):\n",
    "        model = KerasRegressor(build_fn=create_model, random_state=seed, verbose=0, **params)\n",
    "        model.fit(X, y_one_hot)\n",
    "        \n",
    "        # Make predictions with the model\n",
    "        model_predictions = model.predict(X)\n",
    "        \n",
    "        # Calculate error (training error)\n",
    "        train_error = categorical_crossentropy(y_one_hot, model_predictions)\n",
    "        \n",
    "        mean_train_error = np.mean(train_error)\n",
    "\n",
    "        # Update best validation error for this seed\n",
    "        best_validation_errors[seed] = mean_train_error\n",
    "    \n",
    "    # print(\"Best validation errors:\", best_validation_errors)\n",
    "\n",
    "    # Find the best seed for this set of hyperparameters\n",
    "    best_seed_for_params = min(best_validation_errors, key=lambda k: best_validation_errors[k])\n",
    "    best_seeds[str(params)] = best_seed_for_params\n",
    "    \n",
    "    # Create and train the model with the best seed\n",
    "    model = KerasRegressor(build_fn=create_model, random_state=best_seed_for_params, verbose=0, **params)\n",
    "    model.fit(X, y_one_hot)\n",
    "    models.append(model)\n",
    "\n",
    "# Define and train the ensemble model\n",
    "ensemble = MultivariableVotingRegressor(models)\n",
    "ensemble.fit(X, y_one_hot)\n",
    "\n",
    "# Save the best seeds to a JSON file\n",
    "with open('conv_classifier/best_seeds.json', 'w') as f:\n",
    "    json.dump(best_seeds, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clasificacion con el ensamble sobre las redicciones de los modelos generativos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "8/8 [==============================] - 5s 36ms/step - loss: 11.3976 - accuracy: 0.4017\n",
      "Epoch 2/20\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 9.7886 - accuracy: 0.3996\n",
      "Epoch 3/20\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 8.3017 - accuracy: 0.4440\n",
      "Epoch 4/20\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 7.0857 - accuracy: 0.4408\n",
      "Epoch 5/20\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 6.0468 - accuracy: 0.4397\n",
      "Epoch 6/20\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 5.1923 - accuracy: 0.4524\n",
      "Epoch 7/20\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 4.4225 - accuracy: 0.4683\n",
      "Epoch 8/20\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.8545 - accuracy: 0.4535\n",
      "Epoch 9/20\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 3.3517 - accuracy: 0.4556\n",
      "Epoch 10/20\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.9717 - accuracy: 0.4323\n",
      "Epoch 11/20\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 2.6048 - accuracy: 0.4704\n",
      "Epoch 12/20\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 2.3421 - accuracy: 0.4651\n",
      "Epoch 13/20\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 2.1396 - accuracy: 0.4831\n",
      "Epoch 14/20\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 1.9714 - accuracy: 0.4493\n",
      "Epoch 15/20\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 1.8215 - accuracy: 0.4873\n",
      "Epoch 16/20\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 1.7092 - accuracy: 0.4651\n",
      "Epoch 17/20\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 1.5581 - accuracy: 0.4884\n",
      "Epoch 18/20\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 1.4863 - accuracy: 0.4757\n",
      "Epoch 19/20\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.4133 - accuracy: 0.5148\n",
      "Epoch 20/20\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 1.3820 - accuracy: 0.4873\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 5s 47ms/step - loss: 2.0621 - accuracy: 0.3985\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 1.6804 - accuracy: 0.4175\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 1.5190 - accuracy: 0.4471\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 1.3945 - accuracy: 0.4471\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.3483 - accuracy: 0.4302\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.2581 - accuracy: 0.4471\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.1777 - accuracy: 0.4672\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.1467 - accuracy: 0.4799\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 1.1441 - accuracy: 0.4440\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 1.1025 - accuracy: 0.4789\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 1.1062 - accuracy: 0.4831\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 1.0758 - accuracy: 0.4778\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 1.0811 - accuracy: 0.4778\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 1.0713 - accuracy: 0.4715\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.0609 - accuracy: 0.4884\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 1.0538 - accuracy: 0.4767\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 1.0556 - accuracy: 0.4598\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 1.0377 - accuracy: 0.5000\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 1.0291 - accuracy: 0.4947\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 1.0303 - accuracy: 0.4736\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 1.0404 - accuracy: 0.4630\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.0345 - accuracy: 0.4968\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.0213 - accuracy: 0.4725\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 1.0240 - accuracy: 0.4598\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 1.0098 - accuracy: 0.4841\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 1.0354 - accuracy: 0.4852\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.0224 - accuracy: 0.4894\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.0255 - accuracy: 0.4767\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 1.0443 - accuracy: 0.4588\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 1.0396 - accuracy: 0.4598\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 1.0287 - accuracy: 0.4958\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 1.0264 - accuracy: 0.4884\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 1.0228 - accuracy: 0.4820\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 1.0348 - accuracy: 0.4915\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 1.0298 - accuracy: 0.4619\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.0192 - accuracy: 0.4841\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 1.0143 - accuracy: 0.4789\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 1.0149 - accuracy: 0.4937\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 1.0151 - accuracy: 0.4789\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 1.0440 - accuracy: 0.4799\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 1.0242 - accuracy: 0.4651\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 1.0075 - accuracy: 0.4884\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 1.0097 - accuracy: 0.4947\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 1.0169 - accuracy: 0.4810\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0233 - accuracy: 0.4725\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 1.0241 - accuracy: 0.4947\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 1.0188 - accuracy: 0.4683\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 1.0059 - accuracy: 0.5000\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 1.0055 - accuracy: 0.4958\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 1.0249 - accuracy: 0.4778\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 1.0076 - accuracy: 0.5032\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 1.0090 - accuracy: 0.4894\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.0106 - accuracy: 0.5085\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.0034 - accuracy: 0.4926\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 1.0119 - accuracy: 0.5053\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 1.0085 - accuracy: 0.5085\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 1.0015 - accuracy: 0.4968\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 0.9915 - accuracy: 0.4958\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 1.0106 - accuracy: 0.4820\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 1.0103 - accuracy: 0.4979\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 1.0029 - accuracy: 0.5137\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 1.0135 - accuracy: 0.4968\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 1.0122 - accuracy: 0.4905\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.0008 - accuracy: 0.5000\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 0.9914 - accuracy: 0.5021\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 1.0064 - accuracy: 0.4852\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 0.9980 - accuracy: 0.5211\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 1.0010 - accuracy: 0.4905\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 1.0140 - accuracy: 0.4831\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.0047 - accuracy: 0.5085\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.0065 - accuracy: 0.5000\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.0009 - accuracy: 0.5116\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 0.9997 - accuracy: 0.5159\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.0089 - accuracy: 0.4799\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 1.0140 - accuracy: 0.5063\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 0.9976 - accuracy: 0.4926\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 0.9952 - accuracy: 0.5000\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 1.0083 - accuracy: 0.5074\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.0100 - accuracy: 0.4820\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 1.0038 - accuracy: 0.5021\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 0.9965 - accuracy: 0.4905\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 0.9899 - accuracy: 0.5317\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 0.9923 - accuracy: 0.4937\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 0.9950 - accuracy: 0.4937\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 0.9814 - accuracy: 0.5116\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 0.9840 - accuracy: 0.5021\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 0.9942 - accuracy: 0.5063\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 1.0115 - accuracy: 0.4979\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.0163 - accuracy: 0.4915\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 1.0119 - accuracy: 0.4873\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 0.9909 - accuracy: 0.5148\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.0085 - accuracy: 0.4884\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.0031 - accuracy: 0.5106\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.9972 - accuracy: 0.4915\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 0.9905 - accuracy: 0.5159\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 0.9744 - accuracy: 0.5169\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 0.9821 - accuracy: 0.5127\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.9859 - accuracy: 0.5159\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 0.9823 - accuracy: 0.5053\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 0.9828 - accuracy: 0.5180\n",
      "Epoch 1/10\n",
      "15/15 [==============================] - 5s 59ms/step - loss: 42.8091 - accuracy: 0.4281\n",
      "Epoch 2/10\n",
      "15/15 [==============================] - 1s 50ms/step - loss: 40.4128 - accuracy: 0.4376\n",
      "Epoch 3/10\n",
      "15/15 [==============================] - 1s 64ms/step - loss: 38.1178 - accuracy: 0.4937\n",
      "Epoch 4/10\n",
      "15/15 [==============================] - 1s 56ms/step - loss: 35.9858 - accuracy: 0.4905\n",
      "Epoch 5/10\n",
      "15/15 [==============================] - 1s 54ms/step - loss: 33.9603 - accuracy: 0.4789\n",
      "Epoch 6/10\n",
      "15/15 [==============================] - 1s 59ms/step - loss: 32.0212 - accuracy: 0.5042\n",
      "Epoch 7/10\n",
      "15/15 [==============================] - 1s 56ms/step - loss: 30.2478 - accuracy: 0.4863\n",
      "Epoch 8/10\n",
      "15/15 [==============================] - 1s 56ms/step - loss: 28.4941 - accuracy: 0.5254\n",
      "Epoch 9/10\n",
      "15/15 [==============================] - 1s 50ms/step - loss: 26.8619 - accuracy: 0.5476\n",
      "Epoch 10/10\n",
      "15/15 [==============================] - 1s 56ms/step - loss: 25.3420 - accuracy: 0.5180\n",
      "Epoch 1/100\n",
      "30/30 [==============================] - 4s 17ms/step - loss: 1.3181 - accuracy: 0.3753\n",
      "Epoch 2/100\n",
      "30/30 [==============================] - 1s 20ms/step - loss: 1.2105 - accuracy: 0.4281\n",
      "Epoch 3/100\n",
      "30/30 [==============================] - 1s 20ms/step - loss: 1.2178 - accuracy: 0.4038\n",
      "Epoch 4/100\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 1.1733 - accuracy: 0.4271\n",
      "Epoch 5/100\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.2018 - accuracy: 0.4101\n",
      "Epoch 6/100\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 1.1558 - accuracy: 0.4651\n",
      "Epoch 7/100\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 1.1778 - accuracy: 0.4323\n",
      "Epoch 8/100\n",
      "30/30 [==============================] - 1s 22ms/step - loss: 1.1687 - accuracy: 0.4366\n",
      "Epoch 9/100\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.1705 - accuracy: 0.4070\n",
      "Epoch 10/100\n",
      "30/30 [==============================] - 1s 21ms/step - loss: 1.1464 - accuracy: 0.4440\n",
      "Epoch 11/100\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 1.1347 - accuracy: 0.4397\n",
      "Epoch 12/100\n",
      "30/30 [==============================] - 1s 24ms/step - loss: 1.1401 - accuracy: 0.4292\n",
      "Epoch 13/100\n",
      "30/30 [==============================] - 1s 24ms/step - loss: 1.1330 - accuracy: 0.4334\n",
      "Epoch 14/100\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.1366 - accuracy: 0.4154\n",
      "Epoch 15/100\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.1223 - accuracy: 0.4461\n",
      "Epoch 16/100\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.1120 - accuracy: 0.4376\n",
      "Epoch 17/100\n",
      "30/30 [==============================] - 1s 26ms/step - loss: 1.1156 - accuracy: 0.4545\n",
      "Epoch 18/100\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 1.1052 - accuracy: 0.4577\n",
      "Epoch 19/100\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 1.1086 - accuracy: 0.4461\n",
      "Epoch 20/100\n",
      "30/30 [==============================] - 1s 21ms/step - loss: 1.1003 - accuracy: 0.4577\n",
      "Epoch 21/100\n",
      "30/30 [==============================] - 1s 21ms/step - loss: 1.0946 - accuracy: 0.4503\n",
      "Epoch 22/100\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 1.0983 - accuracy: 0.4577\n",
      "Epoch 23/100\n",
      "30/30 [==============================] - 1s 20ms/step - loss: 1.1065 - accuracy: 0.4313\n",
      "Epoch 24/100\n",
      "30/30 [==============================] - 1s 21ms/step - loss: 1.1067 - accuracy: 0.4228\n",
      "Epoch 25/100\n",
      "30/30 [==============================] - 1s 20ms/step - loss: 1.0909 - accuracy: 0.4503\n",
      "Epoch 26/100\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 1.0950 - accuracy: 0.4429\n",
      "Epoch 27/100\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 1.0765 - accuracy: 0.4577\n",
      "Epoch 28/100\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.0877 - accuracy: 0.4429\n",
      "Epoch 29/100\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 1.0708 - accuracy: 0.4588\n",
      "Epoch 30/100\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 1.0733 - accuracy: 0.4545\n",
      "Epoch 31/100\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 1.0706 - accuracy: 0.4630\n",
      "Epoch 32/100\n",
      "30/30 [==============================] - 1s 21ms/step - loss: 1.0768 - accuracy: 0.4376\n",
      "Epoch 33/100\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.0749 - accuracy: 0.4609\n",
      "Epoch 34/100\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.0899 - accuracy: 0.4419\n",
      "Epoch 35/100\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.0824 - accuracy: 0.4471\n",
      "Epoch 36/100\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.0875 - accuracy: 0.4080\n",
      "Epoch 37/100\n",
      "30/30 [==============================] - 1s 23ms/step - loss: 1.0823 - accuracy: 0.4345\n",
      "Epoch 38/100\n",
      "30/30 [==============================] - 1s 20ms/step - loss: 1.0757 - accuracy: 0.4503\n",
      "Epoch 39/100\n",
      "30/30 [==============================] - 1s 20ms/step - loss: 1.0730 - accuracy: 0.4736\n",
      "Epoch 40/100\n",
      "30/30 [==============================] - 1s 20ms/step - loss: 1.0703 - accuracy: 0.4471\n",
      "Epoch 41/100\n",
      "30/30 [==============================] - 1s 20ms/step - loss: 1.0815 - accuracy: 0.4419\n",
      "Epoch 42/100\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 1.0808 - accuracy: 0.4408\n",
      "Epoch 43/100\n",
      "30/30 [==============================] - 1s 23ms/step - loss: 1.0667 - accuracy: 0.4514\n",
      "Epoch 44/100\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.0764 - accuracy: 0.4419\n",
      "Epoch 45/100\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.0618 - accuracy: 0.4704\n",
      "Epoch 46/100\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 1.0551 - accuracy: 0.4493\n",
      "Epoch 47/100\n",
      "30/30 [==============================] - 1s 20ms/step - loss: 1.0589 - accuracy: 0.4630\n",
      "Epoch 48/100\n",
      "30/30 [==============================] - 1s 22ms/step - loss: 1.0575 - accuracy: 0.4440\n",
      "Epoch 49/100\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 1.0679 - accuracy: 0.4366\n",
      "Epoch 50/100\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 1.0605 - accuracy: 0.4662\n",
      "Epoch 51/100\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 1.0575 - accuracy: 0.4556\n",
      "Epoch 52/100\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 1.0542 - accuracy: 0.4715\n",
      "Epoch 53/100\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.0609 - accuracy: 0.4408\n",
      "Epoch 54/100\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 1.0574 - accuracy: 0.4471\n",
      "Epoch 55/100\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 1.0536 - accuracy: 0.4408\n",
      "Epoch 56/100\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 1.0501 - accuracy: 0.4461\n",
      "Epoch 57/100\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 1.0500 - accuracy: 0.4535\n",
      "Epoch 58/100\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 1.0455 - accuracy: 0.4641\n",
      "Epoch 59/100\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 1.0455 - accuracy: 0.4789\n",
      "Epoch 60/100\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 1.0414 - accuracy: 0.4567\n",
      "Epoch 61/100\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 1.0411 - accuracy: 0.4736\n",
      "Epoch 62/100\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 1.0407 - accuracy: 0.4630\n",
      "Epoch 63/100\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 1.0427 - accuracy: 0.4672\n",
      "Epoch 64/100\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.0392 - accuracy: 0.4630\n",
      "Epoch 65/100\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 1.0477 - accuracy: 0.4704\n",
      "Epoch 66/100\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 1.0412 - accuracy: 0.4577\n",
      "Epoch 67/100\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 1.0387 - accuracy: 0.4577\n",
      "Epoch 68/100\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 1.0368 - accuracy: 0.4683\n",
      "Epoch 69/100\n",
      "30/30 [==============================] - 1s 20ms/step - loss: 1.0497 - accuracy: 0.4503\n",
      "Epoch 70/100\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.0565 - accuracy: 0.4387\n",
      "Epoch 71/100\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 1.0386 - accuracy: 0.4683\n",
      "Epoch 72/100\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 1.0382 - accuracy: 0.4704\n",
      "Epoch 73/100\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 1.0370 - accuracy: 0.4693\n",
      "Epoch 74/100\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 1.0514 - accuracy: 0.4651\n",
      "Epoch 75/100\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.0397 - accuracy: 0.4619\n",
      "Epoch 76/100\n",
      "30/30 [==============================] - 0s 17ms/step - loss: 1.0388 - accuracy: 0.4672\n",
      "Epoch 77/100\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 1.0399 - accuracy: 0.4683\n",
      "Epoch 78/100\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 1.0373 - accuracy: 0.4704\n",
      "Epoch 79/100\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 1.0320 - accuracy: 0.4693\n",
      "Epoch 80/100\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.0310 - accuracy: 0.4577\n",
      "Epoch 81/100\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 1.0359 - accuracy: 0.4683\n",
      "Epoch 82/100\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 1.0375 - accuracy: 0.4672\n",
      "Epoch 83/100\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 1.0226 - accuracy: 0.4884\n",
      "Epoch 84/100\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 1.0437 - accuracy: 0.4556\n",
      "Epoch 85/100\n",
      "30/30 [==============================] - 1s 20ms/step - loss: 1.0570 - accuracy: 0.4461\n",
      "Epoch 86/100\n",
      "30/30 [==============================] - 1s 21ms/step - loss: 1.0507 - accuracy: 0.4471\n",
      "Epoch 87/100\n",
      "30/30 [==============================] - 1s 23ms/step - loss: 1.0444 - accuracy: 0.4440\n",
      "Epoch 88/100\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.0376 - accuracy: 0.4598\n",
      "Epoch 89/100\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 1.0371 - accuracy: 0.4514\n",
      "Epoch 90/100\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.0315 - accuracy: 0.4789\n",
      "Epoch 91/100\n",
      "30/30 [==============================] - 1s 20ms/step - loss: 1.0441 - accuracy: 0.4757\n",
      "Epoch 92/100\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 1.0448 - accuracy: 0.4387\n",
      "Epoch 93/100\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 1.0363 - accuracy: 0.4630\n",
      "Epoch 94/100\n",
      "30/30 [==============================] - 1s 22ms/step - loss: 1.0480 - accuracy: 0.4482\n",
      "Epoch 95/100\n",
      "30/30 [==============================] - 1s 20ms/step - loss: 1.0505 - accuracy: 0.4355\n",
      "Epoch 96/100\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.0422 - accuracy: 0.4524\n",
      "Epoch 97/100\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 1.0442 - accuracy: 0.4630\n",
      "Epoch 98/100\n",
      "30/30 [==============================] - 1s 20ms/step - loss: 1.0360 - accuracy: 0.4715\n",
      "Epoch 99/100\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.0497 - accuracy: 0.4577\n",
      "Epoch 100/100\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 1.0421 - accuracy: 0.4609\n",
      "Epoch 1/100\n",
      "30/30 [==============================] - 7s 42ms/step - loss: 1.8026 - accuracy: 0.3584\n",
      "Epoch 2/100\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 1.5578 - accuracy: 0.4260\n",
      "Epoch 3/100\n",
      "30/30 [==============================] - 2s 55ms/step - loss: 1.4857 - accuracy: 0.3816\n",
      "Epoch 4/100\n",
      "30/30 [==============================] - 1s 45ms/step - loss: 1.3214 - accuracy: 0.4249\n",
      "Epoch 5/100\n",
      "30/30 [==============================] - 1s 44ms/step - loss: 1.3290 - accuracy: 0.4387\n",
      "Epoch 6/100\n",
      "30/30 [==============================] - 2s 55ms/step - loss: 1.3083 - accuracy: 0.4218\n",
      "Epoch 7/100\n",
      "30/30 [==============================] - 1s 46ms/step - loss: 1.2216 - accuracy: 0.4249\n",
      "Epoch 8/100\n",
      "30/30 [==============================] - 1s 45ms/step - loss: 1.1860 - accuracy: 0.4397\n",
      "Epoch 9/100\n",
      "30/30 [==============================] - 1s 44ms/step - loss: 1.1948 - accuracy: 0.4313\n",
      "Epoch 10/100\n",
      "30/30 [==============================] - 1s 46ms/step - loss: 1.1775 - accuracy: 0.4292\n",
      "Epoch 11/100\n",
      "30/30 [==============================] - 1s 45ms/step - loss: 1.1680 - accuracy: 0.4197\n",
      "Epoch 12/100\n",
      "30/30 [==============================] - 2s 57ms/step - loss: 1.1622 - accuracy: 0.4133\n",
      "Epoch 13/100\n",
      "30/30 [==============================] - 2s 61ms/step - loss: 1.1591 - accuracy: 0.4366\n",
      "Epoch 14/100\n",
      "30/30 [==============================] - 1s 45ms/step - loss: 1.1739 - accuracy: 0.4175\n",
      "Epoch 15/100\n",
      "30/30 [==============================] - 1s 41ms/step - loss: 1.1548 - accuracy: 0.4175\n",
      "Epoch 16/100\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 1.1362 - accuracy: 0.4228\n",
      "Epoch 17/100\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 1.1453 - accuracy: 0.4186\n",
      "Epoch 18/100\n",
      "30/30 [==============================] - 1s 49ms/step - loss: 1.1281 - accuracy: 0.4355\n",
      "Epoch 19/100\n",
      "30/30 [==============================] - 2s 52ms/step - loss: 1.1171 - accuracy: 0.4419\n",
      "Epoch 20/100\n",
      "30/30 [==============================] - 2s 61ms/step - loss: 1.1049 - accuracy: 0.4545\n",
      "Epoch 21/100\n",
      "30/30 [==============================] - 1s 44ms/step - loss: 1.1094 - accuracy: 0.4387\n",
      "Epoch 22/100\n",
      "30/30 [==============================] - 2s 54ms/step - loss: 1.0928 - accuracy: 0.4651\n",
      "Epoch 23/100\n",
      "30/30 [==============================] - 1s 43ms/step - loss: 1.0971 - accuracy: 0.4482\n",
      "Epoch 24/100\n",
      "30/30 [==============================] - 2s 55ms/step - loss: 1.0973 - accuracy: 0.4482\n",
      "Epoch 25/100\n",
      "30/30 [==============================] - 2s 51ms/step - loss: 1.0963 - accuracy: 0.4292\n",
      "Epoch 26/100\n",
      "30/30 [==============================] - 1s 45ms/step - loss: 1.0897 - accuracy: 0.4302\n",
      "Epoch 27/100\n",
      "30/30 [==============================] - 2s 54ms/step - loss: 1.0717 - accuracy: 0.4672\n",
      "Epoch 28/100\n",
      "30/30 [==============================] - 1s 44ms/step - loss: 1.0987 - accuracy: 0.4503\n",
      "Epoch 29/100\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 1.0857 - accuracy: 0.4535\n",
      "Epoch 30/100\n",
      "30/30 [==============================] - 1s 46ms/step - loss: 1.0664 - accuracy: 0.4641\n",
      "Epoch 31/100\n",
      "30/30 [==============================] - 1s 38ms/step - loss: 1.0732 - accuracy: 0.4619\n",
      "Epoch 32/100\n",
      "30/30 [==============================] - 2s 54ms/step - loss: 1.0761 - accuracy: 0.4397\n",
      "Epoch 33/100\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 1.0723 - accuracy: 0.4461\n",
      "Epoch 34/100\n",
      "30/30 [==============================] - 1s 47ms/step - loss: 1.0691 - accuracy: 0.4693\n",
      "Epoch 35/100\n",
      "30/30 [==============================] - 2s 53ms/step - loss: 1.0610 - accuracy: 0.4767\n",
      "Epoch 36/100\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 1.0667 - accuracy: 0.4778\n",
      "Epoch 37/100\n",
      "30/30 [==============================] - 1s 44ms/step - loss: 1.0701 - accuracy: 0.4503\n",
      "Epoch 38/100\n",
      "30/30 [==============================] - 1s 44ms/step - loss: 1.0657 - accuracy: 0.4736\n",
      "Epoch 39/100\n",
      "30/30 [==============================] - 2s 54ms/step - loss: 1.0648 - accuracy: 0.4440\n",
      "Epoch 40/100\n",
      "30/30 [==============================] - 1s 42ms/step - loss: 1.0618 - accuracy: 0.4662\n",
      "Epoch 41/100\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 1.0495 - accuracy: 0.4725\n",
      "Epoch 42/100\n",
      "30/30 [==============================] - 1s 45ms/step - loss: 1.0623 - accuracy: 0.4662\n",
      "Epoch 43/100\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 1.0555 - accuracy: 0.4630\n",
      "Epoch 44/100\n",
      "30/30 [==============================] - 1s 43ms/step - loss: 1.0776 - accuracy: 0.4355\n",
      "Epoch 45/100\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 1.0642 - accuracy: 0.4609\n",
      "Epoch 46/100\n",
      "30/30 [==============================] - 1s 42ms/step - loss: 1.0710 - accuracy: 0.4313\n",
      "Epoch 47/100\n",
      "30/30 [==============================] - 1s 45ms/step - loss: 1.0466 - accuracy: 0.4767\n",
      "Epoch 48/100\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 1.0628 - accuracy: 0.4482\n",
      "Epoch 49/100\n",
      "30/30 [==============================] - 1s 45ms/step - loss: 1.0560 - accuracy: 0.4662\n",
      "Epoch 50/100\n",
      "30/30 [==============================] - 1s 45ms/step - loss: 1.0611 - accuracy: 0.4556\n",
      "Epoch 51/100\n",
      "30/30 [==============================] - 2s 49ms/step - loss: 1.0464 - accuracy: 0.4672\n",
      "Epoch 52/100\n",
      "30/30 [==============================] - 1s 37ms/step - loss: 1.0685 - accuracy: 0.4376\n",
      "Epoch 53/100\n",
      "30/30 [==============================] - 1s 49ms/step - loss: 1.0617 - accuracy: 0.4482\n",
      "Epoch 54/100\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 1.0547 - accuracy: 0.4672\n",
      "Epoch 55/100\n",
      "30/30 [==============================] - 1s 42ms/step - loss: 1.0548 - accuracy: 0.4789\n",
      "Epoch 56/100\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 1.0671 - accuracy: 0.4556\n",
      "Epoch 57/100\n",
      "30/30 [==============================] - 1s 37ms/step - loss: 1.0573 - accuracy: 0.4577\n",
      "Epoch 58/100\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 1.0551 - accuracy: 0.4598\n",
      "Epoch 59/100\n",
      "30/30 [==============================] - 1s 37ms/step - loss: 1.0731 - accuracy: 0.4619\n",
      "Epoch 60/100\n",
      "30/30 [==============================] - 1s 44ms/step - loss: 1.0528 - accuracy: 0.4662\n",
      "Epoch 61/100\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 1.0550 - accuracy: 0.4799\n",
      "Epoch 62/100\n",
      "30/30 [==============================] - 1s 46ms/step - loss: 1.0420 - accuracy: 0.4810\n",
      "Epoch 63/100\n",
      "30/30 [==============================] - 1s 37ms/step - loss: 1.0316 - accuracy: 0.4937\n",
      "Epoch 64/100\n",
      "30/30 [==============================] - 1s 44ms/step - loss: 1.0518 - accuracy: 0.4651\n",
      "Epoch 65/100\n",
      "30/30 [==============================] - 1s 37ms/step - loss: 1.0615 - accuracy: 0.4514\n",
      "Epoch 66/100\n",
      "30/30 [==============================] - 2s 58ms/step - loss: 1.0569 - accuracy: 0.4577\n",
      "Epoch 67/100\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 1.0387 - accuracy: 0.4704\n",
      "Epoch 68/100\n",
      "30/30 [==============================] - 1s 45ms/step - loss: 1.0559 - accuracy: 0.4715\n",
      "Epoch 69/100\n",
      "30/30 [==============================] - 1s 42ms/step - loss: 1.0574 - accuracy: 0.4609\n",
      "Epoch 70/100\n",
      "30/30 [==============================] - 1s 47ms/step - loss: 1.0488 - accuracy: 0.4873\n",
      "Epoch 71/100\n",
      "30/30 [==============================] - 2s 51ms/step - loss: 1.0504 - accuracy: 0.4672\n",
      "Epoch 72/100\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 1.0563 - accuracy: 0.4630\n",
      "Epoch 73/100\n",
      "30/30 [==============================] - 1s 45ms/step - loss: 1.0658 - accuracy: 0.4651\n",
      "Epoch 74/100\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 1.0542 - accuracy: 0.4715\n",
      "Epoch 75/100\n",
      "30/30 [==============================] - 1s 43ms/step - loss: 1.0594 - accuracy: 0.4641\n",
      "Epoch 76/100\n",
      "30/30 [==============================] - 1s 47ms/step - loss: 1.0632 - accuracy: 0.4567\n",
      "Epoch 77/100\n",
      "30/30 [==============================] - 1s 45ms/step - loss: 1.0467 - accuracy: 0.4588\n",
      "Epoch 78/100\n",
      "30/30 [==============================] - 2s 52ms/step - loss: 1.0359 - accuracy: 0.4662\n",
      "Epoch 79/100\n",
      "30/30 [==============================] - 2s 58ms/step - loss: 1.0457 - accuracy: 0.4693\n",
      "Epoch 80/100\n",
      "30/30 [==============================] - 1s 49ms/step - loss: 1.0352 - accuracy: 0.4736\n",
      "Epoch 81/100\n",
      "30/30 [==============================] - 1s 42ms/step - loss: 1.0496 - accuracy: 0.4482\n",
      "Epoch 82/100\n",
      "30/30 [==============================] - 1s 41ms/step - loss: 1.0375 - accuracy: 0.4683\n",
      "Epoch 83/100\n",
      "30/30 [==============================] - 1s 50ms/step - loss: 1.0503 - accuracy: 0.4651\n",
      "Epoch 84/100\n",
      "30/30 [==============================] - 1s 38ms/step - loss: 1.0365 - accuracy: 0.4609\n",
      "Epoch 85/100\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.0626 - accuracy: 0.4577\n",
      "Epoch 86/100\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 1.0434 - accuracy: 0.4820\n",
      "Epoch 87/100\n",
      "30/30 [==============================] - 2s 50ms/step - loss: 1.0456 - accuracy: 0.4651\n",
      "Epoch 88/100\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 1.0433 - accuracy: 0.4577\n",
      "Epoch 89/100\n",
      "30/30 [==============================] - 1s 47ms/step - loss: 1.0348 - accuracy: 0.4725\n",
      "Epoch 90/100\n",
      "30/30 [==============================] - 1s 41ms/step - loss: 1.0425 - accuracy: 0.4841\n",
      "Epoch 91/100\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 1.0408 - accuracy: 0.4725\n",
      "Epoch 92/100\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 1.0293 - accuracy: 0.4915\n",
      "Epoch 93/100\n",
      "30/30 [==============================] - 1s 46ms/step - loss: 1.0257 - accuracy: 0.4958\n",
      "Epoch 94/100\n",
      "30/30 [==============================] - 1s 44ms/step - loss: 1.0277 - accuracy: 0.4989\n",
      "Epoch 95/100\n",
      "30/30 [==============================] - 1s 42ms/step - loss: 1.0388 - accuracy: 0.4715\n",
      "Epoch 96/100\n",
      "30/30 [==============================] - 1s 44ms/step - loss: 1.0354 - accuracy: 0.4778\n",
      "Epoch 97/100\n",
      "30/30 [==============================] - 1s 49ms/step - loss: 1.0340 - accuracy: 0.4567\n",
      "Epoch 98/100\n",
      "30/30 [==============================] - 1s 45ms/step - loss: 1.0360 - accuracy: 0.4609\n",
      "Epoch 99/100\n",
      "30/30 [==============================] - 2s 58ms/step - loss: 1.0325 - accuracy: 0.4831\n",
      "Epoch 100/100\n",
      "30/30 [==============================] - 2s 69ms/step - loss: 1.0384 - accuracy: 0.4757\n"
     ]
    }
   ],
   "source": [
    "with open('conv_classifier/best_seeds.json', 'r') as f:\n",
    "    best_seeds = json.load(f)\n",
    "\n",
    "# 21 Crear y entrenar los modelos con los hiperparámetros y semillas guardados\n",
    "models = []\n",
    "for params_str, seed in best_seeds.items():\n",
    "    params = json.loads(params_str.replace(\"'\", \"\\\"\"))\n",
    "    model = KerasRegressor(build_fn=create_model, random_state=seed, **params)\n",
    "    model.fit(X, y_one_hot)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "8/8 [==============================] - 4s 33ms/step - loss: 11.3976 - accuracy: 0.4017\n",
      "Epoch 2/20\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 9.7886 - accuracy: 0.3996\n",
      "Epoch 3/20\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 8.3017 - accuracy: 0.4440\n",
      "Epoch 4/20\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 7.0857 - accuracy: 0.4408\n",
      "Epoch 5/20\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 6.0468 - accuracy: 0.4397\n",
      "Epoch 6/20\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 5.1923 - accuracy: 0.4524\n",
      "Epoch 7/20\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.4225 - accuracy: 0.4683\n",
      "Epoch 8/20\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 3.8545 - accuracy: 0.4535\n",
      "Epoch 9/20\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.3517 - accuracy: 0.4556\n",
      "Epoch 10/20\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 2.9717 - accuracy: 0.4323\n",
      "Epoch 11/20\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 2.6048 - accuracy: 0.4704\n",
      "Epoch 12/20\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 2.3421 - accuracy: 0.4651\n",
      "Epoch 13/20\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 2.1396 - accuracy: 0.4831\n",
      "Epoch 14/20\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 1.9714 - accuracy: 0.4493\n",
      "Epoch 15/20\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 1.8215 - accuracy: 0.4873\n",
      "Epoch 16/20\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 1.7092 - accuracy: 0.4651\n",
      "Epoch 17/20\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 1.5581 - accuracy: 0.4884\n",
      "Epoch 18/20\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.4863 - accuracy: 0.4757\n",
      "Epoch 19/20\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.4133 - accuracy: 0.5148\n",
      "Epoch 20/20\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.3820 - accuracy: 0.4873\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 5s 62ms/step - loss: 2.0621 - accuracy: 0.3985\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 1.6804 - accuracy: 0.4175\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 1.5190 - accuracy: 0.4471\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 1.3945 - accuracy: 0.4471\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 1.3483 - accuracy: 0.4302\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 1.2581 - accuracy: 0.4471\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 65ms/step - loss: 1.1777 - accuracy: 0.4672\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 1.1467 - accuracy: 0.4799\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 1.1441 - accuracy: 0.4440\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 1.1025 - accuracy: 0.4789\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 1.1062 - accuracy: 0.4831\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 1.0758 - accuracy: 0.4778\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 1.0811 - accuracy: 0.4778\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 1.0713 - accuracy: 0.4715\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 1.0609 - accuracy: 0.4884\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 1.0538 - accuracy: 0.4767\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 1.0556 - accuracy: 0.4598\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 1.0377 - accuracy: 0.5000\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 66ms/step - loss: 1.0291 - accuracy: 0.4947\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 1.0303 - accuracy: 0.4736\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 1.0404 - accuracy: 0.4630\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 1.0345 - accuracy: 0.4968\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 1.0213 - accuracy: 0.4725\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 1.0240 - accuracy: 0.4598\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 1.0098 - accuracy: 0.4841\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 1.0354 - accuracy: 0.4852\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 1.0224 - accuracy: 0.4894\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 1.0255 - accuracy: 0.4767\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 1.0443 - accuracy: 0.4588\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 1.0396 - accuracy: 0.4598\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 1.0287 - accuracy: 0.4958\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 1.0264 - accuracy: 0.4884\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 1.0228 - accuracy: 0.4820\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 1.0348 - accuracy: 0.4915\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 1.0298 - accuracy: 0.4619\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 1.0192 - accuracy: 0.4841\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 1.0143 - accuracy: 0.4789\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 1.0149 - accuracy: 0.4937\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 1.0151 - accuracy: 0.4789\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 1.0440 - accuracy: 0.4799\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 1.0242 - accuracy: 0.4651\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 1.0075 - accuracy: 0.4884\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 1.0097 - accuracy: 0.4947\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 1.0169 - accuracy: 0.4810\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 1.0233 - accuracy: 0.4725\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 1.0241 - accuracy: 0.4947\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 1.0188 - accuracy: 0.4683\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 1.0059 - accuracy: 0.5000\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 1.0055 - accuracy: 0.4958\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 1.0249 - accuracy: 0.4778\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 1.0076 - accuracy: 0.5032\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 1.0090 - accuracy: 0.4894\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 1.0106 - accuracy: 0.5085\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 1.0034 - accuracy: 0.4926\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 1.0119 - accuracy: 0.5053\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 1.0085 - accuracy: 0.5085\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 1.0015 - accuracy: 0.4968\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.9915 - accuracy: 0.4958\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 1.0106 - accuracy: 0.4820\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 1.0103 - accuracy: 0.4979\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 1.0029 - accuracy: 0.5137\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 1.0135 - accuracy: 0.4968\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 1.0122 - accuracy: 0.4905\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 1.0008 - accuracy: 0.5000\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.9914 - accuracy: 0.5021\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 1.0064 - accuracy: 0.4852\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 0.9980 - accuracy: 0.5211\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 1.0010 - accuracy: 0.4905\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 1.0140 - accuracy: 0.4831\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 1.0047 - accuracy: 0.5085\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 1.0065 - accuracy: 0.5000\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 1.0009 - accuracy: 0.5116\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.9997 - accuracy: 0.5159\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 1.0089 - accuracy: 0.4799\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 1.0140 - accuracy: 0.5063\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 0.9976 - accuracy: 0.4926\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.9952 - accuracy: 0.5000\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 1.0083 - accuracy: 0.5074\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 1.0100 - accuracy: 0.4820\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 1.0038 - accuracy: 0.5021\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 0.9965 - accuracy: 0.4905\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 0.9899 - accuracy: 0.5317\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 0.9923 - accuracy: 0.4937\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 0.9950 - accuracy: 0.4937\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.9814 - accuracy: 0.5116\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.9840 - accuracy: 0.5021\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 0.9942 - accuracy: 0.5063\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 1.0115 - accuracy: 0.4979\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 1.0163 - accuracy: 0.4915\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 1.0119 - accuracy: 0.4873\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 0.9909 - accuracy: 0.5148\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 1.0085 - accuracy: 0.4884\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 1.0031 - accuracy: 0.5106\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.9972 - accuracy: 0.4915\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 0.9905 - accuracy: 0.5159\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.9744 - accuracy: 0.5169\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.9821 - accuracy: 0.5127\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.9859 - accuracy: 0.5159\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.9823 - accuracy: 0.5053\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.9828 - accuracy: 0.5180\n",
      "Epoch 1/10\n",
      "15/15 [==============================] - 5s 77ms/step - loss: 42.8091 - accuracy: 0.4281\n",
      "Epoch 2/10\n",
      "15/15 [==============================] - 1s 62ms/step - loss: 40.4128 - accuracy: 0.4376\n",
      "Epoch 3/10\n",
      "15/15 [==============================] - 1s 65ms/step - loss: 38.1178 - accuracy: 0.4937\n",
      "Epoch 4/10\n",
      "15/15 [==============================] - 1s 78ms/step - loss: 35.9858 - accuracy: 0.4905\n",
      "Epoch 5/10\n",
      "15/15 [==============================] - 1s 72ms/step - loss: 33.9603 - accuracy: 0.4789\n",
      "Epoch 6/10\n",
      "15/15 [==============================] - 1s 92ms/step - loss: 32.0212 - accuracy: 0.5042\n",
      "Epoch 7/10\n",
      "15/15 [==============================] - 1s 81ms/step - loss: 30.2478 - accuracy: 0.4863\n",
      "Epoch 8/10\n",
      "15/15 [==============================] - 1s 62ms/step - loss: 28.4941 - accuracy: 0.5254\n",
      "Epoch 9/10\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 26.8619 - accuracy: 0.5476\n",
      "Epoch 10/10\n",
      "15/15 [==============================] - 1s 71ms/step - loss: 25.3420 - accuracy: 0.5180\n",
      "Epoch 1/100\n",
      "30/30 [==============================] - 5s 18ms/step - loss: 1.3181 - accuracy: 0.3753\n",
      "Epoch 2/100\n",
      "30/30 [==============================] - 0s 17ms/step - loss: 1.2105 - accuracy: 0.4281\n",
      "Epoch 3/100\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 1.2178 - accuracy: 0.4038\n",
      "Epoch 4/100\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 1.1733 - accuracy: 0.4271\n",
      "Epoch 5/100\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 1.2018 - accuracy: 0.4101\n",
      "Epoch 6/100\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.1558 - accuracy: 0.4651\n",
      "Epoch 7/100\n",
      "30/30 [==============================] - 0s 17ms/step - loss: 1.1778 - accuracy: 0.4323\n",
      "Epoch 8/100\n",
      "30/30 [==============================] - 1s 21ms/step - loss: 1.1687 - accuracy: 0.4366\n",
      "Epoch 9/100\n",
      "30/30 [==============================] - 1s 21ms/step - loss: 1.1705 - accuracy: 0.4070\n",
      "Epoch 10/100\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 1.1464 - accuracy: 0.4440\n",
      "Epoch 11/100\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 1.1347 - accuracy: 0.4397\n",
      "Epoch 12/100\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 1.1401 - accuracy: 0.4292\n",
      "Epoch 13/100\n",
      "30/30 [==============================] - 1s 20ms/step - loss: 1.1330 - accuracy: 0.4334\n",
      "Epoch 14/100\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 1.1366 - accuracy: 0.4154\n",
      "Epoch 15/100\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 1.1223 - accuracy: 0.4461\n",
      "Epoch 16/100\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.1120 - accuracy: 0.4376\n",
      "Epoch 17/100\n",
      "30/30 [==============================] - 1s 20ms/step - loss: 1.1156 - accuracy: 0.4545\n",
      "Epoch 18/100\n",
      "30/30 [==============================] - 1s 22ms/step - loss: 1.1052 - accuracy: 0.4577\n",
      "Epoch 19/100\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 1.1086 - accuracy: 0.4461\n",
      "Epoch 20/100\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 1.1003 - accuracy: 0.4577\n",
      "Epoch 21/100\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.0946 - accuracy: 0.4503\n",
      "Epoch 22/100\n",
      "30/30 [==============================] - 1s 21ms/step - loss: 1.0983 - accuracy: 0.4577\n",
      "Epoch 23/100\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.1065 - accuracy: 0.4313\n",
      "Epoch 24/100\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 1.1067 - accuracy: 0.4228\n",
      "Epoch 25/100\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 1.0909 - accuracy: 0.4503\n",
      "Epoch 26/100\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 1.0950 - accuracy: 0.4429\n",
      "Epoch 27/100\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 1.0765 - accuracy: 0.4577\n",
      "Epoch 28/100\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 1.0877 - accuracy: 0.4429\n",
      "Epoch 29/100\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 1.0708 - accuracy: 0.4588\n",
      "Epoch 30/100\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.0733 - accuracy: 0.4545\n",
      "Epoch 31/100\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 1.0706 - accuracy: 0.4630\n",
      "Epoch 32/100\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 1.0768 - accuracy: 0.4376\n",
      "Epoch 33/100\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 1.0749 - accuracy: 0.4609\n",
      "Epoch 34/100\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 1.0899 - accuracy: 0.4419\n",
      "Epoch 35/100\n",
      "30/30 [==============================] - 0s 17ms/step - loss: 1.0824 - accuracy: 0.4471\n",
      "Epoch 36/100\n",
      "30/30 [==============================] - 1s 22ms/step - loss: 1.0875 - accuracy: 0.4080\n",
      "Epoch 37/100\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 1.0823 - accuracy: 0.4345\n",
      "Epoch 38/100\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 1.0757 - accuracy: 0.4503\n",
      "Epoch 39/100\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 1.0730 - accuracy: 0.4736\n",
      "Epoch 40/100\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 1.0703 - accuracy: 0.4471\n",
      "Epoch 41/100\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 1.0815 - accuracy: 0.4419\n",
      "Epoch 42/100\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 1.0808 - accuracy: 0.4408\n",
      "Epoch 43/100\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 1.0667 - accuracy: 0.4514\n",
      "Epoch 44/100\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 1.0764 - accuracy: 0.4419\n",
      "Epoch 45/100\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 1.0618 - accuracy: 0.4704\n",
      "Epoch 46/100\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 1.0551 - accuracy: 0.4493\n",
      "Epoch 47/100\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 1.0589 - accuracy: 0.4630\n",
      "Epoch 48/100\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 1.0575 - accuracy: 0.4440\n",
      "Epoch 49/100\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 1.0679 - accuracy: 0.4366\n",
      "Epoch 50/100\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 1.0605 - accuracy: 0.4662\n",
      "Epoch 51/100\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 1.0575 - accuracy: 0.4556\n",
      "Epoch 52/100\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 1.0542 - accuracy: 0.4715\n",
      "Epoch 53/100\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.0609 - accuracy: 0.4408\n",
      "Epoch 54/100\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.0574 - accuracy: 0.4471\n",
      "Epoch 55/100\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 1.0536 - accuracy: 0.4408\n",
      "Epoch 56/100\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 1.0501 - accuracy: 0.4461\n",
      "Epoch 57/100\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 1.0500 - accuracy: 0.4535\n",
      "Epoch 58/100\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 1.0455 - accuracy: 0.4641\n",
      "Epoch 59/100\n",
      "30/30 [==============================] - 0s 17ms/step - loss: 1.0455 - accuracy: 0.4789\n",
      "Epoch 60/100\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.0414 - accuracy: 0.4567\n",
      "Epoch 61/100\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 1.0411 - accuracy: 0.4736\n",
      "Epoch 62/100\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 1.0407 - accuracy: 0.4630\n",
      "Epoch 63/100\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 1.0427 - accuracy: 0.4672\n",
      "Epoch 64/100\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 1.0392 - accuracy: 0.4630\n",
      "Epoch 65/100\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 1.0477 - accuracy: 0.4704\n",
      "Epoch 66/100\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 1.0412 - accuracy: 0.4577\n",
      "Epoch 67/100\n",
      "30/30 [==============================] - 0s 17ms/step - loss: 1.0387 - accuracy: 0.4577\n",
      "Epoch 68/100\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 1.0368 - accuracy: 0.4683\n",
      "Epoch 69/100\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.0497 - accuracy: 0.4503\n",
      "Epoch 70/100\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 1.0565 - accuracy: 0.4387\n",
      "Epoch 71/100\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 1.0386 - accuracy: 0.4683\n",
      "Epoch 72/100\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 1.0382 - accuracy: 0.4704\n",
      "Epoch 73/100\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.0370 - accuracy: 0.4693\n",
      "Epoch 74/100\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.0514 - accuracy: 0.4651\n",
      "Epoch 75/100\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.0397 - accuracy: 0.4619\n",
      "Epoch 76/100\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 1.0388 - accuracy: 0.4672\n",
      "Epoch 77/100\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 1.0399 - accuracy: 0.4683\n",
      "Epoch 78/100\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 1.0373 - accuracy: 0.4704\n",
      "Epoch 79/100\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 1.0320 - accuracy: 0.4693\n",
      "Epoch 80/100\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 1.0310 - accuracy: 0.4577\n",
      "Epoch 81/100\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 1.0359 - accuracy: 0.4683\n",
      "Epoch 82/100\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 1.0375 - accuracy: 0.4672\n",
      "Epoch 83/100\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 1.0226 - accuracy: 0.4884\n",
      "Epoch 84/100\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 1.0437 - accuracy: 0.4556\n",
      "Epoch 85/100\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 1.0570 - accuracy: 0.4461\n",
      "Epoch 86/100\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.0507 - accuracy: 0.4471\n",
      "Epoch 87/100\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 1.0444 - accuracy: 0.4440\n",
      "Epoch 88/100\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 1.0376 - accuracy: 0.4598\n",
      "Epoch 89/100\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.0371 - accuracy: 0.4514\n",
      "Epoch 90/100\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 1.0315 - accuracy: 0.4789\n",
      "Epoch 91/100\n",
      "30/30 [==============================] - 1s 21ms/step - loss: 1.0441 - accuracy: 0.4757\n",
      "Epoch 92/100\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.0448 - accuracy: 0.4387\n",
      "Epoch 93/100\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 1.0363 - accuracy: 0.4630\n",
      "Epoch 94/100\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.0480 - accuracy: 0.4482\n",
      "Epoch 95/100\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 1.0505 - accuracy: 0.4355\n",
      "Epoch 96/100\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.0422 - accuracy: 0.4524\n",
      "Epoch 97/100\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.0442 - accuracy: 0.4630\n",
      "Epoch 98/100\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 1.0360 - accuracy: 0.4715\n",
      "Epoch 99/100\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 1.0497 - accuracy: 0.4577\n",
      "Epoch 100/100\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 1.0421 - accuracy: 0.4609\n",
      "Epoch 1/100\n",
      "30/30 [==============================] - 6s 43ms/step - loss: 1.8026 - accuracy: 0.3584\n",
      "Epoch 2/100\n",
      "30/30 [==============================] - 1s 45ms/step - loss: 1.5578 - accuracy: 0.4260\n",
      "Epoch 3/100\n",
      "30/30 [==============================] - 1s 38ms/step - loss: 1.4857 - accuracy: 0.3816\n",
      "Epoch 4/100\n",
      "30/30 [==============================] - 1s 41ms/step - loss: 1.3214 - accuracy: 0.4249\n",
      "Epoch 5/100\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 1.3290 - accuracy: 0.4387\n",
      "Epoch 6/100\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 1.3083 - accuracy: 0.4218\n",
      "Epoch 7/100\n",
      "30/30 [==============================] - 1s 50ms/step - loss: 1.2216 - accuracy: 0.4249\n",
      "Epoch 8/100\n",
      "30/30 [==============================] - 1s 46ms/step - loss: 1.1860 - accuracy: 0.4397\n",
      "Epoch 9/100\n",
      "30/30 [==============================] - 1s 44ms/step - loss: 1.1948 - accuracy: 0.4313\n",
      "Epoch 10/100\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 1.1775 - accuracy: 0.4292\n",
      "Epoch 11/100\n",
      "30/30 [==============================] - 1s 46ms/step - loss: 1.1680 - accuracy: 0.4197\n",
      "Epoch 12/100\n",
      "30/30 [==============================] - 1s 38ms/step - loss: 1.1622 - accuracy: 0.4133\n",
      "Epoch 13/100\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 1.1591 - accuracy: 0.4366\n",
      "Epoch 14/100\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 1.1739 - accuracy: 0.4175\n",
      "Epoch 15/100\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 1.1548 - accuracy: 0.4175\n",
      "Epoch 16/100\n",
      "30/30 [==============================] - 1s 46ms/step - loss: 1.1362 - accuracy: 0.4228\n",
      "Epoch 17/100\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 1.1453 - accuracy: 0.4186\n",
      "Epoch 18/100\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 1.1281 - accuracy: 0.4355\n",
      "Epoch 19/100\n",
      "30/30 [==============================] - 1s 43ms/step - loss: 1.1171 - accuracy: 0.4419\n",
      "Epoch 20/100\n",
      "30/30 [==============================] - 1s 38ms/step - loss: 1.1049 - accuracy: 0.4545\n",
      "Epoch 21/100\n",
      "30/30 [==============================] - 1s 43ms/step - loss: 1.1094 - accuracy: 0.4387\n",
      "Epoch 22/100\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 1.0928 - accuracy: 0.4651\n",
      "Epoch 23/100\n",
      "30/30 [==============================] - 1s 37ms/step - loss: 1.0971 - accuracy: 0.4482\n",
      "Epoch 24/100\n",
      "30/30 [==============================] - 1s 37ms/step - loss: 1.0973 - accuracy: 0.4482\n",
      "Epoch 25/100\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 1.0963 - accuracy: 0.4292\n",
      "Epoch 26/100\n",
      "30/30 [==============================] - 1s 45ms/step - loss: 1.0897 - accuracy: 0.4302\n",
      "Epoch 27/100\n",
      "30/30 [==============================] - 1s 37ms/step - loss: 1.0717 - accuracy: 0.4672\n",
      "Epoch 28/100\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 1.0987 - accuracy: 0.4503\n",
      "Epoch 29/100\n",
      "30/30 [==============================] - 1s 38ms/step - loss: 1.0857 - accuracy: 0.4535\n",
      "Epoch 30/100\n",
      "30/30 [==============================] - 1s 41ms/step - loss: 1.0664 - accuracy: 0.4641\n",
      "Epoch 31/100\n",
      "30/30 [==============================] - 1s 44ms/step - loss: 1.0732 - accuracy: 0.4619\n",
      "Epoch 32/100\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 1.0761 - accuracy: 0.4397\n",
      "Epoch 33/100\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 1.0723 - accuracy: 0.4461\n",
      "Epoch 34/100\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 1.0691 - accuracy: 0.4693\n",
      "Epoch 35/100\n",
      "30/30 [==============================] - 1s 38ms/step - loss: 1.0610 - accuracy: 0.4767\n",
      "Epoch 36/100\n",
      "30/30 [==============================] - 1s 46ms/step - loss: 1.0667 - accuracy: 0.4778\n",
      "Epoch 37/100\n",
      "30/30 [==============================] - 1s 37ms/step - loss: 1.0701 - accuracy: 0.4503\n",
      "Epoch 38/100\n",
      "30/30 [==============================] - 1s 42ms/step - loss: 1.0657 - accuracy: 0.4736\n",
      "Epoch 39/100\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 1.0648 - accuracy: 0.4440\n",
      "Epoch 40/100\n",
      "30/30 [==============================] - 1s 44ms/step - loss: 1.0618 - accuracy: 0.4662\n",
      "Epoch 41/100\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 1.0495 - accuracy: 0.4725\n",
      "Epoch 42/100\n",
      "30/30 [==============================] - 1s 41ms/step - loss: 1.0623 - accuracy: 0.4662\n",
      "Epoch 43/100\n",
      "30/30 [==============================] - 1s 43ms/step - loss: 1.0555 - accuracy: 0.4630\n",
      "Epoch 44/100\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 1.0776 - accuracy: 0.4355\n",
      "Epoch 45/100\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 1.0642 - accuracy: 0.4609\n",
      "Epoch 46/100\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 1.0710 - accuracy: 0.4313\n",
      "Epoch 47/100\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 1.0466 - accuracy: 0.4767\n",
      "Epoch 48/100\n",
      "30/30 [==============================] - 1s 38ms/step - loss: 1.0628 - accuracy: 0.4482\n",
      "Epoch 49/100\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 1.0560 - accuracy: 0.4662\n",
      "Epoch 50/100\n",
      "30/30 [==============================] - 1s 49ms/step - loss: 1.0611 - accuracy: 0.4556\n",
      "Epoch 51/100\n",
      "30/30 [==============================] - 1s 38ms/step - loss: 1.0464 - accuracy: 0.4672\n",
      "Epoch 52/100\n",
      "30/30 [==============================] - 1s 38ms/step - loss: 1.0685 - accuracy: 0.4376\n",
      "Epoch 53/100\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 1.0617 - accuracy: 0.4482\n",
      "Epoch 54/100\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 1.0547 - accuracy: 0.4672\n",
      "Epoch 55/100\n",
      "30/30 [==============================] - 1s 41ms/step - loss: 1.0548 - accuracy: 0.4789\n",
      "Epoch 56/100\n",
      "30/30 [==============================] - 1s 38ms/step - loss: 1.0671 - accuracy: 0.4556\n",
      "Epoch 57/100\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 1.0573 - accuracy: 0.4577\n",
      "Epoch 58/100\n",
      "30/30 [==============================] - 1s 43ms/step - loss: 1.0551 - accuracy: 0.4598\n",
      "Epoch 59/100\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 1.0731 - accuracy: 0.4619\n",
      "Epoch 60/100\n",
      "30/30 [==============================] - 1s 43ms/step - loss: 1.0528 - accuracy: 0.4662\n",
      "Epoch 61/100\n",
      "30/30 [==============================] - 2s 51ms/step - loss: 1.0550 - accuracy: 0.4799\n",
      "Epoch 62/100\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 1.0420 - accuracy: 0.4810\n",
      "Epoch 63/100\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 1.0316 - accuracy: 0.4937\n",
      "Epoch 64/100\n",
      "30/30 [==============================] - 1s 37ms/step - loss: 1.0518 - accuracy: 0.4651\n",
      "Epoch 65/100\n",
      "30/30 [==============================] - 1s 37ms/step - loss: 1.0615 - accuracy: 0.4514\n",
      "Epoch 66/100\n",
      "30/30 [==============================] - 1s 38ms/step - loss: 1.0569 - accuracy: 0.4577\n",
      "Epoch 67/100\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 1.0387 - accuracy: 0.4704\n",
      "Epoch 68/100\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 1.0559 - accuracy: 0.4715\n",
      "Epoch 69/100\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 1.0574 - accuracy: 0.4609\n",
      "Epoch 70/100\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 1.0488 - accuracy: 0.4873\n",
      "Epoch 71/100\n",
      "30/30 [==============================] - 1s 43ms/step - loss: 1.0504 - accuracy: 0.4672\n",
      "Epoch 72/100\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 1.0563 - accuracy: 0.4630\n",
      "Epoch 73/100\n",
      "30/30 [==============================] - 1s 43ms/step - loss: 1.0658 - accuracy: 0.4651\n",
      "Epoch 74/100\n",
      "30/30 [==============================] - 1s 43ms/step - loss: 1.0542 - accuracy: 0.4715\n",
      "Epoch 75/100\n",
      "30/30 [==============================] - 1s 38ms/step - loss: 1.0594 - accuracy: 0.4641\n",
      "Epoch 76/100\n",
      "30/30 [==============================] - 1s 43ms/step - loss: 1.0632 - accuracy: 0.4567\n",
      "Epoch 77/100\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 1.0467 - accuracy: 0.4588\n",
      "Epoch 78/100\n",
      "30/30 [==============================] - 1s 43ms/step - loss: 1.0359 - accuracy: 0.4662\n",
      "Epoch 79/100\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 1.0457 - accuracy: 0.4693\n",
      "Epoch 80/100\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 1.0352 - accuracy: 0.4736\n",
      "Epoch 81/100\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 1.0496 - accuracy: 0.4482\n",
      "Epoch 82/100\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 1.0375 - accuracy: 0.4683\n",
      "Epoch 83/100\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 1.0503 - accuracy: 0.4651\n",
      "Epoch 84/100\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 1.0365 - accuracy: 0.4609\n",
      "Epoch 85/100\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 1.0626 - accuracy: 0.4577\n",
      "Epoch 86/100\n",
      "30/30 [==============================] - 1s 30ms/step - loss: 1.0434 - accuracy: 0.4820\n",
      "Epoch 87/100\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 1.0456 - accuracy: 0.4651\n",
      "Epoch 88/100\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 1.0433 - accuracy: 0.4577\n",
      "Epoch 89/100\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 1.0348 - accuracy: 0.4725\n",
      "Epoch 90/100\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 1.0425 - accuracy: 0.4841\n",
      "Epoch 91/100\n",
      "30/30 [==============================] - 1s 43ms/step - loss: 1.0408 - accuracy: 0.4725\n",
      "Epoch 92/100\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 1.0293 - accuracy: 0.4915\n",
      "Epoch 93/100\n",
      "30/30 [==============================] - 1s 47ms/step - loss: 1.0257 - accuracy: 0.4958\n",
      "Epoch 94/100\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 1.0277 - accuracy: 0.4989\n",
      "Epoch 95/100\n",
      "30/30 [==============================] - 1s 41ms/step - loss: 1.0388 - accuracy: 0.4715\n",
      "Epoch 96/100\n",
      "30/30 [==============================] - 1s 38ms/step - loss: 1.0354 - accuracy: 0.4778\n",
      "Epoch 97/100\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 1.0340 - accuracy: 0.4567\n",
      "Epoch 98/100\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 1.0360 - accuracy: 0.4609\n",
      "Epoch 99/100\n",
      "30/30 [==============================] - 1s 42ms/step - loss: 1.0325 - accuracy: 0.4831\n",
      "Epoch 100/100\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 1.0384 - accuracy: 0.4757\n"
     ]
    }
   ],
   "source": [
    "ensemble = MultivariableVotingRegressor(models)\n",
    "ensemble.fit(X, y_one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Datos originales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open_time</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Number of trades</th>\n",
       "      <th>Close_BTCUSDT</th>\n",
       "      <th>Volume_BTCUSDT</th>\n",
       "      <th>Number_of_trades_BTCUSDT</th>\n",
       "      <th>Close_ETHUSDT</th>\n",
       "      <th>Volume_ETHUSDT</th>\n",
       "      <th>Number_of_trades_ETHUSDT</th>\n",
       "      <th>Close_BNBUSDT</th>\n",
       "      <th>Volume_BNBUSDT</th>\n",
       "      <th>Number_of_trades_BNBUSDT</th>\n",
       "      <th>SMA_20</th>\n",
       "      <th>EMA_20</th>\n",
       "      <th>Upper_Band</th>\n",
       "      <th>Middle_Band</th>\n",
       "      <th>Lower_Band</th>\n",
       "      <th>RSI</th>\n",
       "      <th>buy_1000x_high_coinbase</th>\n",
       "      <th>sell_1000x_high_coinbase</th>\n",
       "      <th>total_trades_coinbase</th>\n",
       "      <th>Tweets_Utilizados</th>\n",
       "      <th>Tweets_Utilizados_coin</th>\n",
       "      <th>Tweets_Utilizados_referentes</th>\n",
       "      <th>Tweets_Utilizados_whale_alert</th>\n",
       "      <th>Buy_1000x_high</th>\n",
       "      <th>sell_1000x_high</th>\n",
       "      <th>total_trades_binance</th>\n",
       "      <th>Tendencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>946</th>\n",
       "      <td>2024-04-30</td>\n",
       "      <td>6.59</td>\n",
       "      <td>6.67</td>\n",
       "      <td>6.04</td>\n",
       "      <td>115512.00</td>\n",
       "      <td>60672.00</td>\n",
       "      <td>54947.66</td>\n",
       "      <td>1985671.00</td>\n",
       "      <td>3014.05</td>\n",
       "      <td>561717.49</td>\n",
       "      <td>1292873.00</td>\n",
       "      <td>578.40</td>\n",
       "      <td>766513.45</td>\n",
       "      <td>486465.00</td>\n",
       "      <td>6.93</td>\n",
       "      <td>7.13</td>\n",
       "      <td>7.80</td>\n",
       "      <td>6.93</td>\n",
       "      <td>6.05</td>\n",
       "      <td>34.18</td>\n",
       "      <td>51.00</td>\n",
       "      <td>55.00</td>\n",
       "      <td>48709.00</td>\n",
       "      <td>142</td>\n",
       "      <td>187</td>\n",
       "      <td>1.00</td>\n",
       "      <td>23.00</td>\n",
       "      <td>379.00</td>\n",
       "      <td>377.00</td>\n",
       "      <td>70000.00</td>\n",
       "      <td>Bajista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>947</th>\n",
       "      <td>2024-05-01</td>\n",
       "      <td>6.42</td>\n",
       "      <td>6.93</td>\n",
       "      <td>6.13</td>\n",
       "      <td>175570.00</td>\n",
       "      <td>58364.97</td>\n",
       "      <td>81166.47</td>\n",
       "      <td>2401089.00</td>\n",
       "      <td>2972.46</td>\n",
       "      <td>624963.78</td>\n",
       "      <td>1365039.00</td>\n",
       "      <td>561.80</td>\n",
       "      <td>669027.32</td>\n",
       "      <td>427425.00</td>\n",
       "      <td>6.85</td>\n",
       "      <td>7.11</td>\n",
       "      <td>7.41</td>\n",
       "      <td>6.85</td>\n",
       "      <td>6.29</td>\n",
       "      <td>43.30</td>\n",
       "      <td>42.00</td>\n",
       "      <td>50.00</td>\n",
       "      <td>83718.00</td>\n",
       "      <td>130</td>\n",
       "      <td>177</td>\n",
       "      <td>0.00</td>\n",
       "      <td>36.00</td>\n",
       "      <td>327.00</td>\n",
       "      <td>340.00</td>\n",
       "      <td>107000.00</td>\n",
       "      <td>Alcista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948</th>\n",
       "      <td>2024-05-02</td>\n",
       "      <td>6.90</td>\n",
       "      <td>7.41</td>\n",
       "      <td>6.69</td>\n",
       "      <td>109002.00</td>\n",
       "      <td>59060.61</td>\n",
       "      <td>47583.82</td>\n",
       "      <td>1572898.00</td>\n",
       "      <td>2986.19</td>\n",
       "      <td>365939.72</td>\n",
       "      <td>880167.00</td>\n",
       "      <td>560.50</td>\n",
       "      <td>359794.32</td>\n",
       "      <td>250921.00</td>\n",
       "      <td>6.85</td>\n",
       "      <td>7.12</td>\n",
       "      <td>7.42</td>\n",
       "      <td>6.85</td>\n",
       "      <td>6.29</td>\n",
       "      <td>49.27</td>\n",
       "      <td>87.00</td>\n",
       "      <td>57.00</td>\n",
       "      <td>61208.00</td>\n",
       "      <td>461</td>\n",
       "      <td>374</td>\n",
       "      <td>1.00</td>\n",
       "      <td>25.00</td>\n",
       "      <td>270.00</td>\n",
       "      <td>282.00</td>\n",
       "      <td>71000.00</td>\n",
       "      <td>Alcista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949</th>\n",
       "      <td>2024-05-03</td>\n",
       "      <td>7.27</td>\n",
       "      <td>7.39</td>\n",
       "      <td>7.00</td>\n",
       "      <td>109634.00</td>\n",
       "      <td>62882.01</td>\n",
       "      <td>43628.40</td>\n",
       "      <td>1558661.00</td>\n",
       "      <td>3102.61</td>\n",
       "      <td>355825.84</td>\n",
       "      <td>859542.00</td>\n",
       "      <td>587.00</td>\n",
       "      <td>342906.43</td>\n",
       "      <td>257575.00</td>\n",
       "      <td>6.90</td>\n",
       "      <td>7.14</td>\n",
       "      <td>7.44</td>\n",
       "      <td>6.90</td>\n",
       "      <td>6.35</td>\n",
       "      <td>48.86</td>\n",
       "      <td>52.00</td>\n",
       "      <td>40.00</td>\n",
       "      <td>46255.00</td>\n",
       "      <td>573</td>\n",
       "      <td>474</td>\n",
       "      <td>1.00</td>\n",
       "      <td>22.00</td>\n",
       "      <td>386.00</td>\n",
       "      <td>635.00</td>\n",
       "      <td>69000.00</td>\n",
       "      <td>Lateral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>2024-05-04</td>\n",
       "      <td>7.24</td>\n",
       "      <td>7.28</td>\n",
       "      <td>7.11</td>\n",
       "      <td>71120.00</td>\n",
       "      <td>63892.04</td>\n",
       "      <td>24368.69</td>\n",
       "      <td>1113509.00</td>\n",
       "      <td>3117.23</td>\n",
       "      <td>196263.95</td>\n",
       "      <td>575026.00</td>\n",
       "      <td>585.70</td>\n",
       "      <td>197129.25</td>\n",
       "      <td>210303.00</td>\n",
       "      <td>6.91</td>\n",
       "      <td>7.13</td>\n",
       "      <td>7.46</td>\n",
       "      <td>6.91</td>\n",
       "      <td>6.36</td>\n",
       "      <td>46.98</td>\n",
       "      <td>68.00</td>\n",
       "      <td>50.00</td>\n",
       "      <td>34251.00</td>\n",
       "      <td>407</td>\n",
       "      <td>472</td>\n",
       "      <td>0.00</td>\n",
       "      <td>14.00</td>\n",
       "      <td>203.00</td>\n",
       "      <td>232.00</td>\n",
       "      <td>49000.00</td>\n",
       "      <td>Bajista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951</th>\n",
       "      <td>2024-05-05</td>\n",
       "      <td>7.12</td>\n",
       "      <td>7.40</td>\n",
       "      <td>7.01</td>\n",
       "      <td>72928.00</td>\n",
       "      <td>64012.00</td>\n",
       "      <td>18526.75</td>\n",
       "      <td>992921.00</td>\n",
       "      <td>3136.41</td>\n",
       "      <td>218760.27</td>\n",
       "      <td>600693.00</td>\n",
       "      <td>592.00</td>\n",
       "      <td>180458.24</td>\n",
       "      <td>180794.00</td>\n",
       "      <td>6.94</td>\n",
       "      <td>7.15</td>\n",
       "      <td>7.51</td>\n",
       "      <td>6.94</td>\n",
       "      <td>6.37</td>\n",
       "      <td>50.00</td>\n",
       "      <td>37.00</td>\n",
       "      <td>52.00</td>\n",
       "      <td>29197.00</td>\n",
       "      <td>417</td>\n",
       "      <td>499</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>320.00</td>\n",
       "      <td>284.00</td>\n",
       "      <td>47000.00</td>\n",
       "      <td>Alcista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>2024-05-06</td>\n",
       "      <td>7.30</td>\n",
       "      <td>7.47</td>\n",
       "      <td>7.06</td>\n",
       "      <td>94264.00</td>\n",
       "      <td>63165.19</td>\n",
       "      <td>34674.92</td>\n",
       "      <td>1392557.00</td>\n",
       "      <td>3062.60</td>\n",
       "      <td>355135.30</td>\n",
       "      <td>873200.00</td>\n",
       "      <td>588.20</td>\n",
       "      <td>278669.01</td>\n",
       "      <td>248490.00</td>\n",
       "      <td>6.96</td>\n",
       "      <td>7.15</td>\n",
       "      <td>7.53</td>\n",
       "      <td>6.96</td>\n",
       "      <td>6.39</td>\n",
       "      <td>47.10</td>\n",
       "      <td>49.00</td>\n",
       "      <td>71.00</td>\n",
       "      <td>40027.00</td>\n",
       "      <td>482</td>\n",
       "      <td>531</td>\n",
       "      <td>0.00</td>\n",
       "      <td>25.00</td>\n",
       "      <td>339.00</td>\n",
       "      <td>249.00</td>\n",
       "      <td>59000.00</td>\n",
       "      <td>Bajista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>2024-05-07</td>\n",
       "      <td>7.12</td>\n",
       "      <td>7.29</td>\n",
       "      <td>6.98</td>\n",
       "      <td>64947.00</td>\n",
       "      <td>62312.08</td>\n",
       "      <td>25598.79</td>\n",
       "      <td>1272898.00</td>\n",
       "      <td>3005.69</td>\n",
       "      <td>298796.68</td>\n",
       "      <td>815246.00</td>\n",
       "      <td>576.50</td>\n",
       "      <td>289488.71</td>\n",
       "      <td>266127.00</td>\n",
       "      <td>6.98</td>\n",
       "      <td>7.13</td>\n",
       "      <td>7.52</td>\n",
       "      <td>6.98</td>\n",
       "      <td>6.44</td>\n",
       "      <td>45.10</td>\n",
       "      <td>21.00</td>\n",
       "      <td>25.00</td>\n",
       "      <td>31028.00</td>\n",
       "      <td>495</td>\n",
       "      <td>494</td>\n",
       "      <td>0.00</td>\n",
       "      <td>28.00</td>\n",
       "      <td>296.00</td>\n",
       "      <td>205.00</td>\n",
       "      <td>42000.00</td>\n",
       "      <td>Bajista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>2024-05-08</td>\n",
       "      <td>6.99</td>\n",
       "      <td>7.19</td>\n",
       "      <td>6.89</td>\n",
       "      <td>75550.00</td>\n",
       "      <td>61193.03</td>\n",
       "      <td>26121.19</td>\n",
       "      <td>1415152.00</td>\n",
       "      <td>2974.21</td>\n",
       "      <td>266934.81</td>\n",
       "      <td>830635.00</td>\n",
       "      <td>588.60</td>\n",
       "      <td>297016.62</td>\n",
       "      <td>249379.00</td>\n",
       "      <td>6.99</td>\n",
       "      <td>7.12</td>\n",
       "      <td>7.52</td>\n",
       "      <td>6.99</td>\n",
       "      <td>6.46</td>\n",
       "      <td>44.94</td>\n",
       "      <td>17.00</td>\n",
       "      <td>24.00</td>\n",
       "      <td>32040.00</td>\n",
       "      <td>426</td>\n",
       "      <td>494</td>\n",
       "      <td>0.00</td>\n",
       "      <td>24.00</td>\n",
       "      <td>230.00</td>\n",
       "      <td>177.00</td>\n",
       "      <td>49000.00</td>\n",
       "      <td>Lateral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955</th>\n",
       "      <td>2024-05-09</td>\n",
       "      <td>6.98</td>\n",
       "      <td>7.09</td>\n",
       "      <td>6.78</td>\n",
       "      <td>75016.00</td>\n",
       "      <td>63074.01</td>\n",
       "      <td>30660.81</td>\n",
       "      <td>1381957.00</td>\n",
       "      <td>3036.23</td>\n",
       "      <td>238561.75</td>\n",
       "      <td>686147.00</td>\n",
       "      <td>596.80</td>\n",
       "      <td>464857.60</td>\n",
       "      <td>332988.00</td>\n",
       "      <td>7.01</td>\n",
       "      <td>7.11</td>\n",
       "      <td>7.52</td>\n",
       "      <td>7.01</td>\n",
       "      <td>6.50</td>\n",
       "      <td>46.32</td>\n",
       "      <td>18.00</td>\n",
       "      <td>17.00</td>\n",
       "      <td>29314.00</td>\n",
       "      <td>475</td>\n",
       "      <td>464</td>\n",
       "      <td>0.00</td>\n",
       "      <td>16.00</td>\n",
       "      <td>188.00</td>\n",
       "      <td>257.00</td>\n",
       "      <td>50000.00</td>\n",
       "      <td>Lateral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Open_time  Open  High  Low  Number of trades  Close_BTCUSDT  \\\n",
       "946 2024-04-30  6.59  6.67 6.04         115512.00       60672.00   \n",
       "947 2024-05-01  6.42  6.93 6.13         175570.00       58364.97   \n",
       "948 2024-05-02  6.90  7.41 6.69         109002.00       59060.61   \n",
       "949 2024-05-03  7.27  7.39 7.00         109634.00       62882.01   \n",
       "950 2024-05-04  7.24  7.28 7.11          71120.00       63892.04   \n",
       "951 2024-05-05  7.12  7.40 7.01          72928.00       64012.00   \n",
       "952 2024-05-06  7.30  7.47 7.06          94264.00       63165.19   \n",
       "953 2024-05-07  7.12  7.29 6.98          64947.00       62312.08   \n",
       "954 2024-05-08  6.99  7.19 6.89          75550.00       61193.03   \n",
       "955 2024-05-09  6.98  7.09 6.78          75016.00       63074.01   \n",
       "\n",
       "     Volume_BTCUSDT  Number_of_trades_BTCUSDT  Close_ETHUSDT  Volume_ETHUSDT  \\\n",
       "946        54947.66                1985671.00        3014.05       561717.49   \n",
       "947        81166.47                2401089.00        2972.46       624963.78   \n",
       "948        47583.82                1572898.00        2986.19       365939.72   \n",
       "949        43628.40                1558661.00        3102.61       355825.84   \n",
       "950        24368.69                1113509.00        3117.23       196263.95   \n",
       "951        18526.75                 992921.00        3136.41       218760.27   \n",
       "952        34674.92                1392557.00        3062.60       355135.30   \n",
       "953        25598.79                1272898.00        3005.69       298796.68   \n",
       "954        26121.19                1415152.00        2974.21       266934.81   \n",
       "955        30660.81                1381957.00        3036.23       238561.75   \n",
       "\n",
       "     Number_of_trades_ETHUSDT  Close_BNBUSDT  Volume_BNBUSDT  \\\n",
       "946                1292873.00         578.40       766513.45   \n",
       "947                1365039.00         561.80       669027.32   \n",
       "948                 880167.00         560.50       359794.32   \n",
       "949                 859542.00         587.00       342906.43   \n",
       "950                 575026.00         585.70       197129.25   \n",
       "951                 600693.00         592.00       180458.24   \n",
       "952                 873200.00         588.20       278669.01   \n",
       "953                 815246.00         576.50       289488.71   \n",
       "954                 830635.00         588.60       297016.62   \n",
       "955                 686147.00         596.80       464857.60   \n",
       "\n",
       "     Number_of_trades_BNBUSDT  SMA_20  EMA_20  Upper_Band  Middle_Band  \\\n",
       "946                 486465.00    6.93    7.13        7.80         6.93   \n",
       "947                 427425.00    6.85    7.11        7.41         6.85   \n",
       "948                 250921.00    6.85    7.12        7.42         6.85   \n",
       "949                 257575.00    6.90    7.14        7.44         6.90   \n",
       "950                 210303.00    6.91    7.13        7.46         6.91   \n",
       "951                 180794.00    6.94    7.15        7.51         6.94   \n",
       "952                 248490.00    6.96    7.15        7.53         6.96   \n",
       "953                 266127.00    6.98    7.13        7.52         6.98   \n",
       "954                 249379.00    6.99    7.12        7.52         6.99   \n",
       "955                 332988.00    7.01    7.11        7.52         7.01   \n",
       "\n",
       "     Lower_Band   RSI  buy_1000x_high_coinbase  sell_1000x_high_coinbase  \\\n",
       "946        6.05 34.18                    51.00                     55.00   \n",
       "947        6.29 43.30                    42.00                     50.00   \n",
       "948        6.29 49.27                    87.00                     57.00   \n",
       "949        6.35 48.86                    52.00                     40.00   \n",
       "950        6.36 46.98                    68.00                     50.00   \n",
       "951        6.37 50.00                    37.00                     52.00   \n",
       "952        6.39 47.10                    49.00                     71.00   \n",
       "953        6.44 45.10                    21.00                     25.00   \n",
       "954        6.46 44.94                    17.00                     24.00   \n",
       "955        6.50 46.32                    18.00                     17.00   \n",
       "\n",
       "     total_trades_coinbase  Tweets_Utilizados  Tweets_Utilizados_coin  \\\n",
       "946               48709.00                142                     187   \n",
       "947               83718.00                130                     177   \n",
       "948               61208.00                461                     374   \n",
       "949               46255.00                573                     474   \n",
       "950               34251.00                407                     472   \n",
       "951               29197.00                417                     499   \n",
       "952               40027.00                482                     531   \n",
       "953               31028.00                495                     494   \n",
       "954               32040.00                426                     494   \n",
       "955               29314.00                475                     464   \n",
       "\n",
       "     Tweets_Utilizados_referentes  Tweets_Utilizados_whale_alert  \\\n",
       "946                          1.00                          23.00   \n",
       "947                          0.00                          36.00   \n",
       "948                          1.00                          25.00   \n",
       "949                          1.00                          22.00   \n",
       "950                          0.00                          14.00   \n",
       "951                          0.00                           6.00   \n",
       "952                          0.00                          25.00   \n",
       "953                          0.00                          28.00   \n",
       "954                          0.00                          24.00   \n",
       "955                          0.00                          16.00   \n",
       "\n",
       "     Buy_1000x_high  sell_1000x_high  total_trades_binance Tendencia  \n",
       "946          379.00           377.00              70000.00   Bajista  \n",
       "947          327.00           340.00             107000.00   Alcista  \n",
       "948          270.00           282.00              71000.00   Alcista  \n",
       "949          386.00           635.00              69000.00   Lateral  \n",
       "950          203.00           232.00              49000.00   Bajista  \n",
       "951          320.00           284.00              47000.00   Alcista  \n",
       "952          339.00           249.00              59000.00   Bajista  \n",
       "953          296.00           205.00              42000.00   Bajista  \n",
       "954          230.00           177.00              49000.00   Lateral  \n",
       "955          188.00           257.00              50000.00   Lateral  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(clasifier_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 389ms/step\n",
      "1/1 [==============================] - 0s 316ms/step\n",
      "1/1 [==============================] - 0s 225ms/step\n",
      "1/1 [==============================] - 0s 264ms/step\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fa6620fdaf0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 292ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 2, 2, 0, 0, 0, 0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['Alcista', 'Bajista', 'Lateral'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Bajista',\n",
       " 'Alcista',\n",
       " 'Alcista',\n",
       " 'Alcista',\n",
       " 'Lateral',\n",
       " 'Lateral',\n",
       " 'Alcista',\n",
       " 'Alcista',\n",
       " 'Alcista',\n",
       " 'Alcista']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_clases = 3 \n",
    "\n",
    "validation_predictions = ensemble.predict(clasifier_validation.drop(columns=[\"Open_time\", \"Tendencia\"]))\n",
    "predicciones_one_hot = to_categorical(validation_predictions, num_classes=n_clases)\n",
    "etiquetas_numericas = np.argmax(predicciones_one_hot, axis=1)\n",
    "categorias_clases = onehot_encoder.categories_[0]\n",
    "nombres_clases = [categorias_clases[indice] for indice in etiquetas_numericas]\n",
    "\n",
    "display(validation_predictions)\n",
    "display(categorias_clases)\n",
    "display(nombres_clases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Datos generados por auto ml con prophet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 0, 0, 1, 1, 1, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['Alcista', 'Bajista', 'Lateral'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Bajista',\n",
       " 'Bajista',\n",
       " 'Bajista',\n",
       " 'Bajista',\n",
       " 'Alcista',\n",
       " 'Alcista',\n",
       " 'Bajista',\n",
       " 'Bajista',\n",
       " 'Bajista',\n",
       " 'Bajista']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "auto_ml_prophet_df = pd.read_csv('auto_timeseries_models_prophet/predicciones.csv')\n",
    "auto_mp_prophet_predictions = ensemble.predict(auto_ml_prophet_df.drop(columns=[\"Open_time\", \"Close\"]))\n",
    "\n",
    "predicciones_one_hot = to_categorical(auto_mp_prophet_predictions, num_classes=n_clases)\n",
    "etiquetas_numericas = np.argmax(predicciones_one_hot, axis=1)\n",
    "categorias_clases = onehot_encoder.categories_[0]\n",
    "nombres_clases = [categorias_clases[indice] for indice in etiquetas_numericas]\n",
    "\n",
    "display(auto_mp_prophet_predictions)\n",
    "display(categorias_clases)\n",
    "display(nombres_clases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Datos generados por auto ml con stats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['Alcista', 'Bajista', 'Lateral'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Alcista',\n",
       " 'Alcista',\n",
       " 'Alcista',\n",
       " 'Alcista',\n",
       " 'Alcista',\n",
       " 'Alcista',\n",
       " 'Alcista',\n",
       " 'Alcista',\n",
       " 'Alcista',\n",
       " 'Alcista']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "auto_ml_stats_df = pd.read_csv('auto_timeseries_models/predicciones.csv')\n",
    "auto_mp_stats_predictions = ensemble.predict(auto_ml_stats_df.drop(columns=[\"Open_time\", \"Close\"]))\n",
    "\n",
    "predicciones_one_hot = to_categorical(auto_mp_stats_predictions, num_classes=n_clases)\n",
    "etiquetas_numericas = np.argmax(predicciones_one_hot, axis=1)\n",
    "categorias_clases = onehot_encoder.categories_[0]\n",
    "nombres_clases = [categorias_clases[indice] for indice in etiquetas_numericas]\n",
    "\n",
    "display(auto_mp_stats_predictions)\n",
    "display(categorias_clases)\n",
    "display(nombres_clases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Datos generados por auto ml con modelos clasicos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['Alcista', 'Bajista', 'Lateral'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Bajista',\n",
       " 'Bajista',\n",
       " 'Alcista',\n",
       " 'Alcista',\n",
       " 'Alcista',\n",
       " 'Alcista',\n",
       " 'Alcista',\n",
       " 'Alcista',\n",
       " 'Alcista',\n",
       " 'Alcista']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "auto_ml_df = pd.read_csv('h2o_models/predicciones.csv')\n",
    "auto_mp_predictions = ensemble.predict(auto_ml_df.drop(columns=[\"Open_time\", \"Next_Day_Target\", \"Close\"]))\n",
    "\n",
    "predicciones_one_hot = to_categorical(auto_mp_predictions, num_classes=n_clases)\n",
    "etiquetas_numericas = np.argmax(predicciones_one_hot, axis=1)\n",
    "categorias_clases = onehot_encoder.categories_[0]\n",
    "nombres_clases = [categorias_clases[indice] for indice in etiquetas_numericas]\n",
    "\n",
    "display(auto_mp_predictions)\n",
    "display(categorias_clases)\n",
    "display(nombres_clases)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
