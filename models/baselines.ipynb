{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('/Users/mmarchetta/Desktop/Tesis-2024/data-visualization/final_dataset.csv')\n",
    "dataset1 = pd.read_csv('/Users/mmarchetta/Desktop/Tesis-2024/data-cleaning/final_dataset_first_cleaning.csv')\n",
    "dataset2= pd.read_csv('/Users/mmarchetta/Desktop/Tesis-2024/data-cleaning/final_dataset_cleaned_kbest.csv')\n",
    "dataset3 = pd.read_csv('/Users/mmarchetta/Desktop/Tesis-2024/data-cleaning/final_dataset_cleaned_random_forest_feature_selector.csv')\n",
    "\n",
    "dataset.drop(['Open_time'], axis=1, inplace=True)\n",
    "dataset.drop(['Sentimiento'], axis=1, inplace=True)\n",
    "dataset.drop(['Sentimiento_coin'], axis=1, inplace=True)\n",
    "dataset.drop(['Sentimiento_referentes'], axis=1, inplace=True)\n",
    "\n",
    "dataset1.drop(['Open_time'], axis=1, inplace=True)\n",
    "dataset2.drop(['Open_time'], axis=1, inplace=True)\n",
    "dataset3.drop(['Open_time'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_logistic_regression(dataset):\n",
    "    # Dividir los datos en conjunto de entrenamiento y conjunto de prueba\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dataset.drop(columns=[\"Tendencia\"]), \n",
    "                                                        dataset[\"Tendencia\"], \n",
    "                                                        test_size=0.2, \n",
    "                                                        random_state=42)\n",
    "\n",
    "    # Entrenar un modelo final de Regresión Logística utilizando las características seleccionadas\n",
    "    final_model = LogisticRegression()\n",
    "    final_model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluar el modelo final\n",
    "    accuracy = final_model.score(X_test, y_test)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_gradient_boosting(dataset):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dataset.drop(columns=[\"Tendencia\"]), \n",
    "                                                        dataset[\"Tendencia\"], \n",
    "                                                        test_size=0.2, \n",
    "                                                        random_state=42)\n",
    "\n",
    "    # Crear el clasificador GradientBoostingClassifier\n",
    "    gb_model = GradientBoostingClassifier(\n",
    "        n_estimators=1000,  # Usar 1000 estimadores\n",
    "        learning_rate=0.1,  # Tasa de aprendizaje\n",
    "        max_depth=5,  # Profundidad máxima de cada árbol\n",
    "        min_samples_split=2,  # Número mínimo de muestras requeridas para dividir un nodo interno\n",
    "        min_samples_leaf=1,  # Número mínimo de muestras requeridas para estar en una hoja\n",
    "        subsample=0.8,  # Fracción de muestras a utilizar para ajustar los estimadores base\n",
    "        max_features='sqrt',  # Número máximo de características a considerar al dividir nodos: raíz cuadrada del número de características\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    gb_model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluar el modelo\n",
    "    accuracy = gb_model.score(X_test, y_test)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_svm(dataset):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dataset.drop(columns=[\"Tendencia\"]), \n",
    "                                                        dataset[\"Tendencia\"], \n",
    "                                                        test_size=0.2, \n",
    "                                                        random_state=42)\n",
    "\n",
    "    svc_model = SVC(\n",
    "        kernel='rbf',  # Kernel radial\n",
    "        C=10.0,  # Parámetro de regularización\n",
    "        gamma='scale',  # Coeficiente de kernel para 'rbf'\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    svc_model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluar el modelo\n",
    "    accuracy = svc_model.score(X_test, y_test)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_MLP(dataset):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dataset.drop(columns=[\"Tendencia\"]), \n",
    "                                                        dataset[\"Tendencia\"], \n",
    "                                                        test_size=0.2, \n",
    "                                                        random_state=42)\n",
    "\n",
    "    mlp_model = MLPClassifier(\n",
    "        hidden_layer_sizes=(100, 50),  # Dos capas ocultas con 100 y 50 neuronas respectivamente\n",
    "        activation='relu',  # Función de activación ReLU\n",
    "        solver='adam',  # Optimizador Adam\n",
    "        alpha=0.0001,  # Tasa de regularización L2\n",
    "        learning_rate='adaptive',  # Tasa de aprendizaje adaptativa\n",
    "        max_iter=1000,  # Número máximo de iteraciones\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    mlp_model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluar el modelo\n",
    "    accuracy = mlp_model.score(X_test, y_test)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_constant_class(y_true, constant_class):\n",
    "    y_pred_constant = [constant_class] * len(y_true)\n",
    "    \n",
    "    # Calcular el accuracy entre las etiquetas verdaderas y las predicciones constantes\n",
    "    accuracy = accuracy_score(y_true, y_pred_constant)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmarchetta/Desktop/Tesis-2024/myenv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mmarchetta/Desktop/Tesis-2024/myenv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mmarchetta/Desktop/Tesis-2024/myenv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exactitud del modelo final regresion logistica 0: 0.24571428571428572\n",
      "Exactitud del modelo final regresion logistica 1: 0.3657142857142857\n",
      "Exactitud del modelo final regresion logistica 2: 0.37714285714285717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmarchetta/Desktop/Tesis-2024/myenv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exactitud del modelo final regresion logistica 3: 0.35428571428571426\n",
      "Exactitud del modelo final gradient boosting 0: 0.5085714285714286\n",
      "Exactitud del modelo final gradient boosting 1: 0.48\n",
      "Exactitud del modelo final gradient boosting 2: 0.4857142857142857\n",
      "Exactitud del modelo final gradient boosting 3: 0.4857142857142857\n",
      "Exactitud del modelo final SVM 0: 0.4\n",
      "Exactitud del modelo final SVM 1: 0.46285714285714286\n",
      "Exactitud del modelo final SVM 2: 0.4057142857142857\n",
      "Exactitud del modelo final SVM 3: 0.4057142857142857\n",
      "Exactitud del modelo final MLP 0: 0.2342857142857143\n",
      "Exactitud del modelo final MLP 1: 0.24571428571428572\n",
      "Exactitud del modelo final MLP 2: 0.2742857142857143\n",
      "Exactitud del modelo final MLP 3: 0.25142857142857145\n",
      "Exactitud clase constante - Alcista fuerte: 0.2227324913892078\n",
      "Exactitud clase constante - Alcista fuerte: 0.2227324913892078\n",
      "Exactitud clase constante - Alcista leve: 0.18714121699196326\n",
      "Exactitud clase constante - Lateral: 0.1894374282433984\n",
      "Exactitud clase constante - Bajista leve: 0.16647531572904709\n",
      "Exactitud clase constante - Bajista fuerte: 0.23421354764638347\n"
     ]
    }
   ],
   "source": [
    "print(\"Exactitud del modelo final regresion logistica 0:\", basic_logistic_regression(dataset))\n",
    "print(\"Exactitud del modelo final regresion logistica 1:\", basic_logistic_regression(dataset1))\n",
    "print(\"Exactitud del modelo final regresion logistica 2:\", basic_logistic_regression(dataset2))\n",
    "print(\"Exactitud del modelo final regresion logistica 3:\", basic_logistic_regression(dataset3))\n",
    "\n",
    "print(\"Exactitud del modelo final gradient boosting 0:\", basic_gradient_boosting(dataset))\n",
    "print(\"Exactitud del modelo final gradient boosting 1:\", basic_gradient_boosting(dataset1))\n",
    "print(\"Exactitud del modelo final gradient boosting 2:\", basic_gradient_boosting(dataset2))\n",
    "print(\"Exactitud del modelo final gradient boosting 3:\", basic_gradient_boosting(dataset2))\n",
    "\n",
    "print(\"Exactitud del modelo final SVM 0:\", basic_svm(dataset))\n",
    "print(\"Exactitud del modelo final SVM 1:\", basic_svm(dataset1))\n",
    "print(\"Exactitud del modelo final SVM 2:\", basic_svm(dataset2))\n",
    "print(\"Exactitud del modelo final SVM 3:\", basic_svm(dataset3))\n",
    "\n",
    "print(\"Exactitud del modelo final MLP 0:\", basic_MLP(dataset))\n",
    "print(\"Exactitud del modelo final MLP 1:\", basic_MLP(dataset1))\n",
    "print(\"Exactitud del modelo final MLP 2:\", basic_MLP(dataset2))\n",
    "print(\"Exactitud del modelo final MLP 3:\", basic_MLP(dataset3))\n",
    "\n",
    "print(\"Exactitud clase constante - Alcista fuerte:\", predict_constant_class(dataset['Tendencia'], \"Alcista fuerte\"))\n",
    "print(\"Exactitud clase constante - Alcista fuerte:\", predict_constant_class(dataset['Tendencia'], \"Alcista fuerte\"))\n",
    "print(\"Exactitud clase constante - Alcista leve:\", predict_constant_class(dataset['Tendencia'], \"Alcista leve\"))\n",
    "print(\"Exactitud clase constante - Lateral:\", predict_constant_class(dataset['Tendencia'], \"Lateral\"))\n",
    "print(\"Exactitud clase constante - Bajista leve:\", predict_constant_class(dataset['Tendencia'], \"Bajista leve\"))\n",
    "print(\"Exactitud clase constante - Bajista fuerte:\", predict_constant_class(dataset['Tendencia'], \"Bajista fuerte\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
