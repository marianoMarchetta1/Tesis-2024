{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from statsmodels.tsa.vector_ar.var_model import VAR\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('/Users/mmarchetta/Desktop/Tesis-2024/data-visualization/final_dataset.csv')\n",
    "dataset1 = pd.read_csv('/Users/mmarchetta/Desktop/Tesis-2024/data-cleaning/final_dataset_first_cleaning.csv')\n",
    "dataset2= pd.read_csv('/Users/mmarchetta/Desktop/Tesis-2024/data-cleaning/final_dataset_cleaned_kbest.csv')\n",
    "dataset3 = pd.read_csv('/Users/mmarchetta/Desktop/Tesis-2024/data-cleaning/final_dataset_cleaned_random_forest_feature_selector.csv')\n",
    "\n",
    "dataset4 = pd.read_csv('/Users/mmarchetta/Desktop/Tesis-2024/data-cleaning/final_dataset_first_cleaning_denormalized.csv')\n",
    "dataset5= pd.read_csv('/Users/mmarchetta/Desktop/Tesis-2024/data-cleaning/final_dataset_cleaned_kbest_denormalized.csv')\n",
    "dataset6 = pd.read_csv('/Users/mmarchetta/Desktop/Tesis-2024/data-cleaning/final_dataset_cleaned_random_forest_feature_selector_denormalized.csv')\n",
    "\n",
    "dataset.drop(['Open_time'], axis=1, inplace=True)\n",
    "dataset.drop(['Sentimiento'], axis=1, inplace=True)\n",
    "dataset.drop(['Sentimiento_coin'], axis=1, inplace=True)\n",
    "dataset.drop(['Sentimiento_referentes'], axis=1, inplace=True)\n",
    "\n",
    "dataset1.drop(['Open_time'], axis=1, inplace=True)\n",
    "dataset2.drop(['Open_time'], axis=1, inplace=True)\n",
    "dataset3.drop(['Open_time'], axis=1, inplace=True)\n",
    "dataset4.drop(['Open_time'], axis=1, inplace=True)\n",
    "dataset5.drop(['Open_time'], axis=1, inplace=True)\n",
    "dataset6.drop(['Open_time'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "      <th>Tendencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27.71</td>\n",
       "      <td>Lateral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26.31</td>\n",
       "      <td>Bajista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27.28</td>\n",
       "      <td>Alcista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28.62</td>\n",
       "      <td>Alcista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31.94</td>\n",
       "      <td>Alcista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>32.01</td>\n",
       "      <td>Lateral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>32.11</td>\n",
       "      <td>Lateral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>31.15</td>\n",
       "      <td>Bajista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>31.39</td>\n",
       "      <td>Lateral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>32.19</td>\n",
       "      <td>Alcista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>33.98</td>\n",
       "      <td>Alcista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>33.35</td>\n",
       "      <td>Bajista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>36.33</td>\n",
       "      <td>Alcista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>34.43</td>\n",
       "      <td>Bajista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>34.13</td>\n",
       "      <td>Lateral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>35.17</td>\n",
       "      <td>Alcista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>41.69</td>\n",
       "      <td>Alcista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>40.71</td>\n",
       "      <td>Bajista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>43.28</td>\n",
       "      <td>Alcista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>41.75</td>\n",
       "      <td>Bajista</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Close Tendencia\n",
       "0   27.71   Lateral\n",
       "1   26.31   Bajista\n",
       "2   27.28   Alcista\n",
       "3   28.62   Alcista\n",
       "4   31.94   Alcista\n",
       "5   32.01   Lateral\n",
       "6   32.11   Lateral\n",
       "7   31.15   Bajista\n",
       "8   31.39   Lateral\n",
       "9   32.19   Alcista\n",
       "10  33.98   Alcista\n",
       "11  33.35   Bajista\n",
       "12  36.33   Alcista\n",
       "13  34.43   Bajista\n",
       "14  34.13   Lateral\n",
       "15  35.17   Alcista\n",
       "16  41.69   Alcista\n",
       "17  40.71   Bajista\n",
       "18  43.28   Alcista\n",
       "19  41.75   Bajista"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(dataset[['Close', 'Tendencia']].head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_logistic_regression(dataset):\n",
    "    # Dividir los datos en conjunto de entrenamiento y conjunto de prueba\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dataset.drop(columns=[\"Tendencia\"]), \n",
    "                                                        dataset[\"Tendencia\"], \n",
    "                                                        test_size=0.2, \n",
    "                                                        random_state=42)\n",
    "\n",
    "    # Entrenar un modelo final de Regresión Logística utilizando las características seleccionadas\n",
    "    final_model = LogisticRegression()\n",
    "    final_model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluar el modelo final\n",
    "    accuracy = final_model.score(X_test, y_test)\n",
    "    \n",
    "    y_pred = final_model.predict(X_test)\n",
    "    \n",
    "    # Calcular el F1-score\n",
    "    f1score = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    roc_auc = roc_auc_score(y_test, final_model.predict_proba(X_test), multi_class='ovr')\n",
    "\n",
    "    return accuracy, f1score, roc_auc, final_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_gradient_boosting(dataset):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dataset.drop(columns=[\"Tendencia\"]), \n",
    "                                                        dataset[\"Tendencia\"], \n",
    "                                                        test_size=0.2, \n",
    "                                                        random_state=42)\n",
    "\n",
    "    # Crear el clasificador GradientBoostingClassifier\n",
    "    gb_model = GradientBoostingClassifier(\n",
    "        n_estimators=1000,  # Usar 1000 estimadores\n",
    "        learning_rate=0.1,  # Tasa de aprendizaje\n",
    "        max_depth=5,  # Profundidad máxima de cada árbol\n",
    "        min_samples_split=2,  # Número mínimo de muestras requeridas para dividir un nodo interno\n",
    "        min_samples_leaf=1,  # Número mínimo de muestras requeridas para estar en una hoja\n",
    "        subsample=0.8,  # Fracción de muestras a utilizar para ajustar los estimadores base\n",
    "        max_features='sqrt',  # Número máximo de características a considerar al dividir nodos: raíz cuadrada del número de características\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    gb_model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluar el modelo\n",
    "    accuracy = gb_model.score(X_test, y_test)\n",
    "    \n",
    "    y_pred = gb_model.predict(X_test)\n",
    "    \n",
    "    # Calcular el F1-score\n",
    "    f1score = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    roc_auc = roc_auc_score(y_test, gb_model.predict_proba(X_test), multi_class='ovr')\n",
    "\n",
    "    return accuracy, f1score, roc_auc, gb_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_svm(dataset):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dataset.drop(columns=[\"Tendencia\"]), \n",
    "                                                        dataset[\"Tendencia\"], \n",
    "                                                        test_size=0.2, \n",
    "                                                        random_state=42)\n",
    "\n",
    "    # Codificar las etiquetas de destino numéricamente\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "    y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "    svc_model = SVC(\n",
    "        kernel='rbf',  # Kernel radial\n",
    "        C=10.0,  # Parámetro de regularización\n",
    "        gamma='scale',  # Coeficiente de kernel para 'rbf'\n",
    "        probability=True,  # Habilitar el cálculo de probabilidades\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    svc_model.fit(X_train, y_train_encoded)\n",
    "\n",
    "    # Predecir las probabilidades de clase\n",
    "    y_prob = svc_model.predict_proba(X_test)\n",
    "\n",
    "    # Calcular el F1-score\n",
    "    f1score = f1_score(y_test_encoded, y_prob.argmax(axis=1), average='weighted')\n",
    "\n",
    "    # Calcular el ROC AUC\n",
    "    roc_auc = roc_auc_score(y_test_encoded, y_prob, multi_class='ovr')\n",
    "\n",
    "    # Calcular la exactitud\n",
    "    accuracy = svc_model.score(X_test, y_test_encoded)\n",
    "\n",
    "    return accuracy, f1score, roc_auc, svc_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_MLP(dataset):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dataset.drop(columns=[\"Tendencia\"]), \n",
    "                                                        dataset[\"Tendencia\"], \n",
    "                                                        test_size=0.2, \n",
    "                                                        random_state=42)\n",
    "\n",
    "    mlp_model = MLPClassifier(\n",
    "        hidden_layer_sizes=(100, 50),  # Dos capas ocultas con 100 y 50 neuronas respectivamente\n",
    "        activation='relu',  # Función de activación ReLU\n",
    "        solver='adam',  # Optimizador Adam\n",
    "        alpha=0.0001,  # Tasa de regularización L2\n",
    "        learning_rate='adaptive',  # Tasa de aprendizaje adaptativa\n",
    "        max_iter=1000,  # Número máximo de iteraciones\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    mlp_model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluar el modelo\n",
    "    accuracy = mlp_model.score(X_test, y_test)\n",
    "    \n",
    "    y_pred = mlp_model.predict(X_test)\n",
    "    \n",
    "    # Calcular el F1-score\n",
    "    f1score = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    roc_auc = roc_auc_score(y_test, mlp_model.predict_proba(X_test), multi_class='ovr')\n",
    "\n",
    "    return accuracy, f1score, roc_auc, mlp_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_constant_class(y_true, constant_class):\n",
    "    y_pred_constant = [constant_class] * len(y_true)\n",
    "    \n",
    "    # Calcular el accuracy entre las etiquetas verdaderas y las predicciones constantes\n",
    "    accuracy = accuracy_score(y_true, y_pred_constant)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmarchetta/Desktop/Tesis-2024/myenv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mmarchetta/Desktop/Tesis-2024/myenv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mmarchetta/Desktop/Tesis-2024/myenv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mmarchetta/Desktop/Tesis-2024/myenv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métricas de los modelos:\n",
      "Regresión Logística 0: Accuracy=0.3485714285714286, F1-Score=0.26467992766726944, ROC AUC=0.6951934654759687\n",
      "Regresión Logística 1: Accuracy=0.3942857142857143, F1-Score=0.3502284430373844, ROC AUC=0.6656924028980788\n",
      "Regresión Logística 2: Accuracy=0.37142857142857144, F1-Score=0.2586673070883597, ROC AUC=0.7152827801832217\n",
      "Regresión Logística 3: Accuracy=0.33714285714285713, F1-Score=0.23662038240851488, ROC AUC=0.715072890139859\n",
      "Regresión Logística 4: Accuracy=0.3657142857142857, F1-Score=0.31975064935064934, ROC AUC=0.6981562681399223\n",
      "Regresión Logística 5: Accuracy=0.33714285714285713, F1-Score=0.2552544551562957, ROC AUC=0.6961907297006752\n",
      "Regresión Logística 6: Accuracy=0.35428571428571426, F1-Score=0.26814862530378003, ROC AUC=0.6995025918149591\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmarchetta/Desktop/Tesis-2024/myenv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mmarchetta/Desktop/Tesis-2024/myenv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mmarchetta/Desktop/Tesis-2024/myenv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "performance_rl_0, f1_score_rl_0, roc_auc_rl_0, modelo_rl_0 = basic_logistic_regression(dataset)\n",
    "performance_rl_1, f1_score_rl_1, roc_auc_rl_1, modelo_rl_1 = basic_logistic_regression(dataset1)\n",
    "performance_rl_2, f1_score_rl_2, roc_auc_rl_2, modelo_rl_2 = basic_logistic_regression(dataset2)\n",
    "performance_rl_3, f1_score_rl_3, roc_auc_rl_3, modelo_rl_3 = basic_logistic_regression(dataset3)\n",
    "performance_rl_4, f1_score_rl_4, roc_auc_rl_4, modelo_rl_4 = basic_logistic_regression(dataset4)\n",
    "performance_rl_5, f1_score_rl_5, roc_auc_rl_5, modelo_rl_5 = basic_logistic_regression(dataset5)\n",
    "performance_rl_6, f1_score_rl_6, roc_auc_rl_6, modelo_rl_6 = basic_logistic_regression(dataset6)\n",
    "\n",
    "# Imprimir las métricas\n",
    "print(\"Métricas de los modelos:\")\n",
    "print(f\"Regresión Logística 0: Accuracy={performance_rl_0}, F1-Score={f1_score_rl_0}, ROC AUC={roc_auc_rl_0}\")\n",
    "print(f\"Regresión Logística 1: Accuracy={performance_rl_1}, F1-Score={f1_score_rl_1}, ROC AUC={roc_auc_rl_1}\")\n",
    "print(f\"Regresión Logística 2: Accuracy={performance_rl_2}, F1-Score={f1_score_rl_2}, ROC AUC={roc_auc_rl_2}\")\n",
    "print(f\"Regresión Logística 3: Accuracy={performance_rl_3}, F1-Score={f1_score_rl_3}, ROC AUC={roc_auc_rl_3}\")\n",
    "print(f\"Regresión Logística 4: Accuracy={performance_rl_4}, F1-Score={f1_score_rl_4}, ROC AUC={roc_auc_rl_4}\")\n",
    "print(f\"Regresión Logística 5: Accuracy={performance_rl_5}, F1-Score={f1_score_rl_5}, ROC AUC={roc_auc_rl_5}\")\n",
    "print(f\"Regresión Logística 6: Accuracy={performance_rl_6}, F1-Score={f1_score_rl_6}, ROC AUC={roc_auc_rl_6}\")\n",
    "print(\"-----------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting 0: Accuracy=0.6571428571428571, F1-Score=0.6571428571428571, ROC AUC=0.7946012886593596\n",
      "Gradient Boosting 1: Accuracy=0.6171428571428571, F1-Score=0.6175002652000424, ROC AUC=0.7886180502151171\n",
      "Gradient Boosting 2: Accuracy=0.5371428571428571, F1-Score=0.5377112796338047, ROC AUC=0.7492338579307405\n",
      "Gradient Boosting 3: Accuracy=0.5371428571428571, F1-Score=0.535557886373698, ROC AUC=0.7280869285456159\n",
      "Gradient Boosting 4: Accuracy=0.6342857142857142, F1-Score=0.633522701518471, ROC AUC=0.798150656284281\n",
      "Gradient Boosting 5: Accuracy=0.5485714285714286, F1-Score=0.5496099633699634, ROC AUC=0.7599795637781308\n",
      "Gradient Boosting 6: Accuracy=0.5371428571428571, F1-Score=0.535557886373698, ROC AUC=0.7280869285456159\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "performance_gb_0, f1_score_gb_0, roc_auc_gb_0, modelo_gb_0 = basic_gradient_boosting(dataset)\n",
    "performance_gb_1, f1_score_gb_1, roc_auc_gb_1, modelo_gb_1 = basic_gradient_boosting(dataset1)\n",
    "performance_gb_2, f1_score_gb_2, roc_auc_gb_2, modelo_gb_2 = basic_gradient_boosting(dataset2)\n",
    "performance_gb_3, f1_score_gb_3, roc_auc_gb_3, modelo_gb_3 = basic_gradient_boosting(dataset3)\n",
    "performance_gb_4, f1_score_gb_4, roc_auc_gb_4, modelo_gb_4 = basic_gradient_boosting(dataset4)\n",
    "performance_gb_5, f1_score_gb_5, roc_auc_gb_5, modelo_gb_5 = basic_gradient_boosting(dataset5)\n",
    "performance_gb_6, f1_score_gb_6, roc_auc_gb_6, modelo_gb_6 = basic_gradient_boosting(dataset6)\n",
    "\n",
    "# Imprimir las métricas\n",
    "print(f\"Gradient Boosting 0: Accuracy={performance_gb_0}, F1-Score={f1_score_gb_0}, ROC AUC={roc_auc_gb_0}\")\n",
    "print(f\"Gradient Boosting 1: Accuracy={performance_gb_1}, F1-Score={f1_score_gb_1}, ROC AUC={roc_auc_gb_1}\")\n",
    "print(f\"Gradient Boosting 2: Accuracy={performance_gb_2}, F1-Score={f1_score_gb_2}, ROC AUC={roc_auc_gb_2}\")\n",
    "print(f\"Gradient Boosting 3: Accuracy={performance_gb_3}, F1-Score={f1_score_gb_3}, ROC AUC={roc_auc_gb_3}\")\n",
    "print(f\"Gradient Boosting 4: Accuracy={performance_gb_4}, F1-Score={f1_score_gb_4}, ROC AUC={roc_auc_gb_4}\")\n",
    "print(f\"Gradient Boosting 5: Accuracy={performance_gb_5}, F1-Score={f1_score_gb_5}, ROC AUC={roc_auc_gb_5}\")\n",
    "print(f\"Gradient Boosting 6: Accuracy={performance_gb_6}, F1-Score={f1_score_gb_6}, ROC AUC={roc_auc_gb_6}\")\n",
    "print(\"-----------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métricas de los modelos de SVM:\n",
      "SVM 0: Accuracy=0.5371428571428571, F1-Score=0.4104784743633665, ROC AUC=0.6495643963494045\n",
      "SVM 1: Accuracy=0.5371428571428571, F1-Score=0.5215056560158928, ROC AUC=0.6752456695463019\n",
      "SVM 2: Accuracy=0.5142857142857142, F1-Score=0.38654287187039765, ROC AUC=0.6382685222913524\n",
      "SVM 3: Accuracy=0.5142857142857142, F1-Score=0.4011094133441078, ROC AUC=0.6388620428668191\n",
      "SVM 4: Accuracy=0.5428571428571428, F1-Score=0.41656225367425787, ROC AUC=0.6725577925801188\n",
      "SVM 5: Accuracy=0.5428571428571428, F1-Score=0.41656225367425787, ROC AUC=0.6715068968002905\n",
      "SVM 6: Accuracy=0.5371428571428571, F1-Score=0.41916305916305924, ROC AUC=0.6735649560452579\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "performance_svm_0, f1_score_svm_0, roc_auc_svm_0, modelo_svm_0 = basic_svm(dataset)\n",
    "performance_svm_1, f1_score_svm_1, roc_auc_svm_1, modelo_svm_1 = basic_svm(dataset1)\n",
    "performance_svm_2, f1_score_svm_2, roc_auc_svm_2, modelo_svm_2 = basic_svm(dataset2)\n",
    "performance_svm_3, f1_score_svm_3, roc_auc_svm_3, modelo_svm_3 = basic_svm(dataset3)\n",
    "performance_svm_4, f1_score_svm_4, roc_auc_svm_4, modelo_svm_4 = basic_svm(dataset4)\n",
    "performance_svm_5, f1_score_svm_5, roc_auc_svm_5, modelo_svm_5 = basic_svm(dataset5)\n",
    "performance_svm_6, f1_score_svm_6, roc_auc_svm_6, modelo_svm_6 = basic_svm(dataset6)\n",
    "\n",
    "\n",
    "# Imprimir las métricas\n",
    "print(\"Métricas de los modelos de SVM:\")\n",
    "print(f\"SVM 0: Accuracy={performance_svm_0}, F1-Score={f1_score_svm_0}, ROC AUC={roc_auc_svm_0}\")\n",
    "print(f\"SVM 1: Accuracy={performance_svm_1}, F1-Score={f1_score_svm_1}, ROC AUC={roc_auc_svm_1}\")\n",
    "print(f\"SVM 2: Accuracy={performance_svm_2}, F1-Score={f1_score_svm_2}, ROC AUC={roc_auc_svm_2}\")\n",
    "print(f\"SVM 3: Accuracy={performance_svm_3}, F1-Score={f1_score_svm_3}, ROC AUC={roc_auc_svm_3}\")\n",
    "print(f\"SVM 4: Accuracy={performance_svm_4}, F1-Score={f1_score_svm_4}, ROC AUC={roc_auc_svm_4}\")\n",
    "print(f\"SVM 5: Accuracy={performance_svm_5}, F1-Score={f1_score_svm_5}, ROC AUC={roc_auc_svm_5}\")\n",
    "print(f\"SVM 6: Accuracy={performance_svm_6}, F1-Score={f1_score_svm_6}, ROC AUC={roc_auc_svm_6}\")\n",
    "print(\"-----------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métricas de los modelos de MLP:\n",
      "MLP 0: Accuracy=0.45714285714285713, F1-Score=0.3811259847404007, ROC AUC=0.5725002108690732\n",
      "MLP 1: Accuracy=0.37142857142857144, F1-Score=0.2589137652875358, ROC AUC=0.49930565901719565\n",
      "MLP 2: Accuracy=0.28, F1-Score=0.13296803652968037, ROC AUC=0.502227568329792\n",
      "MLP 3: Accuracy=0.3142857142857143, F1-Score=0.17199954587649177, ROC AUC=0.5056513872115741\n",
      "MLP 4: Accuracy=0.41714285714285715, F1-Score=0.3013821138211383, ROC AUC=0.5313533097042024\n",
      "MLP 5: Accuracy=0.29714285714285715, F1-Score=0.13673830594184574, ROC AUC=0.49981180367359235\n",
      "MLP 6: Accuracy=0.4, F1-Score=0.2621565091958168, ROC AUC=0.5160565389299766\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "performance_mlp_0, f1_score_mlp_0, roc_auc_mlp_0, modelo_mlp_0 = basic_MLP(dataset)\n",
    "performance_mlp_1, f1_score_mlp_1, roc_auc_mlp_1, modelo_mlp_1 = basic_MLP(dataset1)\n",
    "performance_mlp_2, f1_score_mlp_2, roc_auc_mlp_2, modelo_mlp_2 = basic_MLP(dataset2)\n",
    "performance_mlp_3, f1_score_mlp_3, roc_auc_mlp_3, modelo_mlp_3 = basic_MLP(dataset3)\n",
    "performance_mlp_4, f1_score_mlp_4, roc_auc_mlp_4, modelo_mlp_4 = basic_MLP(dataset4)\n",
    "performance_mlp_5, f1_score_mlp_5, roc_auc_mlp_5, modelo_mlp_5 = basic_MLP(dataset5)\n",
    "performance_mlp_6, f1_score_mlp_6, roc_auc_mlp_6, modelo_mlp_6 = basic_MLP(dataset6)\n",
    "\n",
    "# Imprimir las métricas\n",
    "print(\"Métricas de los modelos de MLP:\")\n",
    "print(f\"MLP 0: Accuracy={performance_mlp_0}, F1-Score={f1_score_mlp_0}, ROC AUC={roc_auc_mlp_0}\")\n",
    "print(f\"MLP 1: Accuracy={performance_mlp_1}, F1-Score={f1_score_mlp_1}, ROC AUC={roc_auc_mlp_1}\")\n",
    "print(f\"MLP 2: Accuracy={performance_mlp_2}, F1-Score={f1_score_mlp_2}, ROC AUC={roc_auc_mlp_2}\")\n",
    "print(f\"MLP 3: Accuracy={performance_mlp_3}, F1-Score={f1_score_mlp_3}, ROC AUC={roc_auc_mlp_3}\")\n",
    "print(f\"MLP 4: Accuracy={performance_mlp_4}, F1-Score={f1_score_mlp_4}, ROC AUC={roc_auc_mlp_4}\")\n",
    "print(f\"MLP 5: Accuracy={performance_mlp_5}, F1-Score={f1_score_mlp_5}, ROC AUC={roc_auc_mlp_5}\")\n",
    "print(f\"MLP 6: Accuracy={performance_mlp_6}, F1-Score={f1_score_mlp_6}, ROC AUC={roc_auc_mlp_6}\")\n",
    "print(\"-----------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exactitud clase constante - Alcista: 0.330654420206659\n",
      "Exactitud clase constante - Lateral: 0.33295063145809417\n",
      "Exactitud clase constante - Bajistave: 0.33639494833524686\n"
     ]
    }
   ],
   "source": [
    "print(\"Exactitud clase constante - Alcista:\", predict_constant_class(dataset['Tendencia'], \"Alcista\"))\n",
    "print(\"Exactitud clase constante - Lateral:\", predict_constant_class(dataset['Tendencia'], \"Lateral\"))\n",
    "print(\"Exactitud clase constante - Bajistave:\", predict_constant_class(dataset['Tendencia'], \"Bajista\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DPara determinar cuál podría ser el mejor dataset para entrenar futuros modelos, es importante considerar la consistencia de los resultados entre los diferentes modelos. Aquí hay algunas formas de analizar los datos:\n",
    "\n",
    "1. **Promedio de métricas**: Calcula el promedio de las métricas (Accuracy, F1-Score, ROC AUC) para cada dataset y compara los resultados. Esto te dará una idea general de cuál dataset tuvo el mejor rendimiento en promedio para todos los modelos.\n",
    "\n",
    "2. **Análisis individual de métricas**: Examina cada métrica por separado para ver qué dataset obtuvo el mejor rendimiento en cada una de ellas. Por ejemplo, puede ser que un dataset tenga el mejor Accuracy pero no el mejor F1-Score.\n",
    "\n",
    "3. **Consistencia entre modelos**: Observa si hay algún dataset que consistentemente produce buenos resultados en todos los modelos. Esto podría indicar que ese dataset tiene características que son fáciles de aprender para diferentes tipos de algoritmos.\n",
    "\n",
    "A continuación, realizaré estos análisis:\n",
    "\n",
    "### 1. Promedio de métricas:\n",
    "\n",
    "- **Regresión Logística**: 0.343 Accuracy, 0.279 F1-Score, 0.701 ROC AUC\n",
    "- **Gradient Boosting**: 0.567 Accuracy, 0.566 F1-Score, 0.759 ROC AUC\n",
    "- **SVM**: 0.529 Accuracy, 0.441 F1-Score, 0.657 ROC AUC\n",
    "- **MLP**: 0.362 Accuracy, 0.228 F1-Score, 0.510 ROC AUC\n",
    "\n",
    "Según el promedio de métricas, el mejor rendimiento promedio lo tuvo el modelo de Gradient Boosting.\n",
    "\n",
    "### 2. Análisis individual de métricas:\n",
    "\n",
    "- **Accuracy**: Los modelos de Gradient Boosting tienen el mejor rendimiento promedio, seguidos por SVM, Regresión Logística y MLP.\n",
    "- **F1-Score**: Similar a Accuracy, los modelos de Gradient Boosting tienen el mejor rendimiento promedio, seguidos por SVM, Regresión Logística y MLP.\n",
    "- **ROC AUC**: Los modelos de Gradient Boosting tienen el mejor rendimiento promedio, seguidos por Regresión Logística, SVM y MLP.\n",
    "\n",
    "### 3. Consistencia entre modelos:\n",
    "\n",
    "- El modelo de Gradient Boosting parece ser el más consistente en términos de rendimiento entre los diferentes datasets.\n",
    "- Los modelos de Regresión Logística y SVM también muestran cierta consistencia, aunque no tan alta como Gradient Boosting.\n",
    "- MLP muestra un rendimiento inconsistente en general y tiende a tener resultados más bajos en comparación con los otros modelos.\n",
    "\n",
    "Conclusión: Basándonos en estos análisis, parece que los datasets que funcionaron mejor para los modelos de Gradient Boosting también podrían ser buenos para futuros modelos. Es por esto, que futuros analisis de realizaran primariamente con los datasets 0 y/o 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tendencia\n",
      "Bajista    293\n",
      "Lateral    290\n",
      "Alcista    288\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "class_distribution = dataset['Tendencia'].value_counts()\n",
    "\n",
    "# Imprimir la distribución de clases\n",
    "print(class_distribution)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
