{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported auto_timeseries version:0.0.90. Call by using:\n",
      "model = auto_timeseries(score_type='rmse',\n",
      "        time_interval='M', non_seasonal_pdq=None, seasonality=False,\n",
      "        seasonal_period=12, model_type=['best'], verbose=2, dask_xgboost_flag=0)\n",
      "model.fit(traindata, ts_column,target)\n",
      "model.predict(testdata, model='best')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-13 09:04:09.148880: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from auto_ts import auto_timeseries\n",
    "import dill\n",
    "import talib\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Flatten, Conv1D, MaxPooling1D, BatchNormalization\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from keras.callbacks import EarlyStopping\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.losses import categorical_crossentropy\n",
    "from sklearn.metrics import log_loss\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evito que ciertas columnas se transformen a notacion cientifica en las predicciones\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    'Open_time',\n",
    "    'Open',\n",
    "    'High',\n",
    "    'Low',\n",
    "    # 'Close',\n",
    "    'Number of trades',\n",
    "    'Close_BTCUSDT',\n",
    "    'Volume_BTCUSDT',\n",
    "    'Number_of_trades_BTCUSDT',\n",
    "    'Close_ETHUSDT',\n",
    "    'Volume_ETHUSDT',\n",
    "    'Number_of_trades_ETHUSDT',\n",
    "    'Close_BNBUSDT',\n",
    "    'Volume_BNBUSDT',\n",
    "    'Number_of_trades_BNBUSDT',\n",
    "    'SMA_20',\n",
    "    'EMA_20',\n",
    "    'Upper_Band',\n",
    "    'Middle_Band',\n",
    "    'Lower_Band',\n",
    "    'RSI',\n",
    "    'buy_1000x_high_coinbase',\n",
    "    'sell_1000x_high_coinbase',\n",
    "    'total_trades_coinbase',\t\n",
    "    'Tweets_Utilizados',\n",
    "    'Tweets_Utilizados_coin',\n",
    "    'Tweets_Utilizados_referentes',\n",
    "    'Tweets_Utilizados_whale_alert',\n",
    "    'Buy_1000x_high',\n",
    "    'sell_1000x_high',\n",
    "    'total_trades_binance'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Armado y entrenamiento de un clasificador a partir de los datos originales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelo LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_dataset = pd.read_csv('/Users/mmarchetta/Desktop/Tesis-2024/data-visualization/final_dataset.csv') \n",
    "classifier_dataset = complete_dataset[columns]\n",
    "classifier_dataset['Open_time'] = pd.to_datetime(classifier_dataset['Open_time'])\n",
    "classifier_dataset['Tendencia'] = complete_dataset['Tendencia']\n",
    "\n",
    "clasifier_validation = classifier_dataset[-10:]\n",
    "classifier_dataset = classifier_dataset[:-10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open_time</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Number of trades</th>\n",
       "      <th>Close_BTCUSDT</th>\n",
       "      <th>Volume_BTCUSDT</th>\n",
       "      <th>Number_of_trades_BTCUSDT</th>\n",
       "      <th>Close_ETHUSDT</th>\n",
       "      <th>Volume_ETHUSDT</th>\n",
       "      <th>Number_of_trades_ETHUSDT</th>\n",
       "      <th>Close_BNBUSDT</th>\n",
       "      <th>Volume_BNBUSDT</th>\n",
       "      <th>Number_of_trades_BNBUSDT</th>\n",
       "      <th>SMA_20</th>\n",
       "      <th>EMA_20</th>\n",
       "      <th>Upper_Band</th>\n",
       "      <th>Middle_Band</th>\n",
       "      <th>Lower_Band</th>\n",
       "      <th>RSI</th>\n",
       "      <th>buy_1000x_high_coinbase</th>\n",
       "      <th>sell_1000x_high_coinbase</th>\n",
       "      <th>total_trades_coinbase</th>\n",
       "      <th>Tweets_Utilizados</th>\n",
       "      <th>Tweets_Utilizados_coin</th>\n",
       "      <th>Tweets_Utilizados_referentes</th>\n",
       "      <th>Tweets_Utilizados_whale_alert</th>\n",
       "      <th>Buy_1000x_high</th>\n",
       "      <th>sell_1000x_high</th>\n",
       "      <th>total_trades_binance</th>\n",
       "      <th>Tendencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>941</th>\n",
       "      <td>2024-04-25</td>\n",
       "      <td>6.93</td>\n",
       "      <td>7.00</td>\n",
       "      <td>6.70</td>\n",
       "      <td>71088.00</td>\n",
       "      <td>64498.34</td>\n",
       "      <td>31341.46</td>\n",
       "      <td>1375324.00</td>\n",
       "      <td>3155.80</td>\n",
       "      <td>352288.55</td>\n",
       "      <td>861077.00</td>\n",
       "      <td>613.20</td>\n",
       "      <td>453745.52</td>\n",
       "      <td>353114.00</td>\n",
       "      <td>7.43</td>\n",
       "      <td>7.45</td>\n",
       "      <td>9.08</td>\n",
       "      <td>7.43</td>\n",
       "      <td>5.77</td>\n",
       "      <td>38.83</td>\n",
       "      <td>21.00</td>\n",
       "      <td>26.00</td>\n",
       "      <td>33468.00</td>\n",
       "      <td>151</td>\n",
       "      <td>114</td>\n",
       "      <td>0.00</td>\n",
       "      <td>22.00</td>\n",
       "      <td>242.00</td>\n",
       "      <td>219.00</td>\n",
       "      <td>48000.00</td>\n",
       "      <td>Lateral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942</th>\n",
       "      <td>2024-04-26</td>\n",
       "      <td>6.86</td>\n",
       "      <td>6.95</td>\n",
       "      <td>6.71</td>\n",
       "      <td>67383.00</td>\n",
       "      <td>63770.01</td>\n",
       "      <td>27085.19</td>\n",
       "      <td>1025561.00</td>\n",
       "      <td>3131.30</td>\n",
       "      <td>252522.65</td>\n",
       "      <td>628635.00</td>\n",
       "      <td>598.00</td>\n",
       "      <td>302119.88</td>\n",
       "      <td>269508.00</td>\n",
       "      <td>7.34</td>\n",
       "      <td>7.38</td>\n",
       "      <td>8.94</td>\n",
       "      <td>7.34</td>\n",
       "      <td>5.74</td>\n",
       "      <td>37.81</td>\n",
       "      <td>29.00</td>\n",
       "      <td>24.00</td>\n",
       "      <td>26619.00</td>\n",
       "      <td>117</td>\n",
       "      <td>106</td>\n",
       "      <td>0.00</td>\n",
       "      <td>14.00</td>\n",
       "      <td>292.00</td>\n",
       "      <td>324.00</td>\n",
       "      <td>42000.00</td>\n",
       "      <td>Lateral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>943</th>\n",
       "      <td>2024-04-27</td>\n",
       "      <td>6.76</td>\n",
       "      <td>6.87</td>\n",
       "      <td>6.51</td>\n",
       "      <td>64779.00</td>\n",
       "      <td>63461.98</td>\n",
       "      <td>20933.06</td>\n",
       "      <td>912422.00</td>\n",
       "      <td>3255.56</td>\n",
       "      <td>323811.19</td>\n",
       "      <td>734026.00</td>\n",
       "      <td>596.20</td>\n",
       "      <td>268783.91</td>\n",
       "      <td>233820.00</td>\n",
       "      <td>7.24</td>\n",
       "      <td>7.33</td>\n",
       "      <td>8.73</td>\n",
       "      <td>7.24</td>\n",
       "      <td>5.76</td>\n",
       "      <td>38.57</td>\n",
       "      <td>17.00</td>\n",
       "      <td>17.00</td>\n",
       "      <td>25565.00</td>\n",
       "      <td>101</td>\n",
       "      <td>138</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>248.00</td>\n",
       "      <td>179.00</td>\n",
       "      <td>41000.00</td>\n",
       "      <td>Lateral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>944</th>\n",
       "      <td>2024-04-28</td>\n",
       "      <td>6.81</td>\n",
       "      <td>6.95</td>\n",
       "      <td>6.69</td>\n",
       "      <td>43208.00</td>\n",
       "      <td>63118.62</td>\n",
       "      <td>16949.20</td>\n",
       "      <td>790652.00</td>\n",
       "      <td>3263.45</td>\n",
       "      <td>304766.01</td>\n",
       "      <td>753239.00</td>\n",
       "      <td>600.20</td>\n",
       "      <td>258059.43</td>\n",
       "      <td>206703.00</td>\n",
       "      <td>7.13</td>\n",
       "      <td>7.27</td>\n",
       "      <td>8.38</td>\n",
       "      <td>7.13</td>\n",
       "      <td>5.88</td>\n",
       "      <td>37.66</td>\n",
       "      <td>16.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>20954.00</td>\n",
       "      <td>82</td>\n",
       "      <td>106</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13.00</td>\n",
       "      <td>173.00</td>\n",
       "      <td>165.00</td>\n",
       "      <td>26000.00</td>\n",
       "      <td>Lateral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>945</th>\n",
       "      <td>2024-04-29</td>\n",
       "      <td>6.73</td>\n",
       "      <td>6.83</td>\n",
       "      <td>6.47</td>\n",
       "      <td>63006.00</td>\n",
       "      <td>63866.00</td>\n",
       "      <td>28150.23</td>\n",
       "      <td>1152296.00</td>\n",
       "      <td>3216.73</td>\n",
       "      <td>421831.29</td>\n",
       "      <td>943719.00</td>\n",
       "      <td>592.80</td>\n",
       "      <td>330474.01</td>\n",
       "      <td>271926.00</td>\n",
       "      <td>7.03</td>\n",
       "      <td>7.20</td>\n",
       "      <td>8.08</td>\n",
       "      <td>7.03</td>\n",
       "      <td>5.97</td>\n",
       "      <td>36.02</td>\n",
       "      <td>69.00</td>\n",
       "      <td>37.00</td>\n",
       "      <td>33959.00</td>\n",
       "      <td>115</td>\n",
       "      <td>125</td>\n",
       "      <td>0.00</td>\n",
       "      <td>24.00</td>\n",
       "      <td>260.00</td>\n",
       "      <td>188.00</td>\n",
       "      <td>41000.00</td>\n",
       "      <td>Bajista</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Open_time  Open  High  Low  Number of trades  Close_BTCUSDT  \\\n",
       "941 2024-04-25  6.93  7.00 6.70          71088.00       64498.34   \n",
       "942 2024-04-26  6.86  6.95 6.71          67383.00       63770.01   \n",
       "943 2024-04-27  6.76  6.87 6.51          64779.00       63461.98   \n",
       "944 2024-04-28  6.81  6.95 6.69          43208.00       63118.62   \n",
       "945 2024-04-29  6.73  6.83 6.47          63006.00       63866.00   \n",
       "\n",
       "     Volume_BTCUSDT  Number_of_trades_BTCUSDT  Close_ETHUSDT  Volume_ETHUSDT  \\\n",
       "941        31341.46                1375324.00        3155.80       352288.55   \n",
       "942        27085.19                1025561.00        3131.30       252522.65   \n",
       "943        20933.06                 912422.00        3255.56       323811.19   \n",
       "944        16949.20                 790652.00        3263.45       304766.01   \n",
       "945        28150.23                1152296.00        3216.73       421831.29   \n",
       "\n",
       "     Number_of_trades_ETHUSDT  Close_BNBUSDT  Volume_BNBUSDT  \\\n",
       "941                 861077.00         613.20       453745.52   \n",
       "942                 628635.00         598.00       302119.88   \n",
       "943                 734026.00         596.20       268783.91   \n",
       "944                 753239.00         600.20       258059.43   \n",
       "945                 943719.00         592.80       330474.01   \n",
       "\n",
       "     Number_of_trades_BNBUSDT  SMA_20  EMA_20  Upper_Band  Middle_Band  \\\n",
       "941                 353114.00    7.43    7.45        9.08         7.43   \n",
       "942                 269508.00    7.34    7.38        8.94         7.34   \n",
       "943                 233820.00    7.24    7.33        8.73         7.24   \n",
       "944                 206703.00    7.13    7.27        8.38         7.13   \n",
       "945                 271926.00    7.03    7.20        8.08         7.03   \n",
       "\n",
       "     Lower_Band   RSI  buy_1000x_high_coinbase  sell_1000x_high_coinbase  \\\n",
       "941        5.77 38.83                    21.00                     26.00   \n",
       "942        5.74 37.81                    29.00                     24.00   \n",
       "943        5.76 38.57                    17.00                     17.00   \n",
       "944        5.88 37.66                    16.00                     20.00   \n",
       "945        5.97 36.02                    69.00                     37.00   \n",
       "\n",
       "     total_trades_coinbase  Tweets_Utilizados  Tweets_Utilizados_coin  \\\n",
       "941               33468.00                151                     114   \n",
       "942               26619.00                117                     106   \n",
       "943               25565.00                101                     138   \n",
       "944               20954.00                 82                     106   \n",
       "945               33959.00                115                     125   \n",
       "\n",
       "     Tweets_Utilizados_referentes  Tweets_Utilizados_whale_alert  \\\n",
       "941                          0.00                          22.00   \n",
       "942                          0.00                          14.00   \n",
       "943                          0.00                           7.00   \n",
       "944                          0.00                          13.00   \n",
       "945                          0.00                          24.00   \n",
       "\n",
       "     Buy_1000x_high  sell_1000x_high  total_trades_binance Tendencia  \n",
       "941          242.00           219.00              48000.00   Lateral  \n",
       "942          292.00           324.00              42000.00   Lateral  \n",
       "943          248.00           179.00              41000.00   Lateral  \n",
       "944          173.00           165.00              26000.00   Lateral  \n",
       "945          260.00           188.00              41000.00   Bajista  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(classifier_dataset.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(946, 31)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = classifier_dataset.drop(columns=[\"Tendencia\", \"Open_time\"])\n",
    "y = classifier_dataset[\"Tendencia\"]\n",
    "\n",
    "# label_encoder = LabelEncoder()\n",
    "# y_encoded = label_encoder.fit_transform(classifier_dataset[\"Tendencia\"])\n",
    "\n",
    "y = y.to_numpy().reshape(-1, 1)\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "y_one_hot = onehot_encoder.fit_transform(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(y_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def create_model(activation, units, dropout, learning_rate, l2_penalty, depth, optimizer='adam'):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=int(units/2), activation=activation, input_shape=(len(X.columns), 1), return_sequences=True, kernel_regularizer=l2(l2_penalty)))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    for _ in range(depth - 1):\n",
    "        model.add(LSTM(units=units, activation=activation, return_sequences=True, kernel_regularizer=l2(l2_penalty)),)\n",
    "        model.add(Dropout(dropout))\n",
    "        model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(LSTM(units=int(units*2), activation=activation, kernel_regularizer=l2(l2_penalty)))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(units=3, activation='softmax'))\n",
    "    \n",
    "    if optimizer == 'adam':\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == 'rmsprop':\n",
    "        optimizer = RMSprop(learning_rate=learning_rate)\n",
    "    elif optimizer == 'sgd':\n",
    "        optimizer = SGD(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "regressor = KerasRegressor(build_fn=create_model, verbose=0, activation='relu', units=50, dropout=0.2, learning_rate=0.1, l2_penalty=0.001, depth=2, optimizer='adam')\n",
    "\n",
    "cv = TimeSeriesSplit(n_splits=10).split(X)\n",
    "param_space = {\n",
    "    'depth': [2, 3, 4, 5],\n",
    "    'activation': ['relu', 'tanh', 'swish', 'selu'],\n",
    "    'units': [64, 128, 256, 512],\n",
    "    'dropout': [0.1, 0.2, 0.3, 0.4],\n",
    "    'learning_rate': [0.01, 0.001, 0.0001],\n",
    "    'epochs': [10, 20, 30, 50, 100],\n",
    "    'batch_size': [32, 64, 128],\n",
    "    'optimizer': ['adam', 'rmsprop', 'sgd'],\n",
    "    'l2_penalty': [0.001, 0.01, 0.1]\n",
    "}\n",
    "\n",
    "def categorical_crossentropy_loss(estimator, X_test, y_test):\n",
    "    y_pred = estimator.predict(X_test)\n",
    "    \n",
    "    if np.isnan(y_pred).any():\n",
    "        y_pred[np.isnan(y_pred)] = 0\n",
    "    \n",
    "    loss = log_loss(y_test, y_pred)\n",
    "    return loss\n",
    "\n",
    "bayes_search = BayesSearchCV(regressor, param_space, scoring=categorical_crossentropy_loss, cv=cv, verbose=0)#10)\n",
    "bayes_result = bayes_search.fit(X, y_one_hot, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 5.120278119888486\n",
      "Best parameters: OrderedDict([('activation', 'selu'), ('batch_size', 64), ('depth', 4), ('dropout', 0.1), ('epochs', 10), ('l2_penalty', 0.001), ('learning_rate', 0.001), ('optimizer', 'rmsprop'), ('units', 512)])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KerasRegressor(\n",
       "\tmodel=None\n",
       "\tbuild_fn=&lt;function create_model at 0x7f8ba8b54040&gt;\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=rmsprop\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=64\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=0\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=10\n",
       "\tactivation=selu\n",
       "\tunits=512\n",
       "\tdropout=0.1\n",
       "\tlearning_rate=0.001\n",
       "\tl2_penalty=0.001\n",
       "\tdepth=4\n",
       ")</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KerasRegressor</label><div class=\"sk-toggleable__content\"><pre>KerasRegressor(\n",
       "\tmodel=None\n",
       "\tbuild_fn=&lt;function create_model at 0x7f8ba8b54040&gt;\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=rmsprop\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=64\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=0\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=10\n",
       "\tactivation=selu\n",
       "\tunits=512\n",
       "\tdropout=0.1\n",
       "\tlearning_rate=0.001\n",
       "\tl2_penalty=0.001\n",
       "\tdepth=4\n",
       ")</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KerasRegressor(\n",
       "\tmodel=None\n",
       "\tbuild_fn=<function create_model at 0x7f8ba8b54040>\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=rmsprop\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=64\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=0\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=10\n",
       "\tactivation=selu\n",
       "\tunits=512\n",
       "\tdropout=0.1\n",
       "\tlearning_rate=0.001\n",
       "\tl2_penalty=0.001\n",
       "\tdepth=4\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show best results\n",
    "print(\"Best score:\", bayes_result.best_score_)\n",
    "print(\"Best parameters:\", bayes_result.best_params_)\n",
    "\n",
    "# Entrenar el modelo con los mejores hiperparámetros\n",
    "best_model = bayes_result.best_estimator_\n",
    "best_model.fit(X, y_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 mejores modelos:\n",
      "Modelo 1\n",
      "Hiperparámetros: OrderedDict([('activation', 'relu'), ('batch_size', 64), ('depth', 4), ('dropout', 0.2), ('epochs', 30), ('l2_penalty', 0.01), ('learning_rate', 0.0001), ('optimizer', 'adam'), ('units', 64)])\n",
      "Puntaje: 1.157736949168703\n",
      "Modelo 2\n",
      "Hiperparámetros: OrderedDict([('activation', 'selu'), ('batch_size', 32), ('depth', 4), ('dropout', 0.4), ('epochs', 20), ('l2_penalty', 0.1), ('learning_rate', 0.01), ('optimizer', 'sgd'), ('units', 128)])\n",
      "Puntaje: 1.1129858996812934\n",
      "Modelo 3\n",
      "Hiperparámetros: OrderedDict([('activation', 'relu'), ('batch_size', 64), ('depth', 5), ('dropout', 0.4), ('epochs', 20), ('l2_penalty', 0.1), ('learning_rate', 0.0001), ('optimizer', 'sgd'), ('units', 64)])\n",
      "Puntaje: 1.1185720457866446\n",
      "Modelo 4\n",
      "Hiperparámetros: OrderedDict([('activation', 'relu'), ('batch_size', 128), ('depth', 3), ('dropout', 0.3), ('epochs', 50), ('l2_penalty', 0.1), ('learning_rate', 0.001), ('optimizer', 'sgd'), ('units', 512)])\n",
      "Puntaje: 2.2327541293501865\n",
      "Modelo 5\n",
      "Hiperparámetros: OrderedDict([('activation', 'selu'), ('batch_size', 32), ('depth', 4), ('dropout', 0.3), ('epochs', 100), ('l2_penalty', 0.01), ('learning_rate', 0.0001), ('optimizer', 'sgd'), ('units', 256)])\n",
      "Puntaje: 1.6436253941999612\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Obtener los hiperparámetros y puntajes de los 5 mejores modelos\n",
    "top_n_models = 5\n",
    "best_params_list = []\n",
    "best_scores_list = []\n",
    "\n",
    "for i in range(min(top_n_models, len(bayes_search.cv_results_['params']))):\n",
    "    best_params_list.append(bayes_search.cv_results_['params'][i])\n",
    "    best_scores_list.append(bayes_search.cv_results_['mean_test_score'][i])\n",
    "\n",
    "# Guardar los hiperparámetros de los 5 mejores modelos en un archivo JSON\n",
    "with open('lstm_classifier/top_5_hyperparameters.json', 'w') as f:\n",
    "    json.dump({'best_params': best_params_list, 'best_scores': best_scores_list}, f)\n",
    "\n",
    "# O imprimir los hiperparámetros\n",
    "print(\"Top 5 mejores modelos:\")\n",
    "for i in range(len(best_params_list)):\n",
    "    print(\"Modelo\", i+1)\n",
    "    print(\"Hiperparámetros:\", best_params_list[i])\n",
    "    print(\"Puntaje:\", best_scores_list[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Armado del ensamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prime_seeds(n):\n",
    "    seeds = []\n",
    "    num = 70001  # Comenzamos desde el primer número primo mayor que 70000\n",
    "    while len(seeds) < n:\n",
    "        is_prime = True\n",
    "        for i in range(2, int(num**0.5) + 1):\n",
    "            if num % i == 0:\n",
    "                is_prime = False\n",
    "                break\n",
    "        if is_prime:\n",
    "            seeds.append(num)\n",
    "        num += 1\n",
    "    return seeds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clase personalizada para hacer el ensamble, dado que sklearn no provee ninguna clase que permita hacer ensmble\n",
    "## de modelos re regresion multivariados\n",
    "class MultivariableVotingRegressor:\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for model in self.models:\n",
    "            model.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Hacer predicciones con cada modelo\n",
    "        predictions = np.array([model.predict(X) for model in self.models])\n",
    "        \n",
    "        # Calcular la moda de las predicciones\n",
    "        mode_predictions = np.argmax(np.sum(predictions, axis=0), axis=1)\n",
    "        \n",
    "        return mode_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "# Leer los hiperparámetros desde el archivo JSON\n",
    "with open('lstm_classifier/top_5_hyperparameters.json', 'r') as f:\n",
    "    top_hyperparameters = json.load(f)\n",
    "\n",
    "prime_seeds = generate_prime_seeds(30)\n",
    "models = []\n",
    "best_seeds= {}\n",
    "\n",
    "# Train models with different seeds for each set of hyperparameters\n",
    "for mode_number, params in enumerate(top_hyperparameters['best_params']):\n",
    "    best_validation_errors = {}\n",
    "    \n",
    "    for seed_number, seed in enumerate(prime_seeds):\n",
    "        model = KerasRegressor(build_fn=create_model, random_state=seed, verbose=1, **params)\n",
    "        model.fit(X, y_one_hot)\n",
    "        \n",
    "        # Make predictions with the model\n",
    "        model_predictions = model.predict(X)\n",
    "        \n",
    "        # Calculate error (training error)\n",
    "        train_error = categorical_crossentropy(y_one_hot, model_predictions)\n",
    "        \n",
    "        mean_train_error = np.mean(train_error)\n",
    "\n",
    "        # Update best validation error for this seed\n",
    "        best_validation_errors[seed] = mean_train_error\n",
    "    \n",
    "        print(f\"model number {mode_number}, seed number {seed_number}\")\n",
    "    # print(\"Best validation errors:\", best_validation_errors)\n",
    "\n",
    "    # Find the best seed for this set of hyperparameters\n",
    "    best_seed_for_params = min(best_validation_errors, key=lambda k: best_validation_errors[k])\n",
    "    best_seeds[str(params)] = best_seed_for_params\n",
    "    \n",
    "    # Create and train the model with the best seed\n",
    "    model = KerasRegressor(build_fn=create_model, random_state=best_seed_for_params, verbose=1, **params)\n",
    "    model.fit(X, y_one_hot)\n",
    "    models.append(model)\n",
    "\n",
    "# Define and train the ensemble model\n",
    "ensemble = MultivariableVotingRegressor(models)\n",
    "ensemble.fit(X, y_one_hot)\n",
    "\n",
    "# Save the best seeds to a JSON file\n",
    "with open('lstm_classifier/best_seeds.json', 'w') as f:\n",
    "    json.dump(best_seeds, f)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clasificacion con el ensamble sobre las redicciones de los modelos generativos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "15/15 [==============================] - 14s 113ms/step - loss: 5.3906 - accuracy: 0.3140\n",
      "Epoch 2/30\n",
      "15/15 [==============================] - 2s 113ms/step - loss: 5.2490 - accuracy: 0.3467\n",
      "Epoch 3/30\n",
      "15/15 [==============================] - 2s 127ms/step - loss: 5.1251 - accuracy: 0.3879\n",
      "Epoch 4/30\n",
      "15/15 [==============================] - 2s 133ms/step - loss: 5.1418 - accuracy: 0.3869\n",
      "Epoch 5/30\n",
      "15/15 [==============================] - 2s 147ms/step - loss: 5.0897 - accuracy: 0.3584\n",
      "Epoch 6/30\n",
      "15/15 [==============================] - 2s 145ms/step - loss: 5.0094 - accuracy: 0.4059\n",
      "Epoch 7/30\n",
      "15/15 [==============================] - 3s 180ms/step - loss: 5.0322 - accuracy: 0.3805\n",
      "Epoch 8/30\n",
      "15/15 [==============================] - 3s 181ms/step - loss: 4.9316 - accuracy: 0.3953\n",
      "Epoch 9/30\n",
      "15/15 [==============================] - 3s 208ms/step - loss: 4.9339 - accuracy: 0.3753\n",
      "Epoch 10/30\n",
      "15/15 [==============================] - 3s 209ms/step - loss: 4.8933 - accuracy: 0.4144\n",
      "Epoch 11/30\n",
      "15/15 [==============================] - 3s 220ms/step - loss: 4.8956 - accuracy: 0.4006\n",
      "Epoch 12/30\n",
      "15/15 [==============================] - 3s 236ms/step - loss: 4.8950 - accuracy: 0.3753\n",
      "Epoch 13/30\n",
      "15/15 [==============================] - 4s 234ms/step - loss: 4.8997 - accuracy: 0.3700\n",
      "Epoch 14/30\n",
      "15/15 [==============================] - 4s 264ms/step - loss: 4.7985 - accuracy: 0.3943\n",
      "Epoch 15/30\n",
      "15/15 [==============================] - 4s 258ms/step - loss: 4.8346 - accuracy: 0.3890\n",
      "Epoch 16/30\n",
      "15/15 [==============================] - 4s 273ms/step - loss: 4.8038 - accuracy: 0.3975\n",
      "Epoch 17/30\n",
      "15/15 [==============================] - 4s 283ms/step - loss: 4.7619 - accuracy: 0.4049\n",
      "Epoch 18/30\n",
      "15/15 [==============================] - 4s 263ms/step - loss: 4.7367 - accuracy: 0.4080\n",
      "Epoch 19/30\n",
      "15/15 [==============================] - 4s 298ms/step - loss: 4.7345 - accuracy: 0.3975\n",
      "Epoch 20/30\n",
      "15/15 [==============================] - 4s 254ms/step - loss: 4.7140 - accuracy: 0.4049\n",
      "Epoch 21/30\n",
      "15/15 [==============================] - 4s 287ms/step - loss: 4.6909 - accuracy: 0.3879\n",
      "Epoch 22/30\n",
      "15/15 [==============================] - 5s 312ms/step - loss: 4.6748 - accuracy: 0.4059\n",
      "Epoch 23/30\n",
      "15/15 [==============================] - 5s 317ms/step - loss: 4.6192 - accuracy: 0.4271\n",
      "Epoch 24/30\n",
      "15/15 [==============================] - 4s 265ms/step - loss: 4.6664 - accuracy: 0.4017\n",
      "Epoch 25/30\n",
      "15/15 [==============================] - 4s 264ms/step - loss: 4.6669 - accuracy: 0.3922\n",
      "Epoch 26/30\n",
      "15/15 [==============================] - 4s 239ms/step - loss: 4.6248 - accuracy: 0.4070\n",
      "Epoch 27/30\n",
      "15/15 [==============================] - 3s 225ms/step - loss: 4.6139 - accuracy: 0.3848\n",
      "Epoch 28/30\n",
      "15/15 [==============================] - 3s 235ms/step - loss: 4.6032 - accuracy: 0.3805\n",
      "Epoch 29/30\n",
      "15/15 [==============================] - 3s 184ms/step - loss: 4.5533 - accuracy: 0.4165\n",
      "Epoch 30/30\n",
      "15/15 [==============================] - 3s 175ms/step - loss: 4.5302 - accuracy: 0.3996\n",
      "Epoch 1/20\n",
      "30/30 [==============================] - 20s 313ms/step - loss: 79.8862 - accuracy: 0.3171\n",
      "Epoch 2/20\n",
      "30/30 [==============================] - 12s 411ms/step - loss: 125.2279 - accuracy: 0.3457\n",
      "Epoch 3/20\n",
      "30/30 [==============================] - 14s 463ms/step - loss: 129.5930 - accuracy: 0.3256\n",
      "Epoch 4/20\n",
      "30/30 [==============================] - 15s 485ms/step - loss: 129.3274 - accuracy: 0.3372\n",
      "Epoch 5/20\n",
      "30/30 [==============================] - 16s 515ms/step - loss: 128.7769 - accuracy: 0.3266\n",
      "Epoch 6/20\n",
      "30/30 [==============================] - 14s 479ms/step - loss: 128.1526 - accuracy: 0.3404\n",
      "Epoch 7/20\n",
      "30/30 [==============================] - 9s 298ms/step - loss: 127.4975 - accuracy: 0.3319\n",
      "Epoch 8/20\n",
      "30/30 [==============================] - 11s 366ms/step - loss: 126.7967 - accuracy: 0.3436\n",
      "Epoch 9/20\n",
      "30/30 [==============================] - 12s 401ms/step - loss: 126.0961 - accuracy: 0.3467\n",
      "Epoch 10/20\n",
      "30/30 [==============================] - 14s 462ms/step - loss: 125.3784 - accuracy: 0.3414\n",
      "Epoch 11/20\n",
      "30/30 [==============================] - 16s 532ms/step - loss: 124.6439 - accuracy: 0.3446\n",
      "Epoch 12/20\n",
      "30/30 [==============================] - 18s 604ms/step - loss: 123.9044 - accuracy: 0.3393\n",
      "Epoch 13/20\n",
      "30/30 [==============================] - 19s 617ms/step - loss: 123.1536 - accuracy: 0.3436\n",
      "Epoch 14/20\n",
      "30/30 [==============================] - 17s 561ms/step - loss: 122.3872 - accuracy: 0.3626\n",
      "Epoch 15/20\n",
      "30/30 [==============================] - 14s 475ms/step - loss: 121.6361 - accuracy: 0.3499\n",
      "Epoch 16/20\n",
      "30/30 [==============================] - 14s 453ms/step - loss: 120.8758 - accuracy: 0.3436\n",
      "Epoch 17/20\n",
      "30/30 [==============================] - 13s 433ms/step - loss: 120.1398 - accuracy: 0.3298\n",
      "Epoch 18/20\n",
      "30/30 [==============================] - 13s 420ms/step - loss: 119.4464 - accuracy: 0.3499\n",
      "Epoch 19/20\n",
      "30/30 [==============================] - 11s 380ms/step - loss: 118.6887 - accuracy: 0.3140\n",
      "Epoch 20/20\n",
      "30/30 [==============================] - 12s 416ms/step - loss: 117.9172 - accuracy: 0.3679\n",
      "Epoch 1/20\n",
      "15/15 [==============================] - 17s 355ms/step - loss: 49.1223 - accuracy: 0.3414\n",
      "Epoch 2/20\n",
      "15/15 [==============================] - 6s 385ms/step - loss: 48.1000 - accuracy: 0.3425\n",
      "Epoch 3/20\n",
      "15/15 [==============================] - 6s 398ms/step - loss: 46.9846 - accuracy: 0.3531\n",
      "Epoch 4/20\n",
      "15/15 [==============================] - 5s 343ms/step - loss: 46.0069 - accuracy: 0.3510\n",
      "Epoch 5/20\n",
      "15/15 [==============================] - 6s 413ms/step - loss: 44.9328 - accuracy: 0.3541\n",
      "Epoch 6/20\n",
      "15/15 [==============================] - 6s 400ms/step - loss: 43.9925 - accuracy: 0.3520\n",
      "Epoch 7/20\n",
      "15/15 [==============================] - 5s 358ms/step - loss: 43.0264 - accuracy: 0.3605\n",
      "Epoch 8/20\n",
      "15/15 [==============================] - 5s 324ms/step - loss: 42.0694 - accuracy: 0.3732\n",
      "Epoch 9/20\n",
      "15/15 [==============================] - 5s 354ms/step - loss: 41.1667 - accuracy: 0.3668\n",
      "Epoch 10/20\n",
      "15/15 [==============================] - 5s 361ms/step - loss: 40.2070 - accuracy: 0.3837\n",
      "Epoch 11/20\n",
      "15/15 [==============================] - 5s 328ms/step - loss: 39.3806 - accuracy: 0.3584\n",
      "Epoch 12/20\n",
      "15/15 [==============================] - 4s 301ms/step - loss: 38.6203 - accuracy: 0.3351\n",
      "Epoch 13/20\n",
      "15/15 [==============================] - 5s 315ms/step - loss: 37.7148 - accuracy: 0.3573\n",
      "Epoch 14/20\n",
      "15/15 [==============================] - 4s 276ms/step - loss: 36.8572 - accuracy: 0.3985\n",
      "Epoch 15/20\n",
      "15/15 [==============================] - 4s 252ms/step - loss: 36.0472 - accuracy: 0.3710\n",
      "Epoch 16/20\n",
      "15/15 [==============================] - 4s 270ms/step - loss: 35.2611 - accuracy: 0.3742\n",
      "Epoch 17/20\n",
      "15/15 [==============================] - 4s 289ms/step - loss: 34.5208 - accuracy: 0.3689\n",
      "Epoch 18/20\n",
      "15/15 [==============================] - 4s 272ms/step - loss: 33.7946 - accuracy: 0.3605\n",
      "Epoch 19/20\n",
      "15/15 [==============================] - 4s 298ms/step - loss: 33.0045 - accuracy: 0.3753\n",
      "Epoch 20/20\n",
      "15/15 [==============================] - 4s 289ms/step - loss: 32.3472 - accuracy: 0.3594\n",
      "Epoch 1/50\n",
      "8/8 [==============================] - 28s 3s/step - loss: 188.1955 - accuracy: 0.3562\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 20s 2s/step - loss: 122.8954 - accuracy: 0.3742\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 22s 3s/step - loss: 77.0287 - accuracy: 0.3858\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 23s 3s/step - loss: 46.7540 - accuracy: 0.3943\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 26s 3s/step - loss: 28.3023 - accuracy: 0.4154\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 23s 3s/step - loss: 17.5295 - accuracy: 0.4239\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 23s 3s/step - loss: 11.2099 - accuracy: 0.4366\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 25s 3s/step - loss: 7.5171 - accuracy: 0.4323\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 23s 3s/step - loss: 5.3310 - accuracy: 0.3753\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 21s 3s/step - loss: 3.9155 - accuracy: 0.4397\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 24s 3s/step - loss: 3.0527 - accuracy: 0.4313\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 21s 3s/step - loss: 2.4931 - accuracy: 0.4598\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 22s 3s/step - loss: 2.1422 - accuracy: 0.4397\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 23s 3s/step - loss: 1.8871 - accuracy: 0.4197\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 22s 3s/step - loss: 1.8352 - accuracy: 0.4101\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 22s 3s/step - loss: 2.0712 - accuracy: 0.3805\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 21s 3s/step - loss: 2.5120 - accuracy: 0.3171\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 23s 3s/step - loss: 2.4509 - accuracy: 0.3879\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 24s 3s/step - loss: 2.1256 - accuracy: 0.4440\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 23s 3s/step - loss: 1.8408 - accuracy: 0.4271\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 23s 3s/step - loss: 1.6019 - accuracy: 0.4123\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 24s 3s/step - loss: 1.4818 - accuracy: 0.4186\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 25s 3s/step - loss: 1.4727 - accuracy: 0.3985\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 22s 3s/step - loss: 1.5336 - accuracy: 0.4323\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 19s 2s/step - loss: 1.5272 - accuracy: 0.4260\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 19s 2s/step - loss: 1.4531 - accuracy: 0.4123\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 20s 2s/step - loss: 1.3820 - accuracy: 0.4376\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 20s 2s/step - loss: 1.3266 - accuracy: 0.4345\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 19s 2s/step - loss: 1.3019 - accuracy: 0.4027\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 19s 2s/step - loss: 1.2686 - accuracy: 0.4598\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 19s 2s/step - loss: 1.4605 - accuracy: 0.3256\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 20s 2s/step - loss: 1.5042 - accuracy: 0.3996\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 20s 2s/step - loss: 1.4755 - accuracy: 0.4027\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 20s 2s/step - loss: 1.3565 - accuracy: 0.4440\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 19s 2s/step - loss: 1.2991 - accuracy: 0.4323\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 20s 2s/step - loss: 1.2451 - accuracy: 0.4345\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 21s 3s/step - loss: 1.2425 - accuracy: 0.4366\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 20s 3s/step - loss: 1.2612 - accuracy: 0.4175\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 21s 3s/step - loss: 1.2923 - accuracy: 0.4366\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 22s 3s/step - loss: 1.3046 - accuracy: 0.4271\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 21s 3s/step - loss: 1.2980 - accuracy: 0.4207\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 19s 2s/step - loss: 1.3262 - accuracy: 0.4112\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 19s 2s/step - loss: 1.3101 - accuracy: 0.4302\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 20s 3s/step - loss: 1.2648 - accuracy: 0.4482\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 21s 3s/step - loss: 1.2420 - accuracy: 0.4471\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 22s 3s/step - loss: 1.2306 - accuracy: 0.4313\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 20s 3s/step - loss: 1.2425 - accuracy: 0.4249\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 19s 2s/step - loss: 1.2247 - accuracy: 0.4493\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 19s 2s/step - loss: 1.2349 - accuracy: 0.4376\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 21s 3s/step - loss: 1.2298 - accuracy: 0.4186\n",
      "Epoch 1/100\n",
      "30/30 [==============================] - 34s 791ms/step - loss: 16.4449 - accuracy: 0.3869\n",
      "Epoch 2/100\n",
      "30/30 [==============================] - 27s 891ms/step - loss: 16.1779 - accuracy: 0.3975\n",
      "Epoch 3/100\n",
      "30/30 [==============================] - 27s 912ms/step - loss: 15.9081 - accuracy: 0.4059\n",
      "Epoch 4/100\n",
      "30/30 [==============================] - 30s 984ms/step - loss: 15.7101 - accuracy: 0.3774\n",
      "Epoch 5/100\n",
      "30/30 [==============================] - 23s 768ms/step - loss: 15.4550 - accuracy: 0.4027\n",
      "Epoch 6/100\n",
      "30/30 [==============================] - 26s 865ms/step - loss: 15.2311 - accuracy: 0.4112\n",
      "Epoch 7/100\n",
      "30/30 [==============================] - 25s 815ms/step - loss: 15.0324 - accuracy: 0.3911\n",
      "Epoch 8/100\n",
      "30/30 [==============================] - 24s 798ms/step - loss: 14.8187 - accuracy: 0.3890\n",
      "Epoch 9/100\n",
      "30/30 [==============================] - 24s 796ms/step - loss: 14.5355 - accuracy: 0.4397\n",
      "Epoch 10/100\n",
      "30/30 [==============================] - 26s 857ms/step - loss: 14.3542 - accuracy: 0.4101\n",
      "Epoch 11/100\n",
      "30/30 [==============================] - 25s 830ms/step - loss: 14.1673 - accuracy: 0.3943\n",
      "Epoch 12/100\n",
      "30/30 [==============================] - 25s 843ms/step - loss: 13.9518 - accuracy: 0.4112\n",
      "Epoch 13/100\n",
      "30/30 [==============================] - 24s 789ms/step - loss: 13.6999 - accuracy: 0.4228\n",
      "Epoch 14/100\n",
      "30/30 [==============================] - 26s 874ms/step - loss: 13.5233 - accuracy: 0.4271\n",
      "Epoch 15/100\n",
      "30/30 [==============================] - 27s 919ms/step - loss: 13.3261 - accuracy: 0.4281\n",
      "Epoch 16/100\n",
      "30/30 [==============================] - 23s 772ms/step - loss: 13.1536 - accuracy: 0.4059\n",
      "Epoch 17/100\n",
      "30/30 [==============================] - 28s 932ms/step - loss: 12.9004 - accuracy: 0.4249\n",
      "Epoch 18/100\n",
      "30/30 [==============================] - 28s 926ms/step - loss: 12.7383 - accuracy: 0.3953\n",
      "Epoch 19/100\n",
      "30/30 [==============================] - 24s 808ms/step - loss: 12.4592 - accuracy: 0.4493\n",
      "Epoch 20/100\n",
      "30/30 [==============================] - 23s 759ms/step - loss: 12.4005 - accuracy: 0.4049\n",
      "Epoch 21/100\n",
      "30/30 [==============================] - 25s 831ms/step - loss: 12.1616 - accuracy: 0.4154\n",
      "Epoch 22/100\n",
      "30/30 [==============================] - 23s 784ms/step - loss: 11.9773 - accuracy: 0.4101\n",
      "Epoch 23/100\n",
      "30/30 [==============================] - 25s 815ms/step - loss: 11.8194 - accuracy: 0.3985\n",
      "Epoch 24/100\n",
      "30/30 [==============================] - 20s 673ms/step - loss: 11.6481 - accuracy: 0.3869\n",
      "Epoch 25/100\n",
      "30/30 [==============================] - 24s 816ms/step - loss: 11.4069 - accuracy: 0.4101\n",
      "Epoch 26/100\n",
      "30/30 [==============================] - 24s 808ms/step - loss: 11.2282 - accuracy: 0.3953\n",
      "Epoch 27/100\n",
      "30/30 [==============================] - 24s 796ms/step - loss: 11.0414 - accuracy: 0.4334\n",
      "Epoch 28/100\n",
      "30/30 [==============================] - 24s 792ms/step - loss: 10.8729 - accuracy: 0.4387\n",
      "Epoch 29/100\n",
      "30/30 [==============================] - 24s 800ms/step - loss: 10.7052 - accuracy: 0.4091\n",
      "Epoch 30/100\n",
      "30/30 [==============================] - 20s 653ms/step - loss: 10.5060 - accuracy: 0.4271\n",
      "Epoch 31/100\n",
      "30/30 [==============================] - 23s 773ms/step - loss: 10.3279 - accuracy: 0.4355\n",
      "Epoch 32/100\n",
      "30/30 [==============================] - 25s 854ms/step - loss: 10.2213 - accuracy: 0.4165\n",
      "Epoch 33/100\n",
      "30/30 [==============================] - 24s 805ms/step - loss: 10.0672 - accuracy: 0.4186\n",
      "Epoch 34/100\n",
      "30/30 [==============================] - 22s 748ms/step - loss: 9.8926 - accuracy: 0.4049\n",
      "Epoch 35/100\n",
      "30/30 [==============================] - 30s 991ms/step - loss: 9.7632 - accuracy: 0.4292\n",
      "Epoch 36/100\n",
      "30/30 [==============================] - 27s 899ms/step - loss: 9.5859 - accuracy: 0.4271\n",
      "Epoch 37/100\n",
      "30/30 [==============================] - 25s 841ms/step - loss: 9.4495 - accuracy: 0.4017\n",
      "Epoch 38/100\n",
      "30/30 [==============================] - 26s 871ms/step - loss: 9.2646 - accuracy: 0.4239\n",
      "Epoch 39/100\n",
      "30/30 [==============================] - 22s 728ms/step - loss: 9.1498 - accuracy: 0.4059\n",
      "Epoch 40/100\n",
      "30/30 [==============================] - 23s 758ms/step - loss: 8.9822 - accuracy: 0.4313\n",
      "Epoch 41/100\n",
      "30/30 [==============================] - 21s 702ms/step - loss: 8.8703 - accuracy: 0.4175\n",
      "Epoch 42/100\n",
      "30/30 [==============================] - 22s 754ms/step - loss: 8.6988 - accuracy: 0.4355\n",
      "Epoch 43/100\n",
      "30/30 [==============================] - 28s 953ms/step - loss: 8.5748 - accuracy: 0.4049\n",
      "Epoch 44/100\n",
      "30/30 [==============================] - 24s 779ms/step - loss: 8.4430 - accuracy: 0.4123\n",
      "Epoch 45/100\n",
      "30/30 [==============================] - 24s 816ms/step - loss: 8.3074 - accuracy: 0.4408\n",
      "Epoch 46/100\n",
      "30/30 [==============================] - 24s 786ms/step - loss: 8.1643 - accuracy: 0.4218\n",
      "Epoch 47/100\n",
      "30/30 [==============================] - 25s 851ms/step - loss: 8.0371 - accuracy: 0.4397\n",
      "Epoch 48/100\n",
      "30/30 [==============================] - 27s 898ms/step - loss: 7.9073 - accuracy: 0.4154\n",
      "Epoch 49/100\n",
      "30/30 [==============================] - 23s 742ms/step - loss: 7.7997 - accuracy: 0.4440\n",
      "Epoch 50/100\n",
      "30/30 [==============================] - 20s 650ms/step - loss: 7.6692 - accuracy: 0.4323\n",
      "Epoch 51/100\n",
      "30/30 [==============================] - 21s 718ms/step - loss: 7.5781 - accuracy: 0.4101\n",
      "Epoch 52/100\n",
      "30/30 [==============================] - 23s 773ms/step - loss: 7.4240 - accuracy: 0.4091\n",
      "Epoch 53/100\n",
      "30/30 [==============================] - 22s 730ms/step - loss: 7.3390 - accuracy: 0.4302\n",
      "Epoch 54/100\n",
      "30/30 [==============================] - 24s 801ms/step - loss: 7.1638 - accuracy: 0.4577\n",
      "Epoch 55/100\n",
      "30/30 [==============================] - 24s 795ms/step - loss: 7.0827 - accuracy: 0.4397\n",
      "Epoch 56/100\n",
      "30/30 [==============================] - 26s 859ms/step - loss: 7.0193 - accuracy: 0.4218\n",
      "Epoch 57/100\n",
      "30/30 [==============================] - 26s 873ms/step - loss: 6.8883 - accuracy: 0.4207\n",
      "Epoch 58/100\n",
      "30/30 [==============================] - 23s 753ms/step - loss: 6.7727 - accuracy: 0.4334\n",
      "Epoch 59/100\n",
      "30/30 [==============================] - 24s 817ms/step - loss: 6.6526 - accuracy: 0.4313\n",
      "Epoch 60/100\n",
      "30/30 [==============================] - 22s 747ms/step - loss: 6.5792 - accuracy: 0.4207\n",
      "Epoch 61/100\n",
      "30/30 [==============================] - 25s 843ms/step - loss: 6.4634 - accuracy: 0.4355\n",
      "Epoch 62/100\n",
      "30/30 [==============================] - 28s 946ms/step - loss: 6.3768 - accuracy: 0.4260\n",
      "Epoch 63/100\n",
      "30/30 [==============================] - 22s 726ms/step - loss: 6.2900 - accuracy: 0.3901\n",
      "Epoch 64/100\n",
      "30/30 [==============================] - 24s 807ms/step - loss: 6.1533 - accuracy: 0.4387\n",
      "Epoch 65/100\n",
      "30/30 [==============================] - 23s 758ms/step - loss: 6.0513 - accuracy: 0.4334\n",
      "Epoch 66/100\n",
      "30/30 [==============================] - 20s 674ms/step - loss: 5.9886 - accuracy: 0.4038\n",
      "Epoch 67/100\n",
      "30/30 [==============================] - 23s 784ms/step - loss: 5.9104 - accuracy: 0.4239\n",
      "Epoch 68/100\n",
      "30/30 [==============================] - 22s 741ms/step - loss: 5.8135 - accuracy: 0.4154\n",
      "Epoch 69/100\n",
      "30/30 [==============================] - 22s 739ms/step - loss: 5.7208 - accuracy: 0.4334\n",
      "Epoch 70/100\n",
      "30/30 [==============================] - 25s 834ms/step - loss: 5.6628 - accuracy: 0.4049\n",
      "Epoch 71/100\n",
      "30/30 [==============================] - 25s 843ms/step - loss: 5.5428 - accuracy: 0.4334\n",
      "Epoch 72/100\n",
      "30/30 [==============================] - 21s 713ms/step - loss: 5.4801 - accuracy: 0.3985\n",
      "Epoch 73/100\n",
      "30/30 [==============================] - 24s 812ms/step - loss: 5.4147 - accuracy: 0.4144\n",
      "Epoch 74/100\n",
      "30/30 [==============================] - 23s 752ms/step - loss: 5.3135 - accuracy: 0.4197\n",
      "Epoch 75/100\n",
      "30/30 [==============================] - 23s 761ms/step - loss: 5.2246 - accuracy: 0.4112\n",
      "Epoch 76/100\n",
      "30/30 [==============================] - 25s 833ms/step - loss: 5.1217 - accuracy: 0.4429\n",
      "Epoch 77/100\n",
      "30/30 [==============================] - 24s 793ms/step - loss: 5.0640 - accuracy: 0.4302\n",
      "Epoch 78/100\n",
      "30/30 [==============================] - 19s 631ms/step - loss: 4.9729 - accuracy: 0.4429\n",
      "Epoch 79/100\n",
      "30/30 [==============================] - 24s 797ms/step - loss: 4.9651 - accuracy: 0.4271\n",
      "Epoch 80/100\n",
      "30/30 [==============================] - 22s 724ms/step - loss: 4.8224 - accuracy: 0.4524\n",
      "Epoch 81/100\n",
      "30/30 [==============================] - 23s 761ms/step - loss: 4.8019 - accuracy: 0.4249\n",
      "Epoch 82/100\n",
      "30/30 [==============================] - 27s 896ms/step - loss: 4.7065 - accuracy: 0.4387\n",
      "Epoch 83/100\n",
      "30/30 [==============================] - 26s 852ms/step - loss: 4.6274 - accuracy: 0.4482\n",
      "Epoch 84/100\n",
      "30/30 [==============================] - 23s 780ms/step - loss: 4.6164 - accuracy: 0.4186\n",
      "Epoch 85/100\n",
      "30/30 [==============================] - 25s 854ms/step - loss: 4.5397 - accuracy: 0.4101\n",
      "Epoch 86/100\n",
      "30/30 [==============================] - 22s 741ms/step - loss: 4.4573 - accuracy: 0.4450\n",
      "Epoch 87/100\n",
      "30/30 [==============================] - 25s 840ms/step - loss: 4.3681 - accuracy: 0.4408\n",
      "Epoch 88/100\n",
      "30/30 [==============================] - 27s 899ms/step - loss: 4.3049 - accuracy: 0.4619\n",
      "Epoch 89/100\n",
      "30/30 [==============================] - 23s 770ms/step - loss: 4.2926 - accuracy: 0.4207\n",
      "Epoch 90/100\n",
      "30/30 [==============================] - 25s 837ms/step - loss: 4.2298 - accuracy: 0.4313\n",
      "Epoch 91/100\n",
      "30/30 [==============================] - 21s 709ms/step - loss: 4.1163 - accuracy: 0.4471\n",
      "Epoch 92/100\n",
      "30/30 [==============================] - 25s 836ms/step - loss: 4.1205 - accuracy: 0.4207\n",
      "Epoch 93/100\n",
      "30/30 [==============================] - 26s 851ms/step - loss: 4.0407 - accuracy: 0.4228\n",
      "Epoch 94/100\n",
      "30/30 [==============================] - 24s 793ms/step - loss: 3.9929 - accuracy: 0.4397\n",
      "Epoch 95/100\n",
      "30/30 [==============================] - 27s 916ms/step - loss: 3.9391 - accuracy: 0.4366\n",
      "Epoch 96/100\n",
      "30/30 [==============================] - 25s 831ms/step - loss: 3.8950 - accuracy: 0.4112\n",
      "Epoch 97/100\n",
      "30/30 [==============================] - 24s 788ms/step - loss: 3.8340 - accuracy: 0.4503\n",
      "Epoch 98/100\n",
      "30/30 [==============================] - 25s 829ms/step - loss: 3.7602 - accuracy: 0.4567\n",
      "Epoch 99/100\n",
      "30/30 [==============================] - 24s 787ms/step - loss: 3.6945 - accuracy: 0.4514\n",
      "Epoch 100/100\n",
      "30/30 [==============================] - 24s 801ms/step - loss: 3.6840 - accuracy: 0.4218\n"
     ]
    }
   ],
   "source": [
    "with open('lstm_classifier/best_seeds.json', 'r') as f:\n",
    "    best_seeds = json.load(f)\n",
    "\n",
    "# 21 Crear y entrenar los modelos con los hiperparámetros y semillas guardados\n",
    "models = []\n",
    "for params_str, seed in best_seeds.items():\n",
    "    params = json.loads(params_str.replace(\"'\", \"\\\"\"))\n",
    "    model = KerasRegressor(build_fn=create_model, random_state=seed, **params)\n",
    "    model.fit(X, y_one_hot)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "15/15 [==============================] - 13s 199ms/step - loss: 5.3906 - accuracy: 0.3140\n",
      "Epoch 2/30\n",
      "15/15 [==============================] - 3s 185ms/step - loss: 5.2490 - accuracy: 0.3467\n",
      "Epoch 3/30\n",
      "15/15 [==============================] - 3s 199ms/step - loss: 5.1251 - accuracy: 0.3879\n",
      "Epoch 4/30\n",
      "15/15 [==============================] - 2s 162ms/step - loss: 5.1418 - accuracy: 0.3869\n",
      "Epoch 5/30\n",
      "15/15 [==============================] - 3s 186ms/step - loss: 5.0897 - accuracy: 0.3584\n",
      "Epoch 6/30\n",
      "15/15 [==============================] - 3s 180ms/step - loss: 5.0094 - accuracy: 0.4059\n",
      "Epoch 7/30\n",
      "15/15 [==============================] - 3s 169ms/step - loss: 5.0322 - accuracy: 0.3805\n",
      "Epoch 8/30\n",
      "15/15 [==============================] - 3s 223ms/step - loss: 4.9316 - accuracy: 0.3953\n",
      "Epoch 9/30\n",
      "15/15 [==============================] - 4s 238ms/step - loss: 4.9339 - accuracy: 0.3753\n",
      "Epoch 10/30\n",
      "15/15 [==============================] - 3s 216ms/step - loss: 4.8933 - accuracy: 0.4144\n",
      "Epoch 11/30\n",
      "15/15 [==============================] - 3s 198ms/step - loss: 4.8956 - accuracy: 0.4006\n",
      "Epoch 12/30\n",
      "15/15 [==============================] - 3s 200ms/step - loss: 4.8950 - accuracy: 0.3753\n",
      "Epoch 13/30\n",
      "15/15 [==============================] - 3s 215ms/step - loss: 4.8997 - accuracy: 0.3700\n",
      "Epoch 14/30\n",
      "15/15 [==============================] - 3s 191ms/step - loss: 4.7985 - accuracy: 0.3943\n",
      "Epoch 15/30\n",
      "15/15 [==============================] - 3s 185ms/step - loss: 4.8346 - accuracy: 0.3890\n",
      "Epoch 16/30\n",
      "15/15 [==============================] - 3s 189ms/step - loss: 4.8038 - accuracy: 0.3975\n",
      "Epoch 17/30\n",
      "15/15 [==============================] - 3s 201ms/step - loss: 4.7619 - accuracy: 0.4049\n",
      "Epoch 18/30\n",
      "15/15 [==============================] - 3s 232ms/step - loss: 4.7367 - accuracy: 0.4080\n",
      "Epoch 19/30\n",
      "15/15 [==============================] - 3s 211ms/step - loss: 4.7345 - accuracy: 0.3975\n",
      "Epoch 20/30\n",
      "15/15 [==============================] - 3s 193ms/step - loss: 4.7140 - accuracy: 0.4049\n",
      "Epoch 21/30\n",
      "15/15 [==============================] - 3s 194ms/step - loss: 4.6909 - accuracy: 0.3879\n",
      "Epoch 22/30\n",
      "15/15 [==============================] - 3s 186ms/step - loss: 4.6748 - accuracy: 0.4059\n",
      "Epoch 23/30\n",
      "15/15 [==============================] - 3s 198ms/step - loss: 4.6192 - accuracy: 0.4271\n",
      "Epoch 24/30\n",
      "15/15 [==============================] - 3s 214ms/step - loss: 4.6664 - accuracy: 0.4017\n",
      "Epoch 25/30\n",
      "15/15 [==============================] - 3s 220ms/step - loss: 4.6669 - accuracy: 0.3922\n",
      "Epoch 26/30\n",
      "15/15 [==============================] - 3s 215ms/step - loss: 4.6248 - accuracy: 0.4070\n",
      "Epoch 27/30\n",
      "15/15 [==============================] - 3s 200ms/step - loss: 4.6139 - accuracy: 0.3848\n",
      "Epoch 28/30\n",
      "15/15 [==============================] - 3s 180ms/step - loss: 4.6032 - accuracy: 0.3805\n",
      "Epoch 29/30\n",
      "15/15 [==============================] - 3s 176ms/step - loss: 4.5533 - accuracy: 0.4165\n",
      "Epoch 30/30\n",
      "15/15 [==============================] - 3s 183ms/step - loss: 4.5302 - accuracy: 0.3996\n",
      "Epoch 1/20\n",
      "30/30 [==============================] - 19s 328ms/step - loss: 79.8862 - accuracy: 0.3171\n",
      "Epoch 2/20\n",
      "30/30 [==============================] - 10s 341ms/step - loss: 125.2279 - accuracy: 0.3457\n",
      "Epoch 3/20\n",
      "30/30 [==============================] - 11s 359ms/step - loss: 129.5930 - accuracy: 0.3256\n",
      "Epoch 4/20\n",
      "30/30 [==============================] - 10s 313ms/step - loss: 129.3274 - accuracy: 0.3372\n",
      "Epoch 5/20\n",
      "30/30 [==============================] - 8s 282ms/step - loss: 128.7769 - accuracy: 0.3266\n",
      "Epoch 6/20\n",
      "30/30 [==============================] - 9s 305ms/step - loss: 128.1526 - accuracy: 0.3404\n",
      "Epoch 7/20\n",
      "30/30 [==============================] - 11s 353ms/step - loss: 127.4975 - accuracy: 0.3319\n",
      "Epoch 8/20\n",
      "30/30 [==============================] - 9s 297ms/step - loss: 126.7967 - accuracy: 0.3436\n",
      "Epoch 9/20\n",
      "30/30 [==============================] - 10s 350ms/step - loss: 126.0961 - accuracy: 0.3467\n",
      "Epoch 10/20\n",
      "30/30 [==============================] - 9s 292ms/step - loss: 125.3784 - accuracy: 0.3414\n",
      "Epoch 11/20\n",
      "30/30 [==============================] - 8s 266ms/step - loss: 124.6439 - accuracy: 0.3446\n",
      "Epoch 12/20\n",
      "30/30 [==============================] - 8s 257ms/step - loss: 123.9044 - accuracy: 0.3393\n",
      "Epoch 13/20\n",
      "30/30 [==============================] - 9s 292ms/step - loss: 123.1536 - accuracy: 0.3436\n",
      "Epoch 14/20\n",
      "30/30 [==============================] - 8s 279ms/step - loss: 122.3872 - accuracy: 0.3626\n",
      "Epoch 15/20\n",
      "30/30 [==============================] - 9s 285ms/step - loss: 121.6361 - accuracy: 0.3499\n",
      "Epoch 16/20\n",
      "30/30 [==============================] - 9s 284ms/step - loss: 120.8758 - accuracy: 0.3436\n",
      "Epoch 17/20\n",
      "30/30 [==============================] - 9s 290ms/step - loss: 120.1398 - accuracy: 0.3298\n",
      "Epoch 18/20\n",
      "30/30 [==============================] - 9s 287ms/step - loss: 119.4464 - accuracy: 0.3499\n",
      "Epoch 19/20\n",
      "30/30 [==============================] - 9s 299ms/step - loss: 118.6887 - accuracy: 0.3140\n",
      "Epoch 20/20\n",
      "30/30 [==============================] - 9s 293ms/step - loss: 117.9172 - accuracy: 0.3679\n",
      "Epoch 1/20\n",
      "15/15 [==============================] - 15s 154ms/step - loss: 49.1223 - accuracy: 0.3414\n",
      "Epoch 2/20\n",
      "15/15 [==============================] - 2s 149ms/step - loss: 48.1000 - accuracy: 0.3425\n",
      "Epoch 3/20\n",
      "15/15 [==============================] - 2s 162ms/step - loss: 46.9846 - accuracy: 0.3531\n",
      "Epoch 4/20\n",
      "15/15 [==============================] - 2s 151ms/step - loss: 46.0069 - accuracy: 0.3510\n",
      "Epoch 5/20\n",
      "15/15 [==============================] - 3s 173ms/step - loss: 44.9328 - accuracy: 0.3541\n",
      "Epoch 6/20\n",
      "15/15 [==============================] - 2s 161ms/step - loss: 43.9925 - accuracy: 0.3520\n",
      "Epoch 7/20\n",
      "15/15 [==============================] - 3s 167ms/step - loss: 43.0264 - accuracy: 0.3605\n",
      "Epoch 8/20\n",
      "15/15 [==============================] - 3s 169ms/step - loss: 42.0694 - accuracy: 0.3732\n",
      "Epoch 9/20\n",
      "15/15 [==============================] - 3s 178ms/step - loss: 41.1667 - accuracy: 0.3668\n",
      "Epoch 10/20\n",
      "15/15 [==============================] - 3s 179ms/step - loss: 40.2070 - accuracy: 0.3837\n",
      "Epoch 11/20\n",
      "15/15 [==============================] - 3s 184ms/step - loss: 39.3806 - accuracy: 0.3584\n",
      "Epoch 12/20\n",
      "15/15 [==============================] - 3s 192ms/step - loss: 38.6203 - accuracy: 0.3351\n",
      "Epoch 13/20\n",
      "15/15 [==============================] - 3s 196ms/step - loss: 37.7148 - accuracy: 0.3573\n",
      "Epoch 14/20\n",
      "15/15 [==============================] - 3s 207ms/step - loss: 36.8572 - accuracy: 0.3985\n",
      "Epoch 15/20\n",
      "15/15 [==============================] - 3s 188ms/step - loss: 36.0472 - accuracy: 0.3710\n",
      "Epoch 16/20\n",
      "15/15 [==============================] - 3s 177ms/step - loss: 35.2611 - accuracy: 0.3742\n",
      "Epoch 17/20\n",
      "15/15 [==============================] - 3s 184ms/step - loss: 34.5208 - accuracy: 0.3689\n",
      "Epoch 18/20\n",
      "15/15 [==============================] - 3s 178ms/step - loss: 33.7946 - accuracy: 0.3605\n",
      "Epoch 19/20\n",
      "15/15 [==============================] - 3s 191ms/step - loss: 33.0045 - accuracy: 0.3753\n",
      "Epoch 20/20\n",
      "15/15 [==============================] - 2s 158ms/step - loss: 32.3472 - accuracy: 0.3594\n",
      "Epoch 1/50\n",
      "8/8 [==============================] - 26s 2s/step - loss: 188.1955 - accuracy: 0.3562\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 19s 2s/step - loss: 122.8954 - accuracy: 0.3742\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 19s 2s/step - loss: 77.0287 - accuracy: 0.3858\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 19s 2s/step - loss: 46.7540 - accuracy: 0.3943\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 19s 2s/step - loss: 28.3023 - accuracy: 0.4154\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 19s 2s/step - loss: 17.5295 - accuracy: 0.4239\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 20s 2s/step - loss: 11.2099 - accuracy: 0.4366\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 21s 3s/step - loss: 7.5171 - accuracy: 0.4323\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 19s 2s/step - loss: 5.3310 - accuracy: 0.3753\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 21s 3s/step - loss: 3.9155 - accuracy: 0.4397\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 22s 3s/step - loss: 3.0527 - accuracy: 0.4313\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 20s 2s/step - loss: 2.4931 - accuracy: 0.4598\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 22s 3s/step - loss: 2.1422 - accuracy: 0.4397\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 18s 2s/step - loss: 1.8871 - accuracy: 0.4197\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 17s 2s/step - loss: 1.8352 - accuracy: 0.4101\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 18s 2s/step - loss: 2.0712 - accuracy: 0.3805\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 19s 2s/step - loss: 2.5120 - accuracy: 0.3171\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 20s 2s/step - loss: 2.4509 - accuracy: 0.3879\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 19s 2s/step - loss: 2.1256 - accuracy: 0.4440\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 18s 2s/step - loss: 1.8408 - accuracy: 0.4271\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 17s 2s/step - loss: 1.6019 - accuracy: 0.4123\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 17s 2s/step - loss: 1.4818 - accuracy: 0.4186\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 19s 2s/step - loss: 1.4727 - accuracy: 0.3985\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 18s 2s/step - loss: 1.5336 - accuracy: 0.4323\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 17s 2s/step - loss: 1.5272 - accuracy: 0.4260\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 18s 2s/step - loss: 1.4531 - accuracy: 0.4123\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 19s 2s/step - loss: 1.3820 - accuracy: 0.4376\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 18s 2s/step - loss: 1.3266 - accuracy: 0.4345\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 18s 2s/step - loss: 1.3019 - accuracy: 0.4027\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 17s 2s/step - loss: 1.2686 - accuracy: 0.4598\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 17s 2s/step - loss: 1.4605 - accuracy: 0.3256\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 17s 2s/step - loss: 1.5042 - accuracy: 0.3996\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 17s 2s/step - loss: 1.4755 - accuracy: 0.4027\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 17s 2s/step - loss: 1.3565 - accuracy: 0.4440\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 18s 2s/step - loss: 1.2991 - accuracy: 0.4323\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 18s 2s/step - loss: 1.2451 - accuracy: 0.4345\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 18s 2s/step - loss: 1.2425 - accuracy: 0.4366\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 18s 2s/step - loss: 1.2612 - accuracy: 0.4175\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 18s 2s/step - loss: 1.2923 - accuracy: 0.4366\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 18s 2s/step - loss: 1.3046 - accuracy: 0.4271\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 17s 2s/step - loss: 1.2980 - accuracy: 0.4207\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 19s 2s/step - loss: 1.3262 - accuracy: 0.4112\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 18s 2s/step - loss: 1.3101 - accuracy: 0.4302\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 17s 2s/step - loss: 1.2648 - accuracy: 0.4482\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 17s 2s/step - loss: 1.2420 - accuracy: 0.4471\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 18s 2s/step - loss: 1.2306 - accuracy: 0.4313\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 17s 2s/step - loss: 1.2425 - accuracy: 0.4249\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 17s 2s/step - loss: 1.2247 - accuracy: 0.4493\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 17s 2s/step - loss: 1.2349 - accuracy: 0.4376\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 18s 2s/step - loss: 1.2298 - accuracy: 0.4186\n",
      "Epoch 1/100\n",
      "30/30 [==============================] - 29s 665ms/step - loss: 16.4449 - accuracy: 0.3869\n",
      "Epoch 2/100\n",
      "30/30 [==============================] - 23s 784ms/step - loss: 16.1779 - accuracy: 0.3975\n",
      "Epoch 3/100\n",
      "30/30 [==============================] - 24s 794ms/step - loss: 15.9081 - accuracy: 0.4059\n",
      "Epoch 4/100\n",
      "30/30 [==============================] - 20s 675ms/step - loss: 15.7101 - accuracy: 0.3774\n",
      "Epoch 5/100\n",
      "30/30 [==============================] - 24s 788ms/step - loss: 15.4550 - accuracy: 0.4027\n",
      "Epoch 6/100\n",
      "30/30 [==============================] - 23s 770ms/step - loss: 15.2311 - accuracy: 0.4112\n",
      "Epoch 7/100\n",
      "30/30 [==============================] - 22s 738ms/step - loss: 15.0324 - accuracy: 0.3911\n",
      "Epoch 8/100\n",
      "30/30 [==============================] - 22s 743ms/step - loss: 14.8187 - accuracy: 0.3890\n",
      "Epoch 9/100\n",
      "30/30 [==============================] - 21s 715ms/step - loss: 14.5355 - accuracy: 0.4397\n",
      "Epoch 10/100\n",
      "30/30 [==============================] - 22s 736ms/step - loss: 14.3542 - accuracy: 0.4101\n",
      "Epoch 11/100\n",
      "30/30 [==============================] - 22s 740ms/step - loss: 14.1673 - accuracy: 0.3943\n",
      "Epoch 12/100\n",
      "30/30 [==============================] - 23s 760ms/step - loss: 13.9518 - accuracy: 0.4112\n",
      "Epoch 13/100\n",
      "30/30 [==============================] - 22s 733ms/step - loss: 13.6999 - accuracy: 0.4228\n",
      "Epoch 14/100\n",
      "30/30 [==============================] - 23s 766ms/step - loss: 13.5233 - accuracy: 0.4271\n",
      "Epoch 15/100\n",
      "30/30 [==============================] - 23s 768ms/step - loss: 13.3261 - accuracy: 0.4281\n",
      "Epoch 16/100\n",
      "30/30 [==============================] - 19s 629ms/step - loss: 13.1536 - accuracy: 0.4059\n",
      "Epoch 17/100\n",
      "30/30 [==============================] - 21s 711ms/step - loss: 12.9004 - accuracy: 0.4249\n",
      "Epoch 18/100\n",
      "30/30 [==============================] - 21s 706ms/step - loss: 12.7383 - accuracy: 0.3953\n",
      "Epoch 19/100\n",
      "30/30 [==============================] - 21s 719ms/step - loss: 12.4592 - accuracy: 0.4493\n",
      "Epoch 20/100\n",
      "30/30 [==============================] - 19s 633ms/step - loss: 12.4005 - accuracy: 0.4049\n",
      "Epoch 21/100\n",
      "30/30 [==============================] - 19s 631ms/step - loss: 12.1616 - accuracy: 0.4154\n",
      "Epoch 22/100\n",
      "30/30 [==============================] - 20s 665ms/step - loss: 11.9773 - accuracy: 0.4101\n",
      "Epoch 23/100\n",
      "30/30 [==============================] - 19s 614ms/step - loss: 11.8194 - accuracy: 0.3985\n",
      "Epoch 24/100\n",
      "30/30 [==============================] - 20s 672ms/step - loss: 11.6481 - accuracy: 0.3869\n",
      "Epoch 25/100\n",
      "30/30 [==============================] - 22s 731ms/step - loss: 11.4069 - accuracy: 0.4101\n",
      "Epoch 26/100\n",
      "30/30 [==============================] - 20s 665ms/step - loss: 11.2282 - accuracy: 0.3953\n",
      "Epoch 27/100\n",
      "30/30 [==============================] - 19s 618ms/step - loss: 11.0414 - accuracy: 0.4334\n",
      "Epoch 28/100\n",
      "30/30 [==============================] - 20s 651ms/step - loss: 10.8729 - accuracy: 0.4387\n",
      "Epoch 29/100\n",
      "30/30 [==============================] - 21s 700ms/step - loss: 10.7052 - accuracy: 0.4091\n",
      "Epoch 30/100\n",
      "30/30 [==============================] - 18s 615ms/step - loss: 10.5060 - accuracy: 0.4271\n",
      "Epoch 31/100\n",
      "30/30 [==============================] - 21s 704ms/step - loss: 10.3279 - accuracy: 0.4355\n",
      "Epoch 32/100\n",
      "30/30 [==============================] - 19s 639ms/step - loss: 10.2213 - accuracy: 0.4165\n",
      "Epoch 33/100\n",
      "30/30 [==============================] - 20s 674ms/step - loss: 10.0672 - accuracy: 0.4186\n",
      "Epoch 34/100\n",
      "30/30 [==============================] - 19s 636ms/step - loss: 9.8926 - accuracy: 0.4049\n",
      "Epoch 35/100\n",
      "30/30 [==============================] - 21s 693ms/step - loss: 9.7632 - accuracy: 0.4292\n",
      "Epoch 36/100\n",
      "30/30 [==============================] - 21s 709ms/step - loss: 9.5859 - accuracy: 0.4271\n",
      "Epoch 37/100\n",
      "30/30 [==============================] - 18s 588ms/step - loss: 9.4495 - accuracy: 0.4017\n",
      "Epoch 38/100\n",
      "30/30 [==============================] - 20s 668ms/step - loss: 9.2646 - accuracy: 0.4239\n",
      "Epoch 39/100\n",
      "30/30 [==============================] - 19s 613ms/step - loss: 9.1498 - accuracy: 0.4059\n",
      "Epoch 40/100\n",
      "30/30 [==============================] - 21s 704ms/step - loss: 8.9822 - accuracy: 0.4313\n",
      "Epoch 41/100\n",
      "30/30 [==============================] - 19s 643ms/step - loss: 8.8703 - accuracy: 0.4175\n",
      "Epoch 42/100\n",
      "30/30 [==============================] - 18s 607ms/step - loss: 8.6988 - accuracy: 0.4355\n",
      "Epoch 43/100\n",
      "30/30 [==============================] - 17s 553ms/step - loss: 8.5748 - accuracy: 0.4049\n",
      "Epoch 44/100\n",
      "30/30 [==============================] - 19s 636ms/step - loss: 8.4430 - accuracy: 0.4123\n",
      "Epoch 45/100\n",
      "30/30 [==============================] - 21s 699ms/step - loss: 8.3074 - accuracy: 0.4408\n",
      "Epoch 46/100\n",
      "30/30 [==============================] - 18s 580ms/step - loss: 8.1643 - accuracy: 0.4218\n",
      "Epoch 47/100\n",
      "30/30 [==============================] - 19s 618ms/step - loss: 8.0371 - accuracy: 0.4397\n",
      "Epoch 48/100\n",
      "30/30 [==============================] - 18s 602ms/step - loss: 7.9073 - accuracy: 0.4154\n",
      "Epoch 49/100\n",
      "30/30 [==============================] - 17s 556ms/step - loss: 7.7997 - accuracy: 0.4440\n",
      "Epoch 50/100\n",
      "30/30 [==============================] - 22s 737ms/step - loss: 7.6692 - accuracy: 0.4323\n",
      "Epoch 51/100\n",
      "30/30 [==============================] - 19s 645ms/step - loss: 7.5781 - accuracy: 0.4101\n",
      "Epoch 52/100\n",
      "30/30 [==============================] - 18s 605ms/step - loss: 7.4240 - accuracy: 0.4091\n",
      "Epoch 53/100\n",
      "30/30 [==============================] - 17s 568ms/step - loss: 7.3390 - accuracy: 0.4302\n",
      "Epoch 54/100\n",
      "30/30 [==============================] - 18s 607ms/step - loss: 7.1638 - accuracy: 0.4577\n",
      "Epoch 55/100\n",
      "30/30 [==============================] - 19s 638ms/step - loss: 7.0827 - accuracy: 0.4397\n",
      "Epoch 56/100\n",
      "30/30 [==============================] - 19s 617ms/step - loss: 7.0193 - accuracy: 0.4218\n",
      "Epoch 57/100\n",
      "30/30 [==============================] - 19s 638ms/step - loss: 6.8883 - accuracy: 0.4207\n",
      "Epoch 58/100\n",
      "30/30 [==============================] - 19s 641ms/step - loss: 6.7727 - accuracy: 0.4334\n",
      "Epoch 59/100\n",
      "30/30 [==============================] - 20s 661ms/step - loss: 6.6526 - accuracy: 0.4313\n",
      "Epoch 60/100\n",
      "30/30 [==============================] - 17s 578ms/step - loss: 6.5792 - accuracy: 0.4207\n",
      "Epoch 61/100\n",
      "30/30 [==============================] - 19s 647ms/step - loss: 6.4634 - accuracy: 0.4355\n",
      "Epoch 62/100\n",
      "30/30 [==============================] - 21s 696ms/step - loss: 6.3768 - accuracy: 0.4260\n",
      "Epoch 63/100\n",
      "30/30 [==============================] - 18s 616ms/step - loss: 6.2900 - accuracy: 0.3901\n",
      "Epoch 64/100\n",
      "30/30 [==============================] - 19s 628ms/step - loss: 6.1533 - accuracy: 0.4387\n",
      "Epoch 65/100\n",
      "30/30 [==============================] - 20s 667ms/step - loss: 6.0513 - accuracy: 0.4334\n",
      "Epoch 66/100\n",
      "30/30 [==============================] - 19s 622ms/step - loss: 5.9886 - accuracy: 0.4038\n",
      "Epoch 67/100\n",
      "30/30 [==============================] - 18s 611ms/step - loss: 5.9104 - accuracy: 0.4239\n",
      "Epoch 68/100\n",
      "30/30 [==============================] - 23s 752ms/step - loss: 5.8135 - accuracy: 0.4154\n",
      "Epoch 69/100\n",
      "30/30 [==============================] - 18s 609ms/step - loss: 5.7208 - accuracy: 0.4334\n",
      "Epoch 70/100\n",
      "30/30 [==============================] - 19s 631ms/step - loss: 5.6628 - accuracy: 0.4049\n",
      "Epoch 71/100\n",
      "30/30 [==============================] - 21s 686ms/step - loss: 5.5428 - accuracy: 0.4334\n",
      "Epoch 72/100\n",
      "30/30 [==============================] - 19s 621ms/step - loss: 5.4801 - accuracy: 0.3985\n",
      "Epoch 73/100\n",
      "30/30 [==============================] - 20s 651ms/step - loss: 5.4147 - accuracy: 0.4144\n",
      "Epoch 74/100\n",
      "30/30 [==============================] - 18s 585ms/step - loss: 5.3135 - accuracy: 0.4197\n",
      "Epoch 75/100\n",
      "30/30 [==============================] - 19s 651ms/step - loss: 5.2246 - accuracy: 0.4112\n",
      "Epoch 76/100\n",
      "30/30 [==============================] - 20s 649ms/step - loss: 5.1217 - accuracy: 0.4429\n",
      "Epoch 77/100\n",
      "30/30 [==============================] - 20s 662ms/step - loss: 5.0640 - accuracy: 0.4302\n",
      "Epoch 78/100\n",
      "30/30 [==============================] - 18s 596ms/step - loss: 4.9729 - accuracy: 0.4429\n",
      "Epoch 79/100\n",
      "30/30 [==============================] - 18s 598ms/step - loss: 4.9651 - accuracy: 0.4271\n",
      "Epoch 80/100\n",
      "30/30 [==============================] - 20s 679ms/step - loss: 4.8224 - accuracy: 0.4524\n",
      "Epoch 81/100\n",
      "30/30 [==============================] - 20s 670ms/step - loss: 4.8019 - accuracy: 0.4249\n",
      "Epoch 82/100\n",
      "30/30 [==============================] - 19s 646ms/step - loss: 4.7065 - accuracy: 0.4387\n",
      "Epoch 83/100\n",
      "30/30 [==============================] - 18s 613ms/step - loss: 4.6274 - accuracy: 0.4482\n",
      "Epoch 84/100\n",
      "30/30 [==============================] - 19s 627ms/step - loss: 4.6164 - accuracy: 0.4186\n",
      "Epoch 85/100\n",
      "30/30 [==============================] - 16s 527ms/step - loss: 4.5397 - accuracy: 0.4101\n",
      "Epoch 86/100\n",
      "30/30 [==============================] - 19s 626ms/step - loss: 4.4573 - accuracy: 0.4450\n",
      "Epoch 87/100\n",
      "30/30 [==============================] - 21s 691ms/step - loss: 4.3681 - accuracy: 0.4408\n",
      "Epoch 88/100\n",
      "30/30 [==============================] - 18s 589ms/step - loss: 4.3049 - accuracy: 0.4619\n",
      "Epoch 89/100\n",
      "30/30 [==============================] - 17s 566ms/step - loss: 4.2926 - accuracy: 0.4207\n",
      "Epoch 90/100\n",
      "30/30 [==============================] - 18s 593ms/step - loss: 4.2298 - accuracy: 0.4313\n",
      "Epoch 91/100\n",
      "30/30 [==============================] - 17s 552ms/step - loss: 4.1163 - accuracy: 0.4471\n",
      "Epoch 92/100\n",
      "30/30 [==============================] - 19s 640ms/step - loss: 4.1205 - accuracy: 0.4207\n",
      "Epoch 93/100\n",
      "30/30 [==============================] - 19s 616ms/step - loss: 4.0407 - accuracy: 0.4228\n",
      "Epoch 94/100\n",
      "30/30 [==============================] - 19s 630ms/step - loss: 3.9929 - accuracy: 0.4397\n",
      "Epoch 95/100\n",
      "30/30 [==============================] - 19s 616ms/step - loss: 3.9391 - accuracy: 0.4366\n",
      "Epoch 96/100\n",
      "30/30 [==============================] - 19s 650ms/step - loss: 3.8950 - accuracy: 0.4112\n",
      "Epoch 97/100\n",
      "30/30 [==============================] - 20s 662ms/step - loss: 3.8340 - accuracy: 0.4503\n",
      "Epoch 98/100\n",
      "30/30 [==============================] - 21s 697ms/step - loss: 3.7602 - accuracy: 0.4567\n",
      "Epoch 99/100\n",
      "30/30 [==============================] - 24s 792ms/step - loss: 3.6945 - accuracy: 0.4514\n",
      "Epoch 100/100\n",
      "30/30 [==============================] - 24s 779ms/step - loss: 3.6840 - accuracy: 0.4218\n"
     ]
    }
   ],
   "source": [
    "ensemble = MultivariableVotingRegressor(models)\n",
    "ensemble.fit(X, y_one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Datos originales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open_time</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Number of trades</th>\n",
       "      <th>Close_BTCUSDT</th>\n",
       "      <th>Volume_BTCUSDT</th>\n",
       "      <th>Number_of_trades_BTCUSDT</th>\n",
       "      <th>Close_ETHUSDT</th>\n",
       "      <th>Volume_ETHUSDT</th>\n",
       "      <th>Number_of_trades_ETHUSDT</th>\n",
       "      <th>Close_BNBUSDT</th>\n",
       "      <th>Volume_BNBUSDT</th>\n",
       "      <th>Number_of_trades_BNBUSDT</th>\n",
       "      <th>SMA_20</th>\n",
       "      <th>EMA_20</th>\n",
       "      <th>Upper_Band</th>\n",
       "      <th>Middle_Band</th>\n",
       "      <th>Lower_Band</th>\n",
       "      <th>RSI</th>\n",
       "      <th>buy_1000x_high_coinbase</th>\n",
       "      <th>sell_1000x_high_coinbase</th>\n",
       "      <th>total_trades_coinbase</th>\n",
       "      <th>Tweets_Utilizados</th>\n",
       "      <th>Tweets_Utilizados_coin</th>\n",
       "      <th>Tweets_Utilizados_referentes</th>\n",
       "      <th>Tweets_Utilizados_whale_alert</th>\n",
       "      <th>Buy_1000x_high</th>\n",
       "      <th>sell_1000x_high</th>\n",
       "      <th>total_trades_binance</th>\n",
       "      <th>Tendencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>946</th>\n",
       "      <td>2024-04-30</td>\n",
       "      <td>6.59</td>\n",
       "      <td>6.67</td>\n",
       "      <td>6.04</td>\n",
       "      <td>115512.00</td>\n",
       "      <td>60672.00</td>\n",
       "      <td>54947.66</td>\n",
       "      <td>1985671.00</td>\n",
       "      <td>3014.05</td>\n",
       "      <td>561717.49</td>\n",
       "      <td>1292873.00</td>\n",
       "      <td>578.40</td>\n",
       "      <td>766513.45</td>\n",
       "      <td>486465.00</td>\n",
       "      <td>6.93</td>\n",
       "      <td>7.13</td>\n",
       "      <td>7.80</td>\n",
       "      <td>6.93</td>\n",
       "      <td>6.05</td>\n",
       "      <td>34.18</td>\n",
       "      <td>51.00</td>\n",
       "      <td>55.00</td>\n",
       "      <td>48709.00</td>\n",
       "      <td>142</td>\n",
       "      <td>187</td>\n",
       "      <td>1.00</td>\n",
       "      <td>23.00</td>\n",
       "      <td>379.00</td>\n",
       "      <td>377.00</td>\n",
       "      <td>70000.00</td>\n",
       "      <td>Bajista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>947</th>\n",
       "      <td>2024-05-01</td>\n",
       "      <td>6.42</td>\n",
       "      <td>6.93</td>\n",
       "      <td>6.13</td>\n",
       "      <td>175570.00</td>\n",
       "      <td>58364.97</td>\n",
       "      <td>81166.47</td>\n",
       "      <td>2401089.00</td>\n",
       "      <td>2972.46</td>\n",
       "      <td>624963.78</td>\n",
       "      <td>1365039.00</td>\n",
       "      <td>561.80</td>\n",
       "      <td>669027.32</td>\n",
       "      <td>427425.00</td>\n",
       "      <td>6.85</td>\n",
       "      <td>7.11</td>\n",
       "      <td>7.41</td>\n",
       "      <td>6.85</td>\n",
       "      <td>6.29</td>\n",
       "      <td>43.30</td>\n",
       "      <td>42.00</td>\n",
       "      <td>50.00</td>\n",
       "      <td>83718.00</td>\n",
       "      <td>130</td>\n",
       "      <td>177</td>\n",
       "      <td>0.00</td>\n",
       "      <td>36.00</td>\n",
       "      <td>327.00</td>\n",
       "      <td>340.00</td>\n",
       "      <td>107000.00</td>\n",
       "      <td>Alcista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948</th>\n",
       "      <td>2024-05-02</td>\n",
       "      <td>6.90</td>\n",
       "      <td>7.41</td>\n",
       "      <td>6.69</td>\n",
       "      <td>109002.00</td>\n",
       "      <td>59060.61</td>\n",
       "      <td>47583.82</td>\n",
       "      <td>1572898.00</td>\n",
       "      <td>2986.19</td>\n",
       "      <td>365939.72</td>\n",
       "      <td>880167.00</td>\n",
       "      <td>560.50</td>\n",
       "      <td>359794.32</td>\n",
       "      <td>250921.00</td>\n",
       "      <td>6.85</td>\n",
       "      <td>7.12</td>\n",
       "      <td>7.42</td>\n",
       "      <td>6.85</td>\n",
       "      <td>6.29</td>\n",
       "      <td>49.27</td>\n",
       "      <td>87.00</td>\n",
       "      <td>57.00</td>\n",
       "      <td>61208.00</td>\n",
       "      <td>461</td>\n",
       "      <td>374</td>\n",
       "      <td>1.00</td>\n",
       "      <td>25.00</td>\n",
       "      <td>270.00</td>\n",
       "      <td>282.00</td>\n",
       "      <td>71000.00</td>\n",
       "      <td>Alcista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949</th>\n",
       "      <td>2024-05-03</td>\n",
       "      <td>7.27</td>\n",
       "      <td>7.39</td>\n",
       "      <td>7.00</td>\n",
       "      <td>109634.00</td>\n",
       "      <td>62882.01</td>\n",
       "      <td>43628.40</td>\n",
       "      <td>1558661.00</td>\n",
       "      <td>3102.61</td>\n",
       "      <td>355825.84</td>\n",
       "      <td>859542.00</td>\n",
       "      <td>587.00</td>\n",
       "      <td>342906.43</td>\n",
       "      <td>257575.00</td>\n",
       "      <td>6.90</td>\n",
       "      <td>7.14</td>\n",
       "      <td>7.44</td>\n",
       "      <td>6.90</td>\n",
       "      <td>6.35</td>\n",
       "      <td>48.86</td>\n",
       "      <td>52.00</td>\n",
       "      <td>40.00</td>\n",
       "      <td>46255.00</td>\n",
       "      <td>573</td>\n",
       "      <td>474</td>\n",
       "      <td>1.00</td>\n",
       "      <td>22.00</td>\n",
       "      <td>386.00</td>\n",
       "      <td>635.00</td>\n",
       "      <td>69000.00</td>\n",
       "      <td>Lateral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>2024-05-04</td>\n",
       "      <td>7.24</td>\n",
       "      <td>7.28</td>\n",
       "      <td>7.11</td>\n",
       "      <td>71120.00</td>\n",
       "      <td>63892.04</td>\n",
       "      <td>24368.69</td>\n",
       "      <td>1113509.00</td>\n",
       "      <td>3117.23</td>\n",
       "      <td>196263.95</td>\n",
       "      <td>575026.00</td>\n",
       "      <td>585.70</td>\n",
       "      <td>197129.25</td>\n",
       "      <td>210303.00</td>\n",
       "      <td>6.91</td>\n",
       "      <td>7.13</td>\n",
       "      <td>7.46</td>\n",
       "      <td>6.91</td>\n",
       "      <td>6.36</td>\n",
       "      <td>46.98</td>\n",
       "      <td>68.00</td>\n",
       "      <td>50.00</td>\n",
       "      <td>34251.00</td>\n",
       "      <td>407</td>\n",
       "      <td>472</td>\n",
       "      <td>0.00</td>\n",
       "      <td>14.00</td>\n",
       "      <td>203.00</td>\n",
       "      <td>232.00</td>\n",
       "      <td>49000.00</td>\n",
       "      <td>Bajista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951</th>\n",
       "      <td>2024-05-05</td>\n",
       "      <td>7.12</td>\n",
       "      <td>7.40</td>\n",
       "      <td>7.01</td>\n",
       "      <td>72928.00</td>\n",
       "      <td>64012.00</td>\n",
       "      <td>18526.75</td>\n",
       "      <td>992921.00</td>\n",
       "      <td>3136.41</td>\n",
       "      <td>218760.27</td>\n",
       "      <td>600693.00</td>\n",
       "      <td>592.00</td>\n",
       "      <td>180458.24</td>\n",
       "      <td>180794.00</td>\n",
       "      <td>6.94</td>\n",
       "      <td>7.15</td>\n",
       "      <td>7.51</td>\n",
       "      <td>6.94</td>\n",
       "      <td>6.37</td>\n",
       "      <td>50.00</td>\n",
       "      <td>37.00</td>\n",
       "      <td>52.00</td>\n",
       "      <td>29197.00</td>\n",
       "      <td>417</td>\n",
       "      <td>499</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>320.00</td>\n",
       "      <td>284.00</td>\n",
       "      <td>47000.00</td>\n",
       "      <td>Alcista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>2024-05-06</td>\n",
       "      <td>7.30</td>\n",
       "      <td>7.47</td>\n",
       "      <td>7.06</td>\n",
       "      <td>94264.00</td>\n",
       "      <td>63165.19</td>\n",
       "      <td>34674.92</td>\n",
       "      <td>1392557.00</td>\n",
       "      <td>3062.60</td>\n",
       "      <td>355135.30</td>\n",
       "      <td>873200.00</td>\n",
       "      <td>588.20</td>\n",
       "      <td>278669.01</td>\n",
       "      <td>248490.00</td>\n",
       "      <td>6.96</td>\n",
       "      <td>7.15</td>\n",
       "      <td>7.53</td>\n",
       "      <td>6.96</td>\n",
       "      <td>6.39</td>\n",
       "      <td>47.10</td>\n",
       "      <td>49.00</td>\n",
       "      <td>71.00</td>\n",
       "      <td>40027.00</td>\n",
       "      <td>482</td>\n",
       "      <td>531</td>\n",
       "      <td>0.00</td>\n",
       "      <td>25.00</td>\n",
       "      <td>339.00</td>\n",
       "      <td>249.00</td>\n",
       "      <td>59000.00</td>\n",
       "      <td>Bajista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>2024-05-07</td>\n",
       "      <td>7.12</td>\n",
       "      <td>7.29</td>\n",
       "      <td>6.98</td>\n",
       "      <td>64947.00</td>\n",
       "      <td>62312.08</td>\n",
       "      <td>25598.79</td>\n",
       "      <td>1272898.00</td>\n",
       "      <td>3005.69</td>\n",
       "      <td>298796.68</td>\n",
       "      <td>815246.00</td>\n",
       "      <td>576.50</td>\n",
       "      <td>289488.71</td>\n",
       "      <td>266127.00</td>\n",
       "      <td>6.98</td>\n",
       "      <td>7.13</td>\n",
       "      <td>7.52</td>\n",
       "      <td>6.98</td>\n",
       "      <td>6.44</td>\n",
       "      <td>45.10</td>\n",
       "      <td>21.00</td>\n",
       "      <td>25.00</td>\n",
       "      <td>31028.00</td>\n",
       "      <td>495</td>\n",
       "      <td>494</td>\n",
       "      <td>0.00</td>\n",
       "      <td>28.00</td>\n",
       "      <td>296.00</td>\n",
       "      <td>205.00</td>\n",
       "      <td>42000.00</td>\n",
       "      <td>Bajista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>2024-05-08</td>\n",
       "      <td>6.99</td>\n",
       "      <td>7.19</td>\n",
       "      <td>6.89</td>\n",
       "      <td>75550.00</td>\n",
       "      <td>61193.03</td>\n",
       "      <td>26121.19</td>\n",
       "      <td>1415152.00</td>\n",
       "      <td>2974.21</td>\n",
       "      <td>266934.81</td>\n",
       "      <td>830635.00</td>\n",
       "      <td>588.60</td>\n",
       "      <td>297016.62</td>\n",
       "      <td>249379.00</td>\n",
       "      <td>6.99</td>\n",
       "      <td>7.12</td>\n",
       "      <td>7.52</td>\n",
       "      <td>6.99</td>\n",
       "      <td>6.46</td>\n",
       "      <td>44.94</td>\n",
       "      <td>17.00</td>\n",
       "      <td>24.00</td>\n",
       "      <td>32040.00</td>\n",
       "      <td>426</td>\n",
       "      <td>494</td>\n",
       "      <td>0.00</td>\n",
       "      <td>24.00</td>\n",
       "      <td>230.00</td>\n",
       "      <td>177.00</td>\n",
       "      <td>49000.00</td>\n",
       "      <td>Lateral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955</th>\n",
       "      <td>2024-05-09</td>\n",
       "      <td>6.98</td>\n",
       "      <td>7.09</td>\n",
       "      <td>6.78</td>\n",
       "      <td>75016.00</td>\n",
       "      <td>63074.01</td>\n",
       "      <td>30660.81</td>\n",
       "      <td>1381957.00</td>\n",
       "      <td>3036.23</td>\n",
       "      <td>238561.75</td>\n",
       "      <td>686147.00</td>\n",
       "      <td>596.80</td>\n",
       "      <td>464857.60</td>\n",
       "      <td>332988.00</td>\n",
       "      <td>7.01</td>\n",
       "      <td>7.11</td>\n",
       "      <td>7.52</td>\n",
       "      <td>7.01</td>\n",
       "      <td>6.50</td>\n",
       "      <td>46.32</td>\n",
       "      <td>18.00</td>\n",
       "      <td>17.00</td>\n",
       "      <td>29314.00</td>\n",
       "      <td>475</td>\n",
       "      <td>464</td>\n",
       "      <td>0.00</td>\n",
       "      <td>16.00</td>\n",
       "      <td>188.00</td>\n",
       "      <td>257.00</td>\n",
       "      <td>50000.00</td>\n",
       "      <td>Lateral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Open_time  Open  High  Low  Number of trades  Close_BTCUSDT  \\\n",
       "946 2024-04-30  6.59  6.67 6.04         115512.00       60672.00   \n",
       "947 2024-05-01  6.42  6.93 6.13         175570.00       58364.97   \n",
       "948 2024-05-02  6.90  7.41 6.69         109002.00       59060.61   \n",
       "949 2024-05-03  7.27  7.39 7.00         109634.00       62882.01   \n",
       "950 2024-05-04  7.24  7.28 7.11          71120.00       63892.04   \n",
       "951 2024-05-05  7.12  7.40 7.01          72928.00       64012.00   \n",
       "952 2024-05-06  7.30  7.47 7.06          94264.00       63165.19   \n",
       "953 2024-05-07  7.12  7.29 6.98          64947.00       62312.08   \n",
       "954 2024-05-08  6.99  7.19 6.89          75550.00       61193.03   \n",
       "955 2024-05-09  6.98  7.09 6.78          75016.00       63074.01   \n",
       "\n",
       "     Volume_BTCUSDT  Number_of_trades_BTCUSDT  Close_ETHUSDT  Volume_ETHUSDT  \\\n",
       "946        54947.66                1985671.00        3014.05       561717.49   \n",
       "947        81166.47                2401089.00        2972.46       624963.78   \n",
       "948        47583.82                1572898.00        2986.19       365939.72   \n",
       "949        43628.40                1558661.00        3102.61       355825.84   \n",
       "950        24368.69                1113509.00        3117.23       196263.95   \n",
       "951        18526.75                 992921.00        3136.41       218760.27   \n",
       "952        34674.92                1392557.00        3062.60       355135.30   \n",
       "953        25598.79                1272898.00        3005.69       298796.68   \n",
       "954        26121.19                1415152.00        2974.21       266934.81   \n",
       "955        30660.81                1381957.00        3036.23       238561.75   \n",
       "\n",
       "     Number_of_trades_ETHUSDT  Close_BNBUSDT  Volume_BNBUSDT  \\\n",
       "946                1292873.00         578.40       766513.45   \n",
       "947                1365039.00         561.80       669027.32   \n",
       "948                 880167.00         560.50       359794.32   \n",
       "949                 859542.00         587.00       342906.43   \n",
       "950                 575026.00         585.70       197129.25   \n",
       "951                 600693.00         592.00       180458.24   \n",
       "952                 873200.00         588.20       278669.01   \n",
       "953                 815246.00         576.50       289488.71   \n",
       "954                 830635.00         588.60       297016.62   \n",
       "955                 686147.00         596.80       464857.60   \n",
       "\n",
       "     Number_of_trades_BNBUSDT  SMA_20  EMA_20  Upper_Band  Middle_Band  \\\n",
       "946                 486465.00    6.93    7.13        7.80         6.93   \n",
       "947                 427425.00    6.85    7.11        7.41         6.85   \n",
       "948                 250921.00    6.85    7.12        7.42         6.85   \n",
       "949                 257575.00    6.90    7.14        7.44         6.90   \n",
       "950                 210303.00    6.91    7.13        7.46         6.91   \n",
       "951                 180794.00    6.94    7.15        7.51         6.94   \n",
       "952                 248490.00    6.96    7.15        7.53         6.96   \n",
       "953                 266127.00    6.98    7.13        7.52         6.98   \n",
       "954                 249379.00    6.99    7.12        7.52         6.99   \n",
       "955                 332988.00    7.01    7.11        7.52         7.01   \n",
       "\n",
       "     Lower_Band   RSI  buy_1000x_high_coinbase  sell_1000x_high_coinbase  \\\n",
       "946        6.05 34.18                    51.00                     55.00   \n",
       "947        6.29 43.30                    42.00                     50.00   \n",
       "948        6.29 49.27                    87.00                     57.00   \n",
       "949        6.35 48.86                    52.00                     40.00   \n",
       "950        6.36 46.98                    68.00                     50.00   \n",
       "951        6.37 50.00                    37.00                     52.00   \n",
       "952        6.39 47.10                    49.00                     71.00   \n",
       "953        6.44 45.10                    21.00                     25.00   \n",
       "954        6.46 44.94                    17.00                     24.00   \n",
       "955        6.50 46.32                    18.00                     17.00   \n",
       "\n",
       "     total_trades_coinbase  Tweets_Utilizados  Tweets_Utilizados_coin  \\\n",
       "946               48709.00                142                     187   \n",
       "947               83718.00                130                     177   \n",
       "948               61208.00                461                     374   \n",
       "949               46255.00                573                     474   \n",
       "950               34251.00                407                     472   \n",
       "951               29197.00                417                     499   \n",
       "952               40027.00                482                     531   \n",
       "953               31028.00                495                     494   \n",
       "954               32040.00                426                     494   \n",
       "955               29314.00                475                     464   \n",
       "\n",
       "     Tweets_Utilizados_referentes  Tweets_Utilizados_whale_alert  \\\n",
       "946                          1.00                          23.00   \n",
       "947                          0.00                          36.00   \n",
       "948                          1.00                          25.00   \n",
       "949                          1.00                          22.00   \n",
       "950                          0.00                          14.00   \n",
       "951                          0.00                           6.00   \n",
       "952                          0.00                          25.00   \n",
       "953                          0.00                          28.00   \n",
       "954                          0.00                          24.00   \n",
       "955                          0.00                          16.00   \n",
       "\n",
       "     Buy_1000x_high  sell_1000x_high  total_trades_binance Tendencia  \n",
       "946          379.00           377.00              70000.00   Bajista  \n",
       "947          327.00           340.00             107000.00   Alcista  \n",
       "948          270.00           282.00              71000.00   Alcista  \n",
       "949          386.00           635.00              69000.00   Lateral  \n",
       "950          203.00           232.00              49000.00   Bajista  \n",
       "951          320.00           284.00              47000.00   Alcista  \n",
       "952          339.00           249.00              59000.00   Bajista  \n",
       "953          296.00           205.00              42000.00   Bajista  \n",
       "954          230.00           177.00              49000.00   Lateral  \n",
       "955          188.00           257.00              50000.00   Lateral  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(clasifier_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff3a00e1e50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 0, 0, 1, 0, 0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['Alcista', 'Bajista', 'Lateral'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Alcista',\n",
       " 'Bajista',\n",
       " 'Alcista',\n",
       " 'Alcista',\n",
       " 'Alcista',\n",
       " 'Alcista',\n",
       " 'Alcista',\n",
       " 'Bajista',\n",
       " 'Alcista',\n",
       " 'Alcista']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_clases = 3 \n",
    "\n",
    "validation_predictions = ensemble.predict(clasifier_validation.drop(columns=[\"Open_time\", \"Tendencia\"]))\n",
    "predicciones_one_hot = to_categorical(validation_predictions, num_classes=n_clases)\n",
    "etiquetas_numericas = np.argmax(predicciones_one_hot, axis=1)\n",
    "categorias_clases = onehot_encoder.categories_[0]\n",
    "nombres_clases = [categorias_clases[indice] for indice in etiquetas_numericas]\n",
    "\n",
    "display(validation_predictions)\n",
    "display(categorias_clases)\n",
    "display(nombres_clases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Datos generados por auto ml con prophet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 300ms/step\n",
      "1/1 [==============================] - 0s 202ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1, 1, 1, 1, 1, 1, 0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['Alcista', 'Bajista', 'Lateral'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Alcista',\n",
       " 'Alcista',\n",
       " 'Bajista',\n",
       " 'Bajista',\n",
       " 'Bajista',\n",
       " 'Bajista',\n",
       " 'Bajista',\n",
       " 'Bajista',\n",
       " 'Bajista',\n",
       " 'Alcista']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "auto_ml_prophet_df = pd.read_csv('auto_timeseries_models_prophet/predicciones.csv')\n",
    "auto_mp_prophet_predictions = ensemble.predict(auto_ml_prophet_df.drop(columns=[\"Open_time\", \"Close\"]))\n",
    "\n",
    "predicciones_one_hot = to_categorical(auto_mp_prophet_predictions, num_classes=n_clases)\n",
    "etiquetas_numericas = np.argmax(predicciones_one_hot, axis=1)\n",
    "categorias_clases = onehot_encoder.categories_[0]\n",
    "nombres_clases = [categorias_clases[indice] for indice in etiquetas_numericas]\n",
    "\n",
    "display(auto_mp_prophet_predictions)\n",
    "display(categorias_clases)\n",
    "display(nombres_clases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Datos generados por auto ml con stats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 376ms/step\n",
      "1/1 [==============================] - 0s 190ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 0, 0, 1, 0, 0, 0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['Alcista', 'Bajista', 'Lateral'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Alcista',\n",
       " 'Alcista',\n",
       " 'Bajista',\n",
       " 'Alcista',\n",
       " 'Alcista',\n",
       " 'Alcista',\n",
       " 'Bajista',\n",
       " 'Alcista',\n",
       " 'Alcista',\n",
       " 'Alcista']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "auto_ml_stats_df = pd.read_csv('auto_timeseries_models/predicciones.csv')\n",
    "auto_mp_stats_predictions = ensemble.predict(auto_ml_stats_df.drop(columns=[\"Open_time\", \"Close\"]))\n",
    "\n",
    "predicciones_one_hot = to_categorical(auto_mp_stats_predictions, num_classes=n_clases)\n",
    "etiquetas_numericas = np.argmax(predicciones_one_hot, axis=1)\n",
    "categorias_clases = onehot_encoder.categories_[0]\n",
    "nombres_clases = [categorias_clases[indice] for indice in etiquetas_numericas]\n",
    "\n",
    "display(auto_mp_stats_predictions)\n",
    "display(categorias_clases)\n",
    "display(nombres_clases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Datos generados por auto ml con modelos clasicos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 256ms/step\n",
      "1/1 [==============================] - 0s 171ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['Alcista', 'Bajista', 'Lateral'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Alcista',\n",
       " 'Alcista',\n",
       " 'Alcista',\n",
       " 'Alcista',\n",
       " 'Alcista',\n",
       " 'Alcista',\n",
       " 'Alcista',\n",
       " 'Alcista',\n",
       " 'Alcista',\n",
       " 'Alcista']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "auto_ml_df = pd.read_csv('h2o_models/predicciones.csv')\n",
    "auto_mp_predictions = ensemble.predict(auto_ml_df.drop(columns=[\"Open_time\", \"Next_Day_Target\", \"Close\"]))\n",
    "\n",
    "predicciones_one_hot = to_categorical(auto_mp_predictions, num_classes=n_clases)\n",
    "etiquetas_numericas = np.argmax(predicciones_one_hot, axis=1)\n",
    "categorias_clases = onehot_encoder.categories_[0]\n",
    "nombres_clases = [categorias_clases[indice] for indice in etiquetas_numericas]\n",
    "\n",
    "display(auto_mp_predictions)\n",
    "display(categorias_clases)\n",
    "display(nombres_clases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
